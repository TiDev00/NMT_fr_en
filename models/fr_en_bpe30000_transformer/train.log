2021-12-19 00:37:14,794 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-12-19 00:37:14,805 - INFO - joeynmt.data - Loading training data...
2021-12-19 00:38:01,390 - INFO - joeynmt.data - Building vocabulary...
2021-12-19 00:38:17,418 - INFO - joeynmt.data - Loading dev data...
2021-12-19 00:38:18,024 - INFO - joeynmt.data - Loading test data...
2021-12-19 00:38:18,509 - INFO - joeynmt.data - Data loaded.
2021-12-19 00:38:18,510 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-12-19 00:38:19,671 - INFO - joeynmt.model - Enc-dec model built.
2021-12-19 00:38:20,486 - DEBUG - tensorflow - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2021-12-19 00:38:20,625 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-12-19 00:38:20,625 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-12-19 00:38:20,625 - DEBUG - h5py._conv - Creating converter from 7 to 5
2021-12-19 00:38:20,626 - DEBUG - h5py._conv - Creating converter from 5 to 7
2021-12-19 00:38:21,843 - INFO - joeynmt.training - Total params: 59719168
2021-12-19 00:38:21,845 - DEBUG - joeynmt.training - Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2021-12-19 00:38:24,494 - INFO - joeynmt.helpers - cfg.name                           : fr_en_bpe30000_transformer
2021-12-19 00:38:24,494 - INFO - joeynmt.helpers - cfg.data.src                       : fr
2021-12-19 00:38:24,495 - INFO - joeynmt.helpers - cfg.data.trg                       : en
2021-12-19 00:38:24,495 - INFO - joeynmt.helpers - cfg.data.train                     : /content/drive/MyDrive/nmt_projet2/fr-en-baseline//train.30000.bpe
2021-12-19 00:38:24,495 - INFO - joeynmt.helpers - cfg.data.dev                       : /content/drive/MyDrive/nmt_projet2/fr-en-baseline//dev.30000.bpe
2021-12-19 00:38:24,495 - INFO - joeynmt.helpers - cfg.data.test                      : /content/drive/MyDrive/nmt_projet2/fr-en-baseline//test.30000.bpe
2021-12-19 00:38:24,495 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-12-19 00:38:24,496 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-12-19 00:38:24,496 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100
2021-12-19 00:38:24,496 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : /content/drive/MyDrive/nmt_projet2/fr-en-baseline/joint.30000bpe.vocab
2021-12-19 00:38:24,496 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : /content/drive/MyDrive/nmt_projet2/fr-en-baseline/joint.30000bpe.vocab
2021-12-19 00:38:24,496 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-12-19 00:38:24,496 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-12-19 00:38:24,497 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.remove_whitespace : True
2021-12-19 00:38:24,497 - INFO - joeynmt.helpers - cfg.testing.sacrebleu.tokenize     : none
2021-12-19 00:38:24,497 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-12-19 00:38:24,497 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-12-19 00:38:24,497 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens
2021-12-19 00:38:24,497 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]
2021-12-19 00:38:24,497 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-12-19 00:38:24,498 - INFO - joeynmt.helpers - cfg.training.patience              : 3
2021-12-19 00:38:24,498 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7
2021-12-19 00:38:24,498 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy
2021-12-19 00:38:24,498 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0005
2021-12-19 00:38:24,498 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 3e-06
2021-12-19 00:38:24,498 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-12-19 00:38:24,499 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1
2021-12-19 00:38:24,499 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096
2021-12-19 00:38:24,499 - INFO - joeynmt.helpers - cfg.training.batch_type            : token
2021-12-19 00:38:24,499 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : loss
2021-12-19 00:38:24,499 - INFO - joeynmt.helpers - cfg.training.epochs                : 30
2021-12-19 00:38:24,499 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 5000
2021-12-19 00:38:24,499 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 200
2021-12-19 00:38:24,500 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-12-19 00:38:24,500 - INFO - joeynmt.helpers - cfg.training.model_dir             : /content/drive/MyDrive/nmt_projet2/fr-en-baseline//models/fr_en_bpe30000_transformer
2021-12-19 00:38:24,500 - INFO - joeynmt.helpers - cfg.training.overwrite             : False
2021-12-19 00:38:24,500 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-12-19 00:38:24,500 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-12-19 00:38:24,500 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-12-19 00:38:24,501 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-12-19 00:38:24,501 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 2
2021-12-19 00:38:24,501 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-12-19 00:38:24,501 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-12-19 00:38:24,501 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-12-19 00:38:24,501 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-12-19 00:38:24,502 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-12-19 00:38:24,502 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True
2021-12-19 00:38:24,502 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-12-19 00:38:24,502 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-12-19 00:38:24,502 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-12-19 00:38:24,502 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8
2021-12-19 00:38:24,502 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-12-19 00:38:24,503 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-12-19 00:38:24,503 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2021-12-19 00:38:24,503 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-12-19 00:38:24,503 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048
2021-12-19 00:38:24,503 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3
2021-12-19 00:38:24,503 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-12-19 00:38:24,503 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-12-19 00:38:24,504 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8
2021-12-19 00:38:24,504 - INFO - joeynmt.helpers - cfg.model.decoder.attention        : bahdanau
2021-12-19 00:38:24,504 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-12-19 00:38:24,504 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-12-19 00:38:24,504 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2021-12-19 00:38:24,504 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-12-19 00:38:24,505 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048
2021-12-19 00:38:24,505 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3
2021-12-19 00:38:24,505 - INFO - joeynmt.helpers - Data set sizes: 
	train 1993146,
	valid 3000,
	test 3003
2021-12-19 00:38:24,505 - INFO - joeynmt.helpers - First training example:
	[SRC] Re@@ prise de la session
	[TRG] Res@@ um@@ ption of the session
2021-12-19 00:38:24,505 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) de (6) of (7) la (8) to (9) and
2021-12-19 00:38:24,506 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) de (6) of (7) la (8) to (9) and
2021-12-19 00:38:24,506 - INFO - joeynmt.helpers - Number of Src words (types): 30427
2021-12-19 00:38:24,506 - INFO - joeynmt.helpers - Number of Trg words (types): 30427
2021-12-19 00:38:24,507 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=8),
	decoder=TransformerDecoder(num_layers=6, num_heads=8),
	src_embed=Embeddings(embedding_dim=512, vocab_size=30427),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=30427))
2021-12-19 00:38:24,545 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 4096
	total batch size (w. parallel & accumulation): 4096
2021-12-19 00:38:24,545 - INFO - joeynmt.training - EPOCH 1
2021-12-19 00:39:30,485 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     5.832744, Tokens per Sec:     7857, Lr: 0.000500
2021-12-19 00:40:32,521 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     4.763225, Tokens per Sec:     8164, Lr: 0.000500
2021-12-19 00:41:35,208 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     4.789247, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 00:42:37,289 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     4.685200, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 00:43:39,178 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     4.994488, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 00:44:41,581 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     4.874660, Tokens per Sec:     8261, Lr: 0.000500
2021-12-19 00:45:43,600 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     4.591981, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 00:46:45,659 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     4.680970, Tokens per Sec:     8318, Lr: 0.000500
2021-12-19 00:47:47,728 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     4.585504, Tokens per Sec:     8254, Lr: 0.000500
2021-12-19 00:48:50,156 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     4.559706, Tokens per Sec:     8421, Lr: 0.000500
2021-12-19 00:49:52,732 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     4.274601, Tokens per Sec:     8515, Lr: 0.000500
2021-12-19 00:50:55,746 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     4.276398, Tokens per Sec:     8375, Lr: 0.000500
2021-12-19 00:51:57,610 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     4.198346, Tokens per Sec:     8258, Lr: 0.000500
2021-12-19 00:52:59,721 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     4.078381, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 00:54:01,691 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     4.313047, Tokens per Sec:     8259, Lr: 0.000500
2021-12-19 00:55:04,558 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     4.312753, Tokens per Sec:     8392, Lr: 0.000500
2021-12-19 00:56:06,933 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     4.223178, Tokens per Sec:     8341, Lr: 0.000500
2021-12-19 00:57:09,564 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.953220, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 00:58:11,641 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     4.004976, Tokens per Sec:     8282, Lr: 0.000500
2021-12-19 00:59:13,687 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     3.968144, Tokens per Sec:     8262, Lr: 0.000500
2021-12-19 01:00:16,104 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     4.173170, Tokens per Sec:     8359, Lr: 0.000500
2021-12-19 01:01:18,765 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     3.769296, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 01:02:20,353 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     3.882971, Tokens per Sec:     8291, Lr: 0.000500
2021-12-19 01:03:23,198 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     3.746809, Tokens per Sec:     8412, Lr: 0.000500
2021-12-19 01:04:25,209 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     4.078206, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 01:08:39,872 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 01:08:39,872 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 01:08:42,479 - INFO - joeynmt.training - Example #0
2021-12-19 01:08:42,479 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 01:08:42,480 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 'strategy', 'for', 'the', 'new', 'elections', 'in', 'the', 'Middle', 'East', 'is', 'the', 'first', 'of', 'the', 'past.']
2021-12-19 01:08:42,480 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 01:08:42,480 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 01:08:42,480 - INFO - joeynmt.training - 	Hypothesis: A strategy for the new elections in the Middle East is the first of the past.
2021-12-19 01:08:42,480 - INFO - joeynmt.training - Example #1
2021-12-19 01:08:42,481 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 01:08:42,481 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'leaders', 'of', 'the', 'political', 'leaders', 'of', 'their', 'political', 'need', 'to', 'be', 'able', 'to', 'combat', 'the', 'need', 'for', 'a', 'new', 'parliamentary', 'group.']
2021-12-19 01:08:42,481 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 01:08:42,481 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 01:08:42,481 - INFO - joeynmt.training - 	Hypothesis: The leaders of the political leaders of their political need to be able to combat the need for a new parliamentary group.
2021-12-19 01:08:42,482 - INFO - joeynmt.training - Example #2
2021-12-19 01:08:42,482 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 01:08:42,482 - DEBUG - joeynmt.training - 	Raw hypothesis: ['But', 'the', 'new', 'regime', 'is', 'a', 'good', 'example', 'of', 'this', 'week', 'as', 'a', 'result', 'of', 'the', 'United', 'States', 'is', 'a', 'very', 'difficult', 'example', 'of', 'the', 'United', 'States', 'to', 'be', 'able', 'to', 'be', 'able', 'to', 'have', 'a', 'lot', 'of', 'people', 'in', 'the', 'United', 'States.']
2021-12-19 01:08:42,482 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 01:08:42,482 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 01:08:42,483 - INFO - joeynmt.training - 	Hypothesis: But the new regime is a good example of this week as a result of the United States is a very difficult example of the United States to be able to be able to have a lot of people in the United States.
2021-12-19 01:08:42,483 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     5000: bleu:   1.19, loss: 381816.5000, ppl: 132.4395, duration: 257.2738s
2021-12-19 01:09:44,457 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     3.607060, Tokens per Sec:     8246, Lr: 0.000500
2021-12-19 01:10:46,415 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     4.029386, Tokens per Sec:     8175, Lr: 0.000500
2021-12-19 01:11:49,267 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     2.931266, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 01:12:51,393 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     3.511538, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 01:13:53,921 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     3.227368, Tokens per Sec:     8332, Lr: 0.000500
2021-12-19 01:14:55,920 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     3.882544, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 01:15:57,955 - INFO - joeynmt.training - Epoch   1, Step:     6400, Batch Loss:     3.813042, Tokens per Sec:     8245, Lr: 0.000500
2021-12-19 01:16:59,989 - INFO - joeynmt.training - Epoch   1, Step:     6600, Batch Loss:     3.694970, Tokens per Sec:     8285, Lr: 0.000500
2021-12-19 01:18:02,654 - INFO - joeynmt.training - Epoch   1, Step:     6800, Batch Loss:     3.289944, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 01:19:04,905 - INFO - joeynmt.training - Epoch   1, Step:     7000, Batch Loss:     3.263692, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 01:20:06,651 - INFO - joeynmt.training - Epoch   1, Step:     7200, Batch Loss:     2.818624, Tokens per Sec:     8264, Lr: 0.000500
2021-12-19 01:21:08,633 - INFO - joeynmt.training - Epoch   1, Step:     7400, Batch Loss:     3.811482, Tokens per Sec:     8192, Lr: 0.000500
2021-12-19 01:22:10,533 - INFO - joeynmt.training - Epoch   1, Step:     7600, Batch Loss:     2.809047, Tokens per Sec:     8360, Lr: 0.000500
2021-12-19 01:23:12,398 - INFO - joeynmt.training - Epoch   1, Step:     7800, Batch Loss:     3.054060, Tokens per Sec:     8266, Lr: 0.000500
2021-12-19 01:24:14,119 - INFO - joeynmt.training - Epoch   1, Step:     8000, Batch Loss:     2.867857, Tokens per Sec:     8242, Lr: 0.000500
2021-12-19 01:25:16,695 - INFO - joeynmt.training - Epoch   1, Step:     8200, Batch Loss:     2.908313, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 01:26:18,694 - INFO - joeynmt.training - Epoch   1, Step:     8400, Batch Loss:     3.140188, Tokens per Sec:     8234, Lr: 0.000500
2021-12-19 01:27:20,882 - INFO - joeynmt.training - Epoch   1, Step:     8600, Batch Loss:     3.124762, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 01:28:23,179 - INFO - joeynmt.training - Epoch   1, Step:     8800, Batch Loss:     3.174533, Tokens per Sec:     8361, Lr: 0.000500
2021-12-19 01:29:26,077 - INFO - joeynmt.training - Epoch   1, Step:     9000, Batch Loss:     3.281680, Tokens per Sec:     8387, Lr: 0.000500
2021-12-19 01:30:28,386 - INFO - joeynmt.training - Epoch   1, Step:     9200, Batch Loss:     2.956651, Tokens per Sec:     8311, Lr: 0.000500
2021-12-19 01:31:30,546 - INFO - joeynmt.training - Epoch   1, Step:     9400, Batch Loss:     3.073774, Tokens per Sec:     8218, Lr: 0.000500
2021-12-19 01:32:32,067 - INFO - joeynmt.training - Epoch   1, Step:     9600, Batch Loss:     2.905402, Tokens per Sec:     8298, Lr: 0.000500
2021-12-19 01:33:34,399 - INFO - joeynmt.training - Epoch   1, Step:     9800, Batch Loss:     1.941377, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 01:34:37,029 - INFO - joeynmt.training - Epoch   1, Step:    10000, Batch Loss:     2.947995, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 01:38:40,272 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 01:38:40,272 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 01:38:44,943 - INFO - joeynmt.training - Example #0
2021-12-19 01:38:44,943 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 01:38:44,943 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 'public', 'strategy', 'for', 'the', 'immediate', 'election', 'of', 'the', 'Obama', 'election', 'is', 'being', 're@@', 'forme@@', 'd.']
2021-12-19 01:38:44,944 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 01:38:44,944 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 01:38:44,944 - INFO - joeynmt.training - 	Hypothesis: A public strategy for the immediate election of the Obama election is being reformed.
2021-12-19 01:38:44,944 - INFO - joeynmt.training - Example #1
2021-12-19 01:38:44,944 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 01:38:44,944 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'leaders', 'of', 'the', 'public', 'have', 'their', 'political', 'duty', 'to', 'fight', 'against', 'fraud', 'in', 'the', 'electoral', 'campaign.']
2021-12-19 01:38:44,944 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 01:38:44,945 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 01:38:44,945 - INFO - joeynmt.training - 	Hypothesis: The leaders of the public have their political duty to fight against fraud in the electoral campaign.
2021-12-19 01:38:44,945 - INFO - joeynmt.training - Example #2
2021-12-19 01:38:44,945 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 01:38:44,945 - DEBUG - joeynmt.training - 	Raw hypothesis: ['However,', 'the', 'Bren@@', 'ner', 'Centre', 'for', 'this', 'last', 'year', 'is', 'a', 'former', 'Italian', 'newspaper', 'that', 'the', 'Italian', 'election', 'is', 'more', 'than', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'in', 'the', 'United', 'States', 'of', 'the', 'United', 'States', 'of', 'the', 'United', 'States.']
2021-12-19 01:38:44,945 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 01:38:44,945 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 01:38:44,946 - INFO - joeynmt.training - 	Hypothesis: However, the Brenner Centre for this last year is a former Italian newspaper that the Italian election is more than the United States than the number of people in the United States of the United States of the United States.
2021-12-19 01:38:44,946 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step    10000: bleu:   4.54, loss: 322219.1875, ppl:  61.7723, duration: 247.9163s
2021-12-19 01:39:47,345 - INFO - joeynmt.training - Epoch   1, Step:    10200, Batch Loss:     2.622590, Tokens per Sec:     8399, Lr: 0.000500
2021-12-19 01:40:49,878 - INFO - joeynmt.training - Epoch   1, Step:    10400, Batch Loss:     2.828910, Tokens per Sec:     8296, Lr: 0.000500
2021-12-19 01:41:52,758 - INFO - joeynmt.training - Epoch   1, Step:    10600, Batch Loss:     3.015840, Tokens per Sec:     8457, Lr: 0.000500
2021-12-19 01:42:54,698 - INFO - joeynmt.training - Epoch   1, Step:    10800, Batch Loss:     2.691774, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 01:43:56,943 - INFO - joeynmt.training - Epoch   1, Step:    11000, Batch Loss:     2.574870, Tokens per Sec:     8403, Lr: 0.000500
2021-12-19 01:44:59,211 - INFO - joeynmt.training - Epoch   1, Step:    11200, Batch Loss:     2.608824, Tokens per Sec:     8332, Lr: 0.000500
2021-12-19 01:46:01,063 - INFO - joeynmt.training - Epoch   1, Step:    11400, Batch Loss:     3.142520, Tokens per Sec:     8377, Lr: 0.000500
2021-12-19 01:47:03,034 - INFO - joeynmt.training - Epoch   1, Step:    11600, Batch Loss:     2.299071, Tokens per Sec:     8302, Lr: 0.000500
2021-12-19 01:48:05,040 - INFO - joeynmt.training - Epoch   1, Step:    11800, Batch Loss:     2.534891, Tokens per Sec:     8297, Lr: 0.000500
2021-12-19 01:49:07,391 - INFO - joeynmt.training - Epoch   1, Step:    12000, Batch Loss:     2.904808, Tokens per Sec:     8346, Lr: 0.000500
2021-12-19 01:50:09,399 - INFO - joeynmt.training - Epoch   1, Step:    12200, Batch Loss:     3.024156, Tokens per Sec:     8326, Lr: 0.000500
2021-12-19 01:51:11,903 - INFO - joeynmt.training - Epoch   1, Step:    12400, Batch Loss:     2.697705, Tokens per Sec:     8411, Lr: 0.000500
2021-12-19 01:52:14,105 - INFO - joeynmt.training - Epoch   1, Step:    12600, Batch Loss:     2.943267, Tokens per Sec:     8330, Lr: 0.000500
2021-12-19 01:53:16,238 - INFO - joeynmt.training - Epoch   1, Step:    12800, Batch Loss:     2.323701, Tokens per Sec:     8317, Lr: 0.000500
2021-12-19 01:54:18,446 - INFO - joeynmt.training - Epoch   1, Step:    13000, Batch Loss:     2.434307, Tokens per Sec:     8276, Lr: 0.000500
2021-12-19 01:55:20,840 - INFO - joeynmt.training - Epoch   1, Step:    13200, Batch Loss:     1.678259, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 01:56:23,054 - INFO - joeynmt.training - Epoch   1, Step:    13400, Batch Loss:     3.140678, Tokens per Sec:     8316, Lr: 0.000500
2021-12-19 01:57:25,349 - INFO - joeynmt.training - Epoch   1, Step:    13600, Batch Loss:     1.853561, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 01:58:27,291 - INFO - joeynmt.training - Epoch   1, Step:    13800, Batch Loss:     2.688711, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 01:59:29,415 - INFO - joeynmt.training - Epoch   1, Step:    14000, Batch Loss:     2.331977, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 02:00:31,375 - INFO - joeynmt.training - Epoch   1, Step:    14200, Batch Loss:     2.056325, Tokens per Sec:     8304, Lr: 0.000500
2021-12-19 02:01:33,447 - INFO - joeynmt.training - Epoch   1, Step:    14400, Batch Loss:     2.773031, Tokens per Sec:     8330, Lr: 0.000500
2021-12-19 02:02:35,214 - INFO - joeynmt.training - Epoch   1, Step:    14600, Batch Loss:     2.791363, Tokens per Sec:     8268, Lr: 0.000500
2021-12-19 02:03:37,093 - INFO - joeynmt.training - Epoch   1, Step:    14800, Batch Loss:     2.404306, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 02:04:39,659 - INFO - joeynmt.training - Epoch   1, Step:    15000, Batch Loss:     2.458540, Tokens per Sec:     8407, Lr: 0.000500
2021-12-19 02:07:52,933 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 02:07:52,933 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 02:07:55,364 - WARNING - joeynmt.training - Wanted to delete old checkpoint /content/drive/MyDrive/nmt_projet2/fr-en-baseline//models/fr_en_bpe30000_transformer/5000.ckpt but file does not exist.
2021-12-19 02:07:55,384 - INFO - joeynmt.training - Example #0
2021-12-19 02:07:55,384 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 02:07:55,384 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 'public', 'strategy', 'to', 'counter@@', 'act', 'the', 're@@', 'signation', 'of', 'the', 'Obama', 'elections.']
2021-12-19 02:07:55,384 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 02:07:55,384 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 02:07:55,384 - INFO - joeynmt.training - 	Hypothesis: A public strategy to counteract the resignation of the Obama elections.
2021-12-19 02:07:55,384 - INFO - joeynmt.training - Example #1
2021-12-19 02:07:55,385 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 02:07:55,385 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Public', 'Prosecu@@', 'tors', 'are', 'justified', 'by', 'their', 'political', 'need', 'to', 'combat', 'electoral', 'fraud', 'against', 'the', 'ele@@', 'ction.']
2021-12-19 02:07:55,385 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 02:07:55,385 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 02:07:55,385 - INFO - joeynmt.training - 	Hypothesis: Public Prosecutors are justified by their political need to combat electoral fraud against the election.
2021-12-19 02:07:55,385 - INFO - joeynmt.training - Example #2
2021-12-19 02:07:55,385 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 02:07:55,385 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'is', 'in', 'the', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'th@@', 'th@@', 'th@@', 'an,', 'saying', 'that', 'fraud', 'is', 'more', 'dangerous', 'than', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'who', 'have', 'been', 'killed', 'by', 'the', 'car@@', '.']
2021-12-19 02:07:55,386 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 02:07:55,386 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 02:07:55,386 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre is in the last year as a myththththan, saying that fraud is more dangerous than the United States than the number of people who have been killed by the car.
2021-12-19 02:07:55,386 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step    15000: bleu:   7.82, loss: 283200.1875, ppl:  37.4920, duration: 195.7260s
2021-12-19 02:08:57,859 - INFO - joeynmt.training - Epoch   1, Step:    15200, Batch Loss:     2.261870, Tokens per Sec:     8375, Lr: 0.000500
2021-12-19 02:10:00,182 - INFO - joeynmt.training - Epoch   1, Step:    15400, Batch Loss:     2.586905, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 02:11:02,474 - INFO - joeynmt.training - Epoch   1, Step:    15600, Batch Loss:     2.657368, Tokens per Sec:     8445, Lr: 0.000500
2021-12-19 02:12:04,427 - INFO - joeynmt.training - Epoch   1, Step:    15800, Batch Loss:     2.644516, Tokens per Sec:     8362, Lr: 0.000500
2021-12-19 02:13:07,047 - INFO - joeynmt.training - Epoch   1, Step:    16000, Batch Loss:     2.577631, Tokens per Sec:     8287, Lr: 0.000500
2021-12-19 02:14:09,506 - INFO - joeynmt.training - Epoch   1, Step:    16200, Batch Loss:     2.200405, Tokens per Sec:     8362, Lr: 0.000500
2021-12-19 02:15:11,942 - INFO - joeynmt.training - Epoch   1, Step:    16400, Batch Loss:     2.681917, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 02:16:14,195 - INFO - joeynmt.training - Epoch   1, Step:    16600, Batch Loss:     2.466213, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 02:17:16,719 - INFO - joeynmt.training - Epoch   1, Step:    16800, Batch Loss:     3.519286, Tokens per Sec:     8366, Lr: 0.000500
2021-12-19 02:18:18,931 - INFO - joeynmt.training - Epoch   1, Step:    17000, Batch Loss:     2.168596, Tokens per Sec:     8346, Lr: 0.000500
2021-12-19 02:19:20,973 - INFO - joeynmt.training - Epoch   1, Step:    17200, Batch Loss:     2.492723, Tokens per Sec:     8278, Lr: 0.000500
2021-12-19 02:20:22,104 - INFO - joeynmt.training - Epoch   1, Step:    17400, Batch Loss:     2.758554, Tokens per Sec:     8219, Lr: 0.000500
2021-12-19 02:21:24,400 - INFO - joeynmt.training - Epoch   1, Step:    17600, Batch Loss:     2.310987, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 02:22:26,526 - INFO - joeynmt.training - Epoch   1, Step:    17800, Batch Loss:     2.367706, Tokens per Sec:     8446, Lr: 0.000500
2021-12-19 02:23:28,386 - INFO - joeynmt.training - Epoch   1, Step:    18000, Batch Loss:     2.052797, Tokens per Sec:     8292, Lr: 0.000500
2021-12-19 02:24:30,759 - INFO - joeynmt.training - Epoch   1, Step:    18200, Batch Loss:     2.842501, Tokens per Sec:     8342, Lr: 0.000500
2021-12-19 02:25:32,989 - INFO - joeynmt.training - Epoch   1, Step:    18400, Batch Loss:     2.362405, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 02:26:35,615 - INFO - joeynmt.training - Epoch   1, Step:    18600, Batch Loss:     2.100400, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 02:27:37,491 - INFO - joeynmt.training - Epoch   1, Step:    18800, Batch Loss:     1.907410, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 02:28:39,023 - INFO - joeynmt.training - Epoch   1, Step:    19000, Batch Loss:     2.146594, Tokens per Sec:     8209, Lr: 0.000500
2021-12-19 02:29:40,699 - INFO - joeynmt.training - Epoch   1, Step:    19200, Batch Loss:     2.081630, Tokens per Sec:     8330, Lr: 0.000500
2021-12-19 02:30:42,866 - INFO - joeynmt.training - Epoch   1, Step:    19400, Batch Loss:     2.358660, Tokens per Sec:     8406, Lr: 0.000500
2021-12-19 02:31:44,780 - INFO - joeynmt.training - Epoch   1, Step:    19600, Batch Loss:     2.574917, Tokens per Sec:     8272, Lr: 0.000500
2021-12-19 02:32:46,863 - INFO - joeynmt.training - Epoch   1, Step:    19800, Batch Loss:     2.358916, Tokens per Sec:     8302, Lr: 0.000500
2021-12-19 02:33:48,874 - INFO - joeynmt.training - Epoch   1, Step:    20000, Batch Loss:     2.307383, Tokens per Sec:     8356, Lr: 0.000500
2021-12-19 02:36:51,235 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 02:36:51,236 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 02:36:53,779 - INFO - joeynmt.training - Example #0
2021-12-19 02:36:53,779 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 02:36:53,779 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'public@@', 'ate', 'strategy', 'for', 'the', 're@@', 'signation', 'of', 'the', 'Obama', 'elections.']
2021-12-19 02:36:53,780 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 02:36:53,780 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 02:36:53,780 - INFO - joeynmt.training - 	Hypothesis: A republicate strategy for the resignation of the Obama elections.
2021-12-19 02:36:53,780 - INFO - joeynmt.training - Example #1
2021-12-19 02:36:53,781 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 02:36:53,781 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Public', 'leaders', 'are', 'justified', 'by', 'their', 'political', 'need', 'to', 'fight', 'fraud', 'against', 'electoral', 'propagan@@', 'da.']
2021-12-19 02:36:53,781 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 02:36:53,781 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 02:36:53,781 - INFO - joeynmt.training - 	Hypothesis: Public leaders are justified by their political need to fight fraud against electoral propaganda.
2021-12-19 02:36:53,782 - INFO - joeynmt.training - Example #2
2021-12-19 02:36:53,782 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 02:36:53,782 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'is', 'in', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'the', 'electoral', 'fraud', 'is', 'more', 'frequently', 'the', 'United', 'States', 'than', 'many', 'people', 'killed', 'by', 'the', 'd@@', 'rough@@', 't.']
2021-12-19 02:36:53,782 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 02:36:53,782 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 02:36:53,783 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre is in this last year as a mythy, stating that the electoral fraud is more frequently the United States than many people killed by the drought.
2021-12-19 02:36:53,783 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step    20000: bleu:   9.88, loss: 259300.4219, ppl:  27.6128, duration: 184.9083s
2021-12-19 02:37:55,832 - INFO - joeynmt.training - Epoch   1, Step:    20200, Batch Loss:     2.291458, Tokens per Sec:     8286, Lr: 0.000500
2021-12-19 02:38:57,779 - INFO - joeynmt.training - Epoch   1, Step:    20400, Batch Loss:     2.287278, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 02:39:59,993 - INFO - joeynmt.training - Epoch   1, Step:    20600, Batch Loss:     2.218480, Tokens per Sec:     8318, Lr: 0.000500
2021-12-19 02:41:02,268 - INFO - joeynmt.training - Epoch   1, Step:    20800, Batch Loss:     2.216503, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 02:42:04,444 - INFO - joeynmt.training - Epoch   1, Step:    21000, Batch Loss:     2.135143, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 02:43:06,637 - INFO - joeynmt.training - Epoch   1, Step:    21200, Batch Loss:     2.077623, Tokens per Sec:     8323, Lr: 0.000500
2021-12-19 02:44:08,991 - INFO - joeynmt.training - Epoch   1, Step:    21400, Batch Loss:     2.333494, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 02:44:47,386 - INFO - joeynmt.training - Epoch   1: total training loss 66253.96
2021-12-19 02:44:47,386 - INFO - joeynmt.training - EPOCH 2
2021-12-19 02:45:14,163 - INFO - joeynmt.training - Epoch   2, Step:    21600, Batch Loss:     2.236832, Tokens per Sec:     7322, Lr: 0.000500
2021-12-19 02:46:16,388 - INFO - joeynmt.training - Epoch   2, Step:    21800, Batch Loss:     2.247128, Tokens per Sec:     8300, Lr: 0.000500
2021-12-19 02:47:18,601 - INFO - joeynmt.training - Epoch   2, Step:    22000, Batch Loss:     2.273161, Tokens per Sec:     8437, Lr: 0.000500
2021-12-19 02:48:20,787 - INFO - joeynmt.training - Epoch   2, Step:    22200, Batch Loss:     2.259169, Tokens per Sec:     8326, Lr: 0.000500
2021-12-19 02:49:22,835 - INFO - joeynmt.training - Epoch   2, Step:    22400, Batch Loss:     2.253549, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 02:50:24,842 - INFO - joeynmt.training - Epoch   2, Step:    22600, Batch Loss:     2.803172, Tokens per Sec:     8271, Lr: 0.000500
2021-12-19 02:51:26,689 - INFO - joeynmt.training - Epoch   2, Step:    22800, Batch Loss:     2.688747, Tokens per Sec:     8338, Lr: 0.000500
2021-12-19 02:52:28,935 - INFO - joeynmt.training - Epoch   2, Step:    23000, Batch Loss:     2.300632, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 02:53:31,009 - INFO - joeynmt.training - Epoch   2, Step:    23200, Batch Loss:     2.090116, Tokens per Sec:     8346, Lr: 0.000500
2021-12-19 02:54:33,487 - INFO - joeynmt.training - Epoch   2, Step:    23400, Batch Loss:     2.148474, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 02:55:36,028 - INFO - joeynmt.training - Epoch   2, Step:    23600, Batch Loss:     1.845885, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 02:56:38,162 - INFO - joeynmt.training - Epoch   2, Step:    23800, Batch Loss:     2.136237, Tokens per Sec:     8451, Lr: 0.000500
2021-12-19 02:57:40,401 - INFO - joeynmt.training - Epoch   2, Step:    24000, Batch Loss:     2.028218, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 02:58:42,734 - INFO - joeynmt.training - Epoch   2, Step:    24200, Batch Loss:     2.045983, Tokens per Sec:     8438, Lr: 0.000500
2021-12-19 02:59:44,638 - INFO - joeynmt.training - Epoch   2, Step:    24400, Batch Loss:     1.954006, Tokens per Sec:     8282, Lr: 0.000500
2021-12-19 03:00:47,166 - INFO - joeynmt.training - Epoch   2, Step:    24600, Batch Loss:     2.078951, Tokens per Sec:     8379, Lr: 0.000500
2021-12-19 03:01:49,220 - INFO - joeynmt.training - Epoch   2, Step:    24800, Batch Loss:     2.183855, Tokens per Sec:     8336, Lr: 0.000500
2021-12-19 03:02:51,462 - INFO - joeynmt.training - Epoch   2, Step:    25000, Batch Loss:     2.385637, Tokens per Sec:     8277, Lr: 0.000500
2021-12-19 03:05:50,321 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 03:05:50,321 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 03:05:52,977 - INFO - joeynmt.training - Example #0
2021-12-19 03:05:52,978 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 03:05:52,978 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'public', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'O@@', 'il', 'O@@', 'ne@@', '.']
2021-12-19 03:05:52,978 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 03:05:52,978 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 03:05:52,978 - INFO - joeynmt.training - 	Hypothesis: A republic strategy to counter the reelection of Oil One.
2021-12-19 03:05:52,979 - INFO - joeynmt.training - Example #1
2021-12-19 03:05:52,979 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 03:05:52,979 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'public@@', "'s", 'leaders', 'are', 'justified', 'by', 'their', 'policy', 'that', 'the', 'need', 'to', 'fight', 'fraud', 'in', 'the', 'electoral', 'term.']
2021-12-19 03:05:52,979 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 03:05:52,980 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 03:05:52,980 - INFO - joeynmt.training - 	Hypothesis: The public's leaders are justified by their policy that the need to fight fraud in the electoral term.
2021-12-19 03:05:52,980 - INFO - joeynmt.training - Example #2
2021-12-19 03:05:52,980 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 03:05:52,981 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'is', 'in', 'the', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'the', 'electoral', 'fraud', 'is', 'more', 'often', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'h@@', 'our.']
2021-12-19 03:05:52,981 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 03:05:52,981 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 03:05:52,981 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre is in the last year as a mythy, stating that the electoral fraud is more often the United States than the number of people killed by the hour.
2021-12-19 03:05:52,982 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    25000: bleu:  11.12, loss: 245774.1406, ppl:  23.2240, duration: 181.5191s
2021-12-19 03:06:54,954 - INFO - joeynmt.training - Epoch   2, Step:    25200, Batch Loss:     2.085459, Tokens per Sec:     8332, Lr: 0.000500
2021-12-19 03:07:57,279 - INFO - joeynmt.training - Epoch   2, Step:    25400, Batch Loss:     2.144363, Tokens per Sec:     8383, Lr: 0.000500
2021-12-19 03:08:59,409 - INFO - joeynmt.training - Epoch   2, Step:    25600, Batch Loss:     2.322301, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 03:10:01,125 - INFO - joeynmt.training - Epoch   2, Step:    25800, Batch Loss:     1.935480, Tokens per Sec:     8329, Lr: 0.000500
2021-12-19 03:11:03,418 - INFO - joeynmt.training - Epoch   2, Step:    26000, Batch Loss:     2.145533, Tokens per Sec:     8386, Lr: 0.000500
2021-12-19 03:12:05,591 - INFO - joeynmt.training - Epoch   2, Step:    26200, Batch Loss:     2.079503, Tokens per Sec:     8280, Lr: 0.000500
2021-12-19 03:13:06,904 - INFO - joeynmt.training - Epoch   2, Step:    26400, Batch Loss:     2.409333, Tokens per Sec:     8221, Lr: 0.000500
2021-12-19 03:14:08,939 - INFO - joeynmt.training - Epoch   2, Step:    26600, Batch Loss:     1.864111, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 03:15:11,262 - INFO - joeynmt.training - Epoch   2, Step:    26800, Batch Loss:     2.142987, Tokens per Sec:     8418, Lr: 0.000500
2021-12-19 03:16:13,483 - INFO - joeynmt.training - Epoch   2, Step:    27000, Batch Loss:     2.248650, Tokens per Sec:     8362, Lr: 0.000500
2021-12-19 03:17:15,681 - INFO - joeynmt.training - Epoch   2, Step:    27200, Batch Loss:     2.028758, Tokens per Sec:     8378, Lr: 0.000500
2021-12-19 03:18:17,432 - INFO - joeynmt.training - Epoch   2, Step:    27400, Batch Loss:     2.284039, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 03:19:18,833 - INFO - joeynmt.training - Epoch   2, Step:    27600, Batch Loss:     1.903338, Tokens per Sec:     8289, Lr: 0.000500
2021-12-19 03:20:20,443 - INFO - joeynmt.training - Epoch   2, Step:    27800, Batch Loss:     1.878920, Tokens per Sec:     8218, Lr: 0.000500
2021-12-19 03:21:22,178 - INFO - joeynmt.training - Epoch   2, Step:    28000, Batch Loss:     1.937704, Tokens per Sec:     8201, Lr: 0.000500
2021-12-19 03:22:23,883 - INFO - joeynmt.training - Epoch   2, Step:    28200, Batch Loss:     1.989013, Tokens per Sec:     8276, Lr: 0.000500
2021-12-19 03:23:26,161 - INFO - joeynmt.training - Epoch   2, Step:    28400, Batch Loss:     1.722232, Tokens per Sec:     8309, Lr: 0.000500
2021-12-19 03:24:27,639 - INFO - joeynmt.training - Epoch   2, Step:    28600, Batch Loss:     2.026834, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 03:25:29,180 - INFO - joeynmt.training - Epoch   2, Step:    28800, Batch Loss:     1.480359, Tokens per Sec:     8245, Lr: 0.000500
2021-12-19 03:26:31,325 - INFO - joeynmt.training - Epoch   2, Step:    29000, Batch Loss:     2.169830, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 03:27:33,296 - INFO - joeynmt.training - Epoch   2, Step:    29200, Batch Loss:     2.353945, Tokens per Sec:     8412, Lr: 0.000500
2021-12-19 03:28:35,147 - INFO - joeynmt.training - Epoch   2, Step:    29400, Batch Loss:     2.172024, Tokens per Sec:     8409, Lr: 0.000500
2021-12-19 03:29:37,470 - INFO - joeynmt.training - Epoch   2, Step:    29600, Batch Loss:     1.824479, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 03:30:39,338 - INFO - joeynmt.training - Epoch   2, Step:    29800, Batch Loss:     1.862850, Tokens per Sec:     8262, Lr: 0.000500
2021-12-19 03:31:40,700 - INFO - joeynmt.training - Epoch   2, Step:    30000, Batch Loss:     1.762403, Tokens per Sec:     8268, Lr: 0.000500
2021-12-19 03:34:22,643 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 03:34:22,644 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 03:34:25,123 - INFO - joeynmt.training - Example #0
2021-12-19 03:34:25,123 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 03:34:25,123 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'public', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'the', 'Obama', 'reac@@', 'tor@@', '.']
2021-12-19 03:34:25,124 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 03:34:25,124 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 03:34:25,124 - INFO - joeynmt.training - 	Hypothesis: A republic strategy to counter the reelection of the Obama reactor.
2021-12-19 03:34:25,124 - INFO - joeynmt.training - Example #1
2021-12-19 03:34:25,124 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 03:34:25,124 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 're@@', 'public', 'leaders', 'are', 'justified', 'by', 'the', 'need', 'to', 'fight', 'electoral', 'fraud.']
2021-12-19 03:34:25,124 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 03:34:25,124 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 03:34:25,125 - INFO - joeynmt.training - 	Hypothesis: The republic leaders are justified by the need to fight electoral fraud.
2021-12-19 03:34:25,125 - INFO - joeynmt.training - Example #2
2021-12-19 03:34:25,125 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 03:34:25,125 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'e,', 'claiming', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'than', 'the', 'United', 'States', 'of', 'America', 'than', 'many', 'of', 'the', 'people', 'killed', 'by', 'the', 'ra@@', 'ge.']
2021-12-19 03:34:25,125 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 03:34:25,125 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 03:34:25,125 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this last year as a mythe, claiming that electoral fraud is more rare than the United States of America than many of the people killed by the rage.
2021-12-19 03:34:25,125 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    30000: bleu:  12.15, loss: 236515.1094, ppl:  20.6290, duration: 164.4250s
2021-12-19 03:35:27,447 - INFO - joeynmt.training - Epoch   2, Step:    30200, Batch Loss:     2.083989, Tokens per Sec:     8380, Lr: 0.000500
2021-12-19 03:36:29,556 - INFO - joeynmt.training - Epoch   2, Step:    30400, Batch Loss:     2.134393, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 03:37:31,449 - INFO - joeynmt.training - Epoch   2, Step:    30600, Batch Loss:     2.175060, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 03:38:33,593 - INFO - joeynmt.training - Epoch   2, Step:    30800, Batch Loss:     1.913359, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 03:39:35,477 - INFO - joeynmt.training - Epoch   2, Step:    31000, Batch Loss:     2.228765, Tokens per Sec:     8342, Lr: 0.000500
2021-12-19 03:40:37,726 - INFO - joeynmt.training - Epoch   2, Step:    31200, Batch Loss:     2.167354, Tokens per Sec:     8357, Lr: 0.000500
2021-12-19 03:41:39,593 - INFO - joeynmt.training - Epoch   2, Step:    31400, Batch Loss:     1.913288, Tokens per Sec:     8467, Lr: 0.000500
2021-12-19 03:42:41,184 - INFO - joeynmt.training - Epoch   2, Step:    31600, Batch Loss:     1.830631, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 03:43:43,226 - INFO - joeynmt.training - Epoch   2, Step:    31800, Batch Loss:     1.700838, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 03:44:44,897 - INFO - joeynmt.training - Epoch   2, Step:    32000, Batch Loss:     2.174503, Tokens per Sec:     8311, Lr: 0.000500
2021-12-19 03:45:46,775 - INFO - joeynmt.training - Epoch   2, Step:    32200, Batch Loss:     2.477111, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 03:46:48,379 - INFO - joeynmt.training - Epoch   2, Step:    32400, Batch Loss:     1.884993, Tokens per Sec:     8343, Lr: 0.000500
2021-12-19 03:47:50,825 - INFO - joeynmt.training - Epoch   2, Step:    32600, Batch Loss:     1.965684, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 03:48:52,747 - INFO - joeynmt.training - Epoch   2, Step:    32800, Batch Loss:     1.969584, Tokens per Sec:     8400, Lr: 0.000500
2021-12-19 03:49:54,470 - INFO - joeynmt.training - Epoch   2, Step:    33000, Batch Loss:     1.714952, Tokens per Sec:     8291, Lr: 0.000500
2021-12-19 03:50:56,946 - INFO - joeynmt.training - Epoch   2, Step:    33200, Batch Loss:     1.886653, Tokens per Sec:     8300, Lr: 0.000500
2021-12-19 03:51:59,040 - INFO - joeynmt.training - Epoch   2, Step:    33400, Batch Loss:     2.424550, Tokens per Sec:     8292, Lr: 0.000500
2021-12-19 03:53:01,173 - INFO - joeynmt.training - Epoch   2, Step:    33600, Batch Loss:     2.209301, Tokens per Sec:     8420, Lr: 0.000500
2021-12-19 03:54:02,938 - INFO - joeynmt.training - Epoch   2, Step:    33800, Batch Loss:     1.960897, Tokens per Sec:     8243, Lr: 0.000500
2021-12-19 03:55:05,415 - INFO - joeynmt.training - Epoch   2, Step:    34000, Batch Loss:     2.068276, Tokens per Sec:     8436, Lr: 0.000500
2021-12-19 03:56:07,327 - INFO - joeynmt.training - Epoch   2, Step:    34200, Batch Loss:     1.990861, Tokens per Sec:     8236, Lr: 0.000500
2021-12-19 03:57:09,389 - INFO - joeynmt.training - Epoch   2, Step:    34400, Batch Loss:     2.032487, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 03:58:11,205 - INFO - joeynmt.training - Epoch   2, Step:    34600, Batch Loss:     1.829871, Tokens per Sec:     8375, Lr: 0.000500
2021-12-19 03:59:13,306 - INFO - joeynmt.training - Epoch   2, Step:    34800, Batch Loss:     2.058485, Tokens per Sec:     8330, Lr: 0.000500
2021-12-19 04:00:15,319 - INFO - joeynmt.training - Epoch   2, Step:    35000, Batch Loss:     1.845122, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 04:02:40,776 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 04:02:40,776 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 04:02:43,397 - INFO - joeynmt.training - Example #0
2021-12-19 04:02:43,398 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 04:02:43,398 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'public', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'Obama']
2021-12-19 04:02:43,398 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 04:02:43,398 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 04:02:43,399 - INFO - joeynmt.training - 	Hypothesis: A republic strategy to counter the reelection of Obama
2021-12-19 04:02:43,399 - INFO - joeynmt.training - Example #1
2021-12-19 04:02:43,399 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 04:02:43,399 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 're@@', 'public', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'fight', 'fraud', 'against', 'electoral', 'fraud.']
2021-12-19 04:02:43,399 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 04:02:43,399 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 04:02:43,399 - INFO - joeynmt.training - 	Hypothesis: The republic leaders justify their policy by the need to fight fraud against electoral fraud.
2021-12-19 04:02:43,400 - INFO - joeynmt.training - Example #2
2021-12-19 04:02:43,400 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 04:02:43,400 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'y,', 'claiming', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'to', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'in.']
2021-12-19 04:02:43,400 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 04:02:43,400 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 04:02:43,400 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this last year as a mythy, claiming that electoral fraud is more rare to the United States than the number of people killed by the rain.
2021-12-19 04:02:43,400 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    35000: bleu:  12.93, loss: 227227.6250, ppl:  18.3173, duration: 148.0810s
2021-12-19 04:03:45,299 - INFO - joeynmt.training - Epoch   2, Step:    35200, Batch Loss:     1.852042, Tokens per Sec:     8427, Lr: 0.000500
2021-12-19 04:04:47,309 - INFO - joeynmt.training - Epoch   2, Step:    35400, Batch Loss:     1.961597, Tokens per Sec:     8408, Lr: 0.000500
2021-12-19 04:05:49,214 - INFO - joeynmt.training - Epoch   2, Step:    35600, Batch Loss:     2.010874, Tokens per Sec:     8231, Lr: 0.000500
2021-12-19 04:06:51,551 - INFO - joeynmt.training - Epoch   2, Step:    35800, Batch Loss:     2.017889, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 04:07:53,257 - INFO - joeynmt.training - Epoch   2, Step:    36000, Batch Loss:     1.797301, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 04:08:55,544 - INFO - joeynmt.training - Epoch   2, Step:    36200, Batch Loss:     1.981931, Tokens per Sec:     8398, Lr: 0.000500
2021-12-19 04:09:57,578 - INFO - joeynmt.training - Epoch   2, Step:    36400, Batch Loss:     2.033881, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 04:10:59,918 - INFO - joeynmt.training - Epoch   2, Step:    36600, Batch Loss:     1.921697, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 04:12:01,671 - INFO - joeynmt.training - Epoch   2, Step:    36800, Batch Loss:     2.178684, Tokens per Sec:     8322, Lr: 0.000500
2021-12-19 04:13:03,498 - INFO - joeynmt.training - Epoch   2, Step:    37000, Batch Loss:     2.258584, Tokens per Sec:     8385, Lr: 0.000500
2021-12-19 04:14:05,463 - INFO - joeynmt.training - Epoch   2, Step:    37200, Batch Loss:     2.357255, Tokens per Sec:     8336, Lr: 0.000500
2021-12-19 04:15:07,379 - INFO - joeynmt.training - Epoch   2, Step:    37400, Batch Loss:     2.195091, Tokens per Sec:     8348, Lr: 0.000500
2021-12-19 04:16:09,430 - INFO - joeynmt.training - Epoch   2, Step:    37600, Batch Loss:     1.857681, Tokens per Sec:     8324, Lr: 0.000500
2021-12-19 04:17:11,281 - INFO - joeynmt.training - Epoch   2, Step:    37800, Batch Loss:     1.983217, Tokens per Sec:     8320, Lr: 0.000500
2021-12-19 04:18:13,389 - INFO - joeynmt.training - Epoch   2, Step:    38000, Batch Loss:     2.081948, Tokens per Sec:     8389, Lr: 0.000500
2021-12-19 04:19:15,582 - INFO - joeynmt.training - Epoch   2, Step:    38200, Batch Loss:     1.744780, Tokens per Sec:     8356, Lr: 0.000500
2021-12-19 04:20:17,290 - INFO - joeynmt.training - Epoch   2, Step:    38400, Batch Loss:     1.952767, Tokens per Sec:     8306, Lr: 0.000500
2021-12-19 04:21:19,670 - INFO - joeynmt.training - Epoch   2, Step:    38600, Batch Loss:     2.204706, Tokens per Sec:     8280, Lr: 0.000500
2021-12-19 04:22:21,958 - INFO - joeynmt.training - Epoch   2, Step:    38800, Batch Loss:     2.305029, Tokens per Sec:     8482, Lr: 0.000500
2021-12-19 04:23:23,915 - INFO - joeynmt.training - Epoch   2, Step:    39000, Batch Loss:     2.281485, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 04:24:25,790 - INFO - joeynmt.training - Epoch   2, Step:    39200, Batch Loss:     1.977440, Tokens per Sec:     8443, Lr: 0.000500
2021-12-19 04:25:28,515 - INFO - joeynmt.training - Epoch   2, Step:    39400, Batch Loss:     1.818945, Tokens per Sec:     8434, Lr: 0.000500
2021-12-19 04:26:30,506 - INFO - joeynmt.training - Epoch   2, Step:    39600, Batch Loss:     1.886918, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 04:27:32,505 - INFO - joeynmt.training - Epoch   2, Step:    39800, Batch Loss:     1.772901, Tokens per Sec:     8362, Lr: 0.000500
2021-12-19 04:28:34,671 - INFO - joeynmt.training - Epoch   2, Step:    40000, Batch Loss:     1.926083, Tokens per Sec:     8343, Lr: 0.000500
2021-12-19 04:31:05,787 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 04:31:05,788 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 04:31:10,327 - INFO - joeynmt.training - Example #0
2021-12-19 04:31:10,328 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 04:31:10,328 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'ci@@', 'ous', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'Obama']
2021-12-19 04:31:10,328 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 04:31:10,328 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 04:31:10,328 - INFO - joeynmt.training - 	Hypothesis: A republicious strategy to counter the reelection of Obama
2021-12-19 04:31:10,328 - INFO - joeynmt.training - Example #1
2021-12-19 04:31:10,328 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 04:31:10,328 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 're@@', 'publi@@', 'cl@@', 'y-@@', 'based', 'public', 'leaders', 'are', 'justified', 'by', 'the', 'need', 'to', 'fight', 'electoral', 'fraud.']
2021-12-19 04:31:10,329 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 04:31:10,329 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 04:31:10,329 - INFO - joeynmt.training - 	Hypothesis: The republicly-based public leaders are justified by the need to fight electoral fraud.
2021-12-19 04:31:10,329 - INFO - joeynmt.training - Example #2
2021-12-19 04:31:10,329 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 04:31:10,329 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'than', 'the', 'US', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'd@@', 'y.']
2021-12-19 04:31:10,329 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 04:31:10,329 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 04:31:10,330 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last year as a mythy, saying that electoral fraud is more rare than the US than the number of people killed by the rady.
2021-12-19 04:31:10,330 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step    40000: bleu:  13.28, loss: 221312.9062, ppl:  16.9820, duration: 155.6580s
2021-12-19 04:32:13,302 - INFO - joeynmt.training - Epoch   2, Step:    40200, Batch Loss:     1.831488, Tokens per Sec:     8293, Lr: 0.000500
2021-12-19 04:33:15,778 - INFO - joeynmt.training - Epoch   2, Step:    40400, Batch Loss:     1.572050, Tokens per Sec:     8387, Lr: 0.000500
2021-12-19 04:34:18,108 - INFO - joeynmt.training - Epoch   2, Step:    40600, Batch Loss:     2.045496, Tokens per Sec:     8324, Lr: 0.000500
2021-12-19 04:35:20,053 - INFO - joeynmt.training - Epoch   2, Step:    40800, Batch Loss:     2.054158, Tokens per Sec:     8354, Lr: 0.000500
2021-12-19 04:36:22,183 - INFO - joeynmt.training - Epoch   2, Step:    41000, Batch Loss:     1.859050, Tokens per Sec:     8440, Lr: 0.000500
2021-12-19 04:37:24,456 - INFO - joeynmt.training - Epoch   2, Step:    41200, Batch Loss:     1.885892, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 04:38:26,206 - INFO - joeynmt.training - Epoch   2, Step:    41400, Batch Loss:     1.956896, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 04:39:28,045 - INFO - joeynmt.training - Epoch   2, Step:    41600, Batch Loss:     2.299962, Tokens per Sec:     8330, Lr: 0.000500
2021-12-19 04:40:30,287 - INFO - joeynmt.training - Epoch   2, Step:    41800, Batch Loss:     1.832715, Tokens per Sec:     8405, Lr: 0.000500
2021-12-19 04:41:32,484 - INFO - joeynmt.training - Epoch   2, Step:    42000, Batch Loss:     2.047035, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 04:42:34,516 - INFO - joeynmt.training - Epoch   2, Step:    42200, Batch Loss:     1.801238, Tokens per Sec:     8336, Lr: 0.000500
2021-12-19 04:43:37,000 - INFO - joeynmt.training - Epoch   2, Step:    42400, Batch Loss:     1.935481, Tokens per Sec:     8424, Lr: 0.000500
2021-12-19 04:44:39,030 - INFO - joeynmt.training - Epoch   2, Step:    42600, Batch Loss:     1.905982, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 04:45:41,324 - INFO - joeynmt.training - Epoch   2, Step:    42800, Batch Loss:     1.710172, Tokens per Sec:     8403, Lr: 0.000500
2021-12-19 04:46:43,864 - INFO - joeynmt.training - Epoch   2, Step:    43000, Batch Loss:     1.894214, Tokens per Sec:     8324, Lr: 0.000500
2021-12-19 04:46:59,769 - INFO - joeynmt.training - Epoch   2: total training loss 43562.07
2021-12-19 04:46:59,770 - INFO - joeynmt.training - EPOCH 3
2021-12-19 04:47:49,387 - INFO - joeynmt.training - Epoch   3, Step:    43200, Batch Loss:     1.891887, Tokens per Sec:     7897, Lr: 0.000500
2021-12-19 04:48:51,254 - INFO - joeynmt.training - Epoch   3, Step:    43400, Batch Loss:     1.739605, Tokens per Sec:     8295, Lr: 0.000500
2021-12-19 04:49:52,958 - INFO - joeynmt.training - Epoch   3, Step:    43600, Batch Loss:     1.993723, Tokens per Sec:     8358, Lr: 0.000500
2021-12-19 04:50:55,137 - INFO - joeynmt.training - Epoch   3, Step:    43800, Batch Loss:     1.889540, Tokens per Sec:     8335, Lr: 0.000500
2021-12-19 04:51:57,212 - INFO - joeynmt.training - Epoch   3, Step:    44000, Batch Loss:     1.428011, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 04:52:59,539 - INFO - joeynmt.training - Epoch   3, Step:    44200, Batch Loss:     1.895836, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 04:54:02,082 - INFO - joeynmt.training - Epoch   3, Step:    44400, Batch Loss:     1.811202, Tokens per Sec:     8532, Lr: 0.000500
2021-12-19 04:55:04,230 - INFO - joeynmt.training - Epoch   3, Step:    44600, Batch Loss:     1.804854, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 04:56:05,814 - INFO - joeynmt.training - Epoch   3, Step:    44800, Batch Loss:     1.778306, Tokens per Sec:     8283, Lr: 0.000500
2021-12-19 04:57:07,689 - INFO - joeynmt.training - Epoch   3, Step:    45000, Batch Loss:     1.958209, Tokens per Sec:     8256, Lr: 0.000500
2021-12-19 04:59:42,202 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 04:59:42,202 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 04:59:44,661 - INFO - joeynmt.training - Example #0
2021-12-19 04:59:44,662 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 04:59:44,662 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'public', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'O@@', 'ce@@', 'remon@@', 'y.']
2021-12-19 04:59:44,662 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 04:59:44,662 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 04:59:44,662 - INFO - joeynmt.training - 	Hypothesis: A republic strategy to counter the reelection of Oceremony.
2021-12-19 04:59:44,663 - INFO - joeynmt.training - Example #1
2021-12-19 04:59:44,663 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 04:59:44,663 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 're@@', 'public', 'leaders', 'are', 'justified', 'by', 'their', 'policy', 'of', 'the', 'need', 'to', 'fight', 'against', 'electoral', 'fraud.']
2021-12-19 04:59:44,663 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 04:59:44,663 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 04:59:44,663 - INFO - joeynmt.training - 	Hypothesis: The republic leaders are justified by their policy of the need to fight against electoral fraud.
2021-12-19 04:59:44,663 - INFO - joeynmt.training - Example #2
2021-12-19 04:59:44,663 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 04:59:44,664 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'ic,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'than', 'the', 'United', 'States', 'and', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'in.']
2021-12-19 04:59:44,664 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 04:59:44,664 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 04:59:44,664 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last year as a mythic, saying that electoral fraud is more rare than the United States and the number of people killed by the rain.
2021-12-19 04:59:44,664 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    45000: bleu:  13.78, loss: 216876.3281, ppl:  16.0447, duration: 156.9744s
2021-12-19 05:00:47,102 - INFO - joeynmt.training - Epoch   3, Step:    45200, Batch Loss:     1.957152, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 05:01:48,950 - INFO - joeynmt.training - Epoch   3, Step:    45400, Batch Loss:     1.840039, Tokens per Sec:     8341, Lr: 0.000500
2021-12-19 05:02:51,179 - INFO - joeynmt.training - Epoch   3, Step:    45600, Batch Loss:     1.687951, Tokens per Sec:     8282, Lr: 0.000500
2021-12-19 05:03:53,320 - INFO - joeynmt.training - Epoch   3, Step:    45800, Batch Loss:     1.723974, Tokens per Sec:     8392, Lr: 0.000500
2021-12-19 05:04:55,483 - INFO - joeynmt.training - Epoch   3, Step:    46000, Batch Loss:     2.240105, Tokens per Sec:     8391, Lr: 0.000500
2021-12-19 05:05:57,195 - INFO - joeynmt.training - Epoch   3, Step:    46200, Batch Loss:     1.828964, Tokens per Sec:     8260, Lr: 0.000500
2021-12-19 05:06:58,975 - INFO - joeynmt.training - Epoch   3, Step:    46400, Batch Loss:     1.712628, Tokens per Sec:     8331, Lr: 0.000500
2021-12-19 05:08:01,100 - INFO - joeynmt.training - Epoch   3, Step:    46600, Batch Loss:     2.090404, Tokens per Sec:     8416, Lr: 0.000500
2021-12-19 05:09:03,169 - INFO - joeynmt.training - Epoch   3, Step:    46800, Batch Loss:     1.934832, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 05:10:04,884 - INFO - joeynmt.training - Epoch   3, Step:    47000, Batch Loss:     1.953412, Tokens per Sec:     8386, Lr: 0.000500
2021-12-19 05:11:06,979 - INFO - joeynmt.training - Epoch   3, Step:    47200, Batch Loss:     1.964908, Tokens per Sec:     8367, Lr: 0.000500
2021-12-19 05:12:08,800 - INFO - joeynmt.training - Epoch   3, Step:    47400, Batch Loss:     1.909301, Tokens per Sec:     8282, Lr: 0.000500
2021-12-19 05:13:10,244 - INFO - joeynmt.training - Epoch   3, Step:    47600, Batch Loss:     1.928552, Tokens per Sec:     8316, Lr: 0.000500
2021-12-19 05:14:12,513 - INFO - joeynmt.training - Epoch   3, Step:    47800, Batch Loss:     2.134353, Tokens per Sec:     8480, Lr: 0.000500
2021-12-19 05:15:14,465 - INFO - joeynmt.training - Epoch   3, Step:    48000, Batch Loss:     1.967310, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 05:16:16,685 - INFO - joeynmt.training - Epoch   3, Step:    48200, Batch Loss:     1.873340, Tokens per Sec:     8436, Lr: 0.000500
2021-12-19 05:17:18,985 - INFO - joeynmt.training - Epoch   3, Step:    48400, Batch Loss:     1.828797, Tokens per Sec:     8369, Lr: 0.000500
2021-12-19 05:18:20,921 - INFO - joeynmt.training - Epoch   3, Step:    48600, Batch Loss:     1.765948, Tokens per Sec:     8421, Lr: 0.000500
2021-12-19 05:19:23,064 - INFO - joeynmt.training - Epoch   3, Step:    48800, Batch Loss:     1.441342, Tokens per Sec:     8290, Lr: 0.000500
2021-12-19 05:20:24,845 - INFO - joeynmt.training - Epoch   3, Step:    49000, Batch Loss:     1.986012, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 05:21:26,498 - INFO - joeynmt.training - Epoch   3, Step:    49200, Batch Loss:     1.838727, Tokens per Sec:     8258, Lr: 0.000500
2021-12-19 05:22:28,385 - INFO - joeynmt.training - Epoch   3, Step:    49400, Batch Loss:     2.128228, Tokens per Sec:     8328, Lr: 0.000500
2021-12-19 05:23:31,119 - INFO - joeynmt.training - Epoch   3, Step:    49600, Batch Loss:     1.729566, Tokens per Sec:     8421, Lr: 0.000500
2021-12-19 05:24:32,987 - INFO - joeynmt.training - Epoch   3, Step:    49800, Batch Loss:     1.742279, Tokens per Sec:     8276, Lr: 0.000500
2021-12-19 05:25:34,455 - INFO - joeynmt.training - Epoch   3, Step:    50000, Batch Loss:     1.712222, Tokens per Sec:     8275, Lr: 0.000500
2021-12-19 05:27:56,634 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 05:27:56,635 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 05:27:59,107 - INFO - joeynmt.training - Example #0
2021-12-19 05:27:59,107 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 05:27:59,107 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 've', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'Obama']
2021-12-19 05:27:59,107 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 05:27:59,107 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 05:27:59,108 - INFO - joeynmt.training - 	Hypothesis: A republive strategy to counter the reelection of Obama
2021-12-19 05:27:59,108 - INFO - joeynmt.training - Example #1
2021-12-19 05:27:59,108 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 05:27:59,108 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 're@@', 'publi@@', 'cl@@', 'ing', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'fight', 'electoral', 'fraud.']
2021-12-19 05:27:59,108 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 05:27:59,108 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 05:27:59,108 - INFO - joeynmt.training - 	Hypothesis: The republicling leaders justify their policy by the need to fight electoral fraud.
2021-12-19 05:27:59,108 - INFO - joeynmt.training - Example #2
2021-12-19 05:27:59,108 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 05:27:59,109 - DEBUG - joeynmt.training - 	Raw hypothesis: ['However,', 'the', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'than', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'pe.']
2021-12-19 05:27:59,109 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 05:27:59,109 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 05:27:59,109 - INFO - joeynmt.training - 	Hypothesis: However, the Brennan Centre considers this last to be a mythy, stating that electoral fraud is more rare than the United States than the number of people killed by the rape.
2021-12-19 05:27:59,109 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    50000: bleu:  14.21, loss: 212819.0781, ppl:  15.2329, duration: 144.6535s
2021-12-19 05:29:01,512 - INFO - joeynmt.training - Epoch   3, Step:    50200, Batch Loss:     1.946749, Tokens per Sec:     8345, Lr: 0.000500
2021-12-19 05:30:03,766 - INFO - joeynmt.training - Epoch   3, Step:    50400, Batch Loss:     1.703143, Tokens per Sec:     8417, Lr: 0.000500
2021-12-19 05:31:06,084 - INFO - joeynmt.training - Epoch   3, Step:    50600, Batch Loss:     1.839875, Tokens per Sec:     8439, Lr: 0.000500
2021-12-19 05:32:08,329 - INFO - joeynmt.training - Epoch   3, Step:    50800, Batch Loss:     1.887287, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 05:33:09,695 - INFO - joeynmt.training - Epoch   3, Step:    51000, Batch Loss:     1.976384, Tokens per Sec:     8279, Lr: 0.000500
2021-12-19 05:34:12,050 - INFO - joeynmt.training - Epoch   3, Step:    51200, Batch Loss:     1.530307, Tokens per Sec:     8345, Lr: 0.000500
2021-12-19 05:35:13,805 - INFO - joeynmt.training - Epoch   3, Step:    51400, Batch Loss:     1.963060, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 05:36:16,026 - INFO - joeynmt.training - Epoch   3, Step:    51600, Batch Loss:     1.831667, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 05:37:17,810 - INFO - joeynmt.training - Epoch   3, Step:    51800, Batch Loss:     2.304132, Tokens per Sec:     8296, Lr: 0.000500
2021-12-19 05:38:20,081 - INFO - joeynmt.training - Epoch   3, Step:    52000, Batch Loss:     1.829492, Tokens per Sec:     8320, Lr: 0.000500
2021-12-19 05:39:22,343 - INFO - joeynmt.training - Epoch   3, Step:    52200, Batch Loss:     1.831149, Tokens per Sec:     8417, Lr: 0.000500
2021-12-19 05:40:24,050 - INFO - joeynmt.training - Epoch   3, Step:    52400, Batch Loss:     1.876739, Tokens per Sec:     8310, Lr: 0.000500
2021-12-19 05:41:25,457 - INFO - joeynmt.training - Epoch   3, Step:    52600, Batch Loss:     1.939194, Tokens per Sec:     8199, Lr: 0.000500
2021-12-19 05:42:27,610 - INFO - joeynmt.training - Epoch   3, Step:    52800, Batch Loss:     1.833521, Tokens per Sec:     8343, Lr: 0.000500
2021-12-19 05:43:29,291 - INFO - joeynmt.training - Epoch   3, Step:    53000, Batch Loss:     1.718977, Tokens per Sec:     8268, Lr: 0.000500
2021-12-19 05:44:31,284 - INFO - joeynmt.training - Epoch   3, Step:    53200, Batch Loss:     1.625922, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 05:45:33,657 - INFO - joeynmt.training - Epoch   3, Step:    53400, Batch Loss:     1.925281, Tokens per Sec:     8417, Lr: 0.000500
2021-12-19 05:46:35,244 - INFO - joeynmt.training - Epoch   3, Step:    53600, Batch Loss:     1.759583, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 05:47:37,026 - INFO - joeynmt.training - Epoch   3, Step:    53800, Batch Loss:     1.679822, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 05:48:38,948 - INFO - joeynmt.training - Epoch   3, Step:    54000, Batch Loss:     1.736730, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 05:49:41,037 - INFO - joeynmt.training - Epoch   3, Step:    54200, Batch Loss:     1.934868, Tokens per Sec:     8407, Lr: 0.000500
2021-12-19 05:50:42,935 - INFO - joeynmt.training - Epoch   3, Step:    54400, Batch Loss:     2.019068, Tokens per Sec:     8336, Lr: 0.000500
2021-12-19 05:51:45,229 - INFO - joeynmt.training - Epoch   3, Step:    54600, Batch Loss:     1.739467, Tokens per Sec:     8438, Lr: 0.000500
2021-12-19 05:52:47,727 - INFO - joeynmt.training - Epoch   3, Step:    54800, Batch Loss:     1.621067, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 05:53:49,848 - INFO - joeynmt.training - Epoch   3, Step:    55000, Batch Loss:     1.644965, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 05:56:23,577 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 05:56:23,577 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 05:56:26,014 - INFO - joeynmt.training - Example #0
2021-12-19 05:56:26,014 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 05:56:26,014 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'O@@', 'ba@@', '.']
2021-12-19 05:56:26,015 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 05:56:26,015 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 05:56:26,015 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Oba.
2021-12-19 05:56:26,015 - INFO - joeynmt.training - Example #1
2021-12-19 05:56:26,015 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 05:56:26,015 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'cl@@', 'y-@@', 'elected', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 05:56:26,015 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 05:56:26,015 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 05:56:26,016 - INFO - joeynmt.training - 	Hypothesis: Republicly-elected leaders justify their policy by the need to combat electoral fraud.
2021-12-19 05:56:26,016 - INFO - joeynmt.training - Example #2
2021-12-19 05:56:26,016 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 05:56:26,016 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'time', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'than', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'in.']
2021-12-19 05:56:26,016 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 05:56:26,016 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 05:56:26,016 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last time to be a mythy, saying that electoral fraud is more rare than the United States than the number of people killed by the rain.
2021-12-19 05:56:26,017 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    55000: bleu:  14.88, loss: 208469.8906, ppl:  14.4082, duration: 156.1685s
2021-12-19 05:57:27,858 - INFO - joeynmt.training - Epoch   3, Step:    55200, Batch Loss:     1.706821, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 05:58:30,111 - INFO - joeynmt.training - Epoch   3, Step:    55400, Batch Loss:     1.739430, Tokens per Sec:     8414, Lr: 0.000500
2021-12-19 05:59:31,813 - INFO - joeynmt.training - Epoch   3, Step:    55600, Batch Loss:     1.685681, Tokens per Sec:     8281, Lr: 0.000500
2021-12-19 06:00:34,276 - INFO - joeynmt.training - Epoch   3, Step:    55800, Batch Loss:     1.895101, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 06:01:35,540 - INFO - joeynmt.training - Epoch   3, Step:    56000, Batch Loss:     1.822834, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 06:02:37,785 - INFO - joeynmt.training - Epoch   3, Step:    56200, Batch Loss:     2.317383, Tokens per Sec:     8436, Lr: 0.000500
2021-12-19 06:03:39,420 - INFO - joeynmt.training - Epoch   3, Step:    56400, Batch Loss:     1.544783, Tokens per Sec:     8446, Lr: 0.000500
2021-12-19 06:04:41,431 - INFO - joeynmt.training - Epoch   3, Step:    56600, Batch Loss:     1.644594, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 06:05:43,046 - INFO - joeynmt.training - Epoch   3, Step:    56800, Batch Loss:     1.938777, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 06:06:45,421 - INFO - joeynmt.training - Epoch   3, Step:    57000, Batch Loss:     1.802556, Tokens per Sec:     8459, Lr: 0.000500
2021-12-19 06:07:47,193 - INFO - joeynmt.training - Epoch   3, Step:    57200, Batch Loss:     1.764614, Tokens per Sec:     8300, Lr: 0.000500
2021-12-19 06:08:48,853 - INFO - joeynmt.training - Epoch   3, Step:    57400, Batch Loss:     1.721835, Tokens per Sec:     8406, Lr: 0.000500
2021-12-19 06:09:50,504 - INFO - joeynmt.training - Epoch   3, Step:    57600, Batch Loss:     1.737890, Tokens per Sec:     8418, Lr: 0.000500
2021-12-19 06:10:52,525 - INFO - joeynmt.training - Epoch   3, Step:    57800, Batch Loss:     1.996490, Tokens per Sec:     8358, Lr: 0.000500
2021-12-19 06:11:54,756 - INFO - joeynmt.training - Epoch   3, Step:    58000, Batch Loss:     1.670229, Tokens per Sec:     8387, Lr: 0.000500
2021-12-19 06:12:56,051 - INFO - joeynmt.training - Epoch   3, Step:    58200, Batch Loss:     1.785456, Tokens per Sec:     8281, Lr: 0.000500
2021-12-19 06:13:57,647 - INFO - joeynmt.training - Epoch   3, Step:    58400, Batch Loss:     1.893686, Tokens per Sec:     8306, Lr: 0.000500
2021-12-19 06:14:59,418 - INFO - joeynmt.training - Epoch   3, Step:    58600, Batch Loss:     1.801519, Tokens per Sec:     8289, Lr: 0.000500
2021-12-19 06:16:01,234 - INFO - joeynmt.training - Epoch   3, Step:    58800, Batch Loss:     2.176774, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 06:17:03,323 - INFO - joeynmt.training - Epoch   3, Step:    59000, Batch Loss:     1.780929, Tokens per Sec:     8378, Lr: 0.000500
2021-12-19 06:18:04,956 - INFO - joeynmt.training - Epoch   3, Step:    59200, Batch Loss:     1.892435, Tokens per Sec:     8256, Lr: 0.000500
2021-12-19 06:19:06,939 - INFO - joeynmt.training - Epoch   3, Step:    59400, Batch Loss:     1.839515, Tokens per Sec:     8425, Lr: 0.000500
2021-12-19 06:20:09,039 - INFO - joeynmt.training - Epoch   3, Step:    59600, Batch Loss:     1.896227, Tokens per Sec:     8389, Lr: 0.000500
2021-12-19 06:21:11,204 - INFO - joeynmt.training - Epoch   3, Step:    59800, Batch Loss:     1.679211, Tokens per Sec:     8411, Lr: 0.000500
2021-12-19 06:22:13,527 - INFO - joeynmt.training - Epoch   3, Step:    60000, Batch Loss:     1.859963, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 06:24:43,840 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 06:24:43,841 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 06:24:46,335 - INFO - joeynmt.training - Example #0
2021-12-19 06:24:46,336 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 06:24:46,336 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'O@@', 'ba@@', '.']
2021-12-19 06:24:46,336 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 06:24:46,336 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 06:24:46,336 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the reelection of Oba.
2021-12-19 06:24:46,337 - INFO - joeynmt.training - Example #1
2021-12-19 06:24:46,337 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 06:24:46,337 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 're@@', 'publi@@', 'cl@@', 'y-@@', 'minded', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 06:24:46,337 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 06:24:46,337 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 06:24:46,337 - INFO - joeynmt.training - 	Hypothesis: The republicly-minded leaders justify their policy by the need to combat electoral fraud.
2021-12-19 06:24:46,337 - INFO - joeynmt.training - Example #2
2021-12-19 06:24:46,338 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 06:24:46,338 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'in.']
2021-12-19 06:24:46,338 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 06:24:46,338 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 06:24:46,338 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last year as a mythy, saying that electoral fraud is more rare than the number of people killed by the rain.
2021-12-19 06:24:46,338 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step    60000: bleu:  15.11, loss: 206196.3594, ppl:  13.9951, duration: 152.8103s
2021-12-19 06:25:48,548 - INFO - joeynmt.training - Epoch   3, Step:    60200, Batch Loss:     1.761401, Tokens per Sec:     8308, Lr: 0.000500
2021-12-19 06:26:50,731 - INFO - joeynmt.training - Epoch   3, Step:    60400, Batch Loss:     1.733352, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 06:27:52,370 - INFO - joeynmt.training - Epoch   3, Step:    60600, Batch Loss:     1.755119, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 06:28:54,342 - INFO - joeynmt.training - Epoch   3, Step:    60800, Batch Loss:     1.544265, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 06:29:56,355 - INFO - joeynmt.training - Epoch   3, Step:    61000, Batch Loss:     1.667081, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 06:30:59,133 - INFO - joeynmt.training - Epoch   3, Step:    61200, Batch Loss:     1.744559, Tokens per Sec:     8428, Lr: 0.000500
2021-12-19 06:32:00,801 - INFO - joeynmt.training - Epoch   3, Step:    61400, Batch Loss:     1.892683, Tokens per Sec:     8396, Lr: 0.000500
2021-12-19 06:33:02,881 - INFO - joeynmt.training - Epoch   3, Step:    61600, Batch Loss:     2.042202, Tokens per Sec:     8363, Lr: 0.000500
2021-12-19 06:34:04,954 - INFO - joeynmt.training - Epoch   3, Step:    61800, Batch Loss:     1.761355, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 06:35:07,514 - INFO - joeynmt.training - Epoch   3, Step:    62000, Batch Loss:     1.820540, Tokens per Sec:     8441, Lr: 0.000500
2021-12-19 06:36:09,789 - INFO - joeynmt.training - Epoch   3, Step:    62200, Batch Loss:     1.588279, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 06:37:11,684 - INFO - joeynmt.training - Epoch   3, Step:    62400, Batch Loss:     1.772941, Tokens per Sec:     8411, Lr: 0.000500
2021-12-19 06:38:14,024 - INFO - joeynmt.training - Epoch   3, Step:    62600, Batch Loss:     1.705703, Tokens per Sec:     8335, Lr: 0.000500
2021-12-19 06:39:16,443 - INFO - joeynmt.training - Epoch   3, Step:    62800, Batch Loss:     1.695257, Tokens per Sec:     8443, Lr: 0.000500
2021-12-19 06:40:18,509 - INFO - joeynmt.training - Epoch   3, Step:    63000, Batch Loss:     1.840615, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 06:41:20,585 - INFO - joeynmt.training - Epoch   3, Step:    63200, Batch Loss:     1.775689, Tokens per Sec:     8385, Lr: 0.000500
2021-12-19 06:42:22,475 - INFO - joeynmt.training - Epoch   3, Step:    63400, Batch Loss:     1.759725, Tokens per Sec:     8385, Lr: 0.000500
2021-12-19 06:43:24,345 - INFO - joeynmt.training - Epoch   3, Step:    63600, Batch Loss:     1.749915, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 06:44:26,659 - INFO - joeynmt.training - Epoch   3, Step:    63800, Batch Loss:     1.939153, Tokens per Sec:     8328, Lr: 0.000500
2021-12-19 06:45:28,362 - INFO - joeynmt.training - Epoch   3, Step:    64000, Batch Loss:     1.880098, Tokens per Sec:     8256, Lr: 0.000500
2021-12-19 06:46:30,717 - INFO - joeynmt.training - Epoch   3, Step:    64200, Batch Loss:     1.556635, Tokens per Sec:     8302, Lr: 0.000500
2021-12-19 06:47:32,771 - INFO - joeynmt.training - Epoch   3, Step:    64400, Batch Loss:     1.866349, Tokens per Sec:     8399, Lr: 0.000500
2021-12-19 06:48:23,907 - INFO - joeynmt.training - Epoch   3: total training loss 39347.06
2021-12-19 06:48:23,908 - INFO - joeynmt.training - EPOCH 4
2021-12-19 06:48:37,939 - INFO - joeynmt.training - Epoch   4, Step:    64600, Batch Loss:     1.984973, Tokens per Sec:     6621, Lr: 0.000500
2021-12-19 06:49:40,454 - INFO - joeynmt.training - Epoch   4, Step:    64800, Batch Loss:     1.772021, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 06:50:41,784 - INFO - joeynmt.training - Epoch   4, Step:    65000, Batch Loss:     1.660788, Tokens per Sec:     8289, Lr: 0.000500
2021-12-19 06:53:09,736 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 06:53:09,737 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 06:53:12,266 - INFO - joeynmt.training - Example #0
2021-12-19 06:53:12,266 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 06:53:12,266 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'public@@', 'ised', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 06:53:12,267 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 06:53:12,267 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 06:53:12,267 - INFO - joeynmt.training - 	Hypothesis: A republicised strategy to counter the re-election of Obama
2021-12-19 06:53:12,267 - INFO - joeynmt.training - Example #1
2021-12-19 06:53:12,267 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 06:53:12,267 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 're@@', 'public@@', 'ised', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 06:53:12,267 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 06:53:12,268 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 06:53:12,268 - INFO - joeynmt.training - 	Hypothesis: The republicised leaders justify their policy by the need to combat electoral fraud.
2021-12-19 06:53:12,268 - INFO - joeynmt.training - Example #2
2021-12-19 06:53:12,268 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 06:53:12,268 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'one', 'my@@', 'th,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'to', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'd@@', 'augh@@', 'ter.']
2021-12-19 06:53:12,268 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 06:53:12,268 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 06:53:12,269 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last one myth, saying that electoral fraud is more rare to the United States than the number of people killed by the daughter.
2021-12-19 06:53:12,269 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    65000: bleu:  15.32, loss: 204900.3125, ppl:  13.7649, duration: 150.4845s
2021-12-19 06:54:14,329 - INFO - joeynmt.training - Epoch   4, Step:    65200, Batch Loss:     2.038803, Tokens per Sec:     8314, Lr: 0.000500
2021-12-19 06:55:16,270 - INFO - joeynmt.training - Epoch   4, Step:    65400, Batch Loss:     1.602808, Tokens per Sec:     8483, Lr: 0.000500
2021-12-19 06:56:18,285 - INFO - joeynmt.training - Epoch   4, Step:    65600, Batch Loss:     2.225201, Tokens per Sec:     8423, Lr: 0.000500
2021-12-19 06:57:20,394 - INFO - joeynmt.training - Epoch   4, Step:    65800, Batch Loss:     1.697170, Tokens per Sec:     8369, Lr: 0.000500
2021-12-19 06:58:22,532 - INFO - joeynmt.training - Epoch   4, Step:    66000, Batch Loss:     1.878170, Tokens per Sec:     8357, Lr: 0.000500
2021-12-19 06:59:24,751 - INFO - joeynmt.training - Epoch   4, Step:    66200, Batch Loss:     1.754642, Tokens per Sec:     8421, Lr: 0.000500
2021-12-19 07:00:27,153 - INFO - joeynmt.training - Epoch   4, Step:    66400, Batch Loss:     2.038799, Tokens per Sec:     8467, Lr: 0.000500
2021-12-19 07:01:28,893 - INFO - joeynmt.training - Epoch   4, Step:    66600, Batch Loss:     1.773984, Tokens per Sec:     8392, Lr: 0.000500
2021-12-19 07:02:30,560 - INFO - joeynmt.training - Epoch   4, Step:    66800, Batch Loss:     2.040263, Tokens per Sec:     8260, Lr: 0.000500
2021-12-19 07:03:33,052 - INFO - joeynmt.training - Epoch   4, Step:    67000, Batch Loss:     1.581866, Tokens per Sec:     8331, Lr: 0.000500
2021-12-19 07:04:35,318 - INFO - joeynmt.training - Epoch   4, Step:    67200, Batch Loss:     1.935451, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 07:05:37,601 - INFO - joeynmt.training - Epoch   4, Step:    67400, Batch Loss:     1.740948, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 07:06:40,057 - INFO - joeynmt.training - Epoch   4, Step:    67600, Batch Loss:     1.774634, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 07:07:42,115 - INFO - joeynmt.training - Epoch   4, Step:    67800, Batch Loss:     1.947024, Tokens per Sec:     8411, Lr: 0.000500
2021-12-19 07:08:44,349 - INFO - joeynmt.training - Epoch   4, Step:    68000, Batch Loss:     1.910714, Tokens per Sec:     8326, Lr: 0.000500
2021-12-19 07:09:46,650 - INFO - joeynmt.training - Epoch   4, Step:    68200, Batch Loss:     1.674067, Tokens per Sec:     8385, Lr: 0.000500
2021-12-19 07:10:48,608 - INFO - joeynmt.training - Epoch   4, Step:    68400, Batch Loss:     1.866952, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 07:11:50,853 - INFO - joeynmt.training - Epoch   4, Step:    68600, Batch Loss:     1.625550, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 07:12:52,554 - INFO - joeynmt.training - Epoch   4, Step:    68800, Batch Loss:     1.341071, Tokens per Sec:     8366, Lr: 0.000500
2021-12-19 07:13:54,474 - INFO - joeynmt.training - Epoch   4, Step:    69000, Batch Loss:     1.713270, Tokens per Sec:     8332, Lr: 0.000500
2021-12-19 07:14:56,604 - INFO - joeynmt.training - Epoch   4, Step:    69200, Batch Loss:     1.671029, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 07:15:58,740 - INFO - joeynmt.training - Epoch   4, Step:    69400, Batch Loss:     2.194574, Tokens per Sec:     8343, Lr: 0.000500
2021-12-19 07:17:00,540 - INFO - joeynmt.training - Epoch   4, Step:    69600, Batch Loss:     1.821996, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 07:18:02,366 - INFO - joeynmt.training - Epoch   4, Step:    69800, Batch Loss:     1.682600, Tokens per Sec:     8254, Lr: 0.000500
2021-12-19 07:19:04,782 - INFO - joeynmt.training - Epoch   4, Step:    70000, Batch Loss:     1.627733, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 07:21:25,262 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 07:21:25,263 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 07:21:27,885 - INFO - joeynmt.training - Example #0
2021-12-19 07:21:27,886 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 07:21:27,886 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'public@@', 'ised', 'strategy', 'to', 'counter@@', 'act', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 07:21:27,886 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 07:21:27,886 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 07:21:27,886 - INFO - joeynmt.training - 	Hypothesis: A republicised strategy to counteract the re-election of Obama
2021-12-19 07:21:27,887 - INFO - joeynmt.training - Example #1
2021-12-19 07:21:27,887 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 07:21:27,887 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'public@@', 'ising', 'leaders', 'are', 'justified', 'by', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 07:21:27,887 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 07:21:27,887 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 07:21:27,887 - INFO - joeynmt.training - 	Hypothesis: Republicising leaders are justified by their policy by the need to combat electoral fraud.
2021-12-19 07:21:27,887 - INFO - joeynmt.training - Example #2
2021-12-19 07:21:27,887 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 07:21:27,888 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'in.']
2021-12-19 07:21:27,888 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 07:21:27,888 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 07:21:27,888 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this last year as a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the rain.
2021-12-19 07:21:27,888 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    70000: bleu:  15.46, loss: 202394.4375, ppl:  13.3305, duration: 143.1060s
2021-12-19 07:22:30,136 - INFO - joeynmt.training - Epoch   4, Step:    70200, Batch Loss:     1.784184, Tokens per Sec:     8289, Lr: 0.000500
2021-12-19 07:23:32,393 - INFO - joeynmt.training - Epoch   4, Step:    70400, Batch Loss:     1.669462, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 07:24:34,424 - INFO - joeynmt.training - Epoch   4, Step:    70600, Batch Loss:     1.710510, Tokens per Sec:     8362, Lr: 0.000500
2021-12-19 07:25:36,485 - INFO - joeynmt.training - Epoch   4, Step:    70800, Batch Loss:     1.838031, Tokens per Sec:     8338, Lr: 0.000500
2021-12-19 07:26:38,234 - INFO - joeynmt.training - Epoch   4, Step:    71000, Batch Loss:     1.673610, Tokens per Sec:     8267, Lr: 0.000500
2021-12-19 07:27:40,209 - INFO - joeynmt.training - Epoch   4, Step:    71200, Batch Loss:     1.681833, Tokens per Sec:     8300, Lr: 0.000500
2021-12-19 07:28:42,296 - INFO - joeynmt.training - Epoch   4, Step:    71400, Batch Loss:     1.762280, Tokens per Sec:     8369, Lr: 0.000500
2021-12-19 07:29:44,198 - INFO - joeynmt.training - Epoch   4, Step:    71600, Batch Loss:     1.682335, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 07:30:46,457 - INFO - joeynmt.training - Epoch   4, Step:    71800, Batch Loss:     1.661204, Tokens per Sec:     8322, Lr: 0.000500
2021-12-19 07:31:47,603 - INFO - joeynmt.training - Epoch   4, Step:    72000, Batch Loss:     1.772422, Tokens per Sec:     8255, Lr: 0.000500
2021-12-19 07:32:49,529 - INFO - joeynmt.training - Epoch   4, Step:    72200, Batch Loss:     1.683708, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 07:33:51,235 - INFO - joeynmt.training - Epoch   4, Step:    72400, Batch Loss:     1.839770, Tokens per Sec:     8266, Lr: 0.000500
2021-12-19 07:34:52,425 - INFO - joeynmt.training - Epoch   4, Step:    72600, Batch Loss:     1.714600, Tokens per Sec:     8286, Lr: 0.000500
2021-12-19 07:35:54,337 - INFO - joeynmt.training - Epoch   4, Step:    72800, Batch Loss:     1.542124, Tokens per Sec:     8284, Lr: 0.000500
2021-12-19 07:36:56,100 - INFO - joeynmt.training - Epoch   4, Step:    73000, Batch Loss:     1.749321, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 07:37:58,474 - INFO - joeynmt.training - Epoch   4, Step:    73200, Batch Loss:     1.630103, Tokens per Sec:     8422, Lr: 0.000500
2021-12-19 07:39:01,346 - INFO - joeynmt.training - Epoch   4, Step:    73400, Batch Loss:     1.634910, Tokens per Sec:     8436, Lr: 0.000500
2021-12-19 07:40:02,967 - INFO - joeynmt.training - Epoch   4, Step:    73600, Batch Loss:     1.678416, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 07:41:04,792 - INFO - joeynmt.training - Epoch   4, Step:    73800, Batch Loss:     1.816262, Tokens per Sec:     8400, Lr: 0.000500
2021-12-19 07:42:06,843 - INFO - joeynmt.training - Epoch   4, Step:    74000, Batch Loss:     1.900388, Tokens per Sec:     8308, Lr: 0.000500
2021-12-19 07:43:09,035 - INFO - joeynmt.training - Epoch   4, Step:    74200, Batch Loss:     1.848586, Tokens per Sec:     8405, Lr: 0.000500
2021-12-19 07:44:10,657 - INFO - joeynmt.training - Epoch   4, Step:    74400, Batch Loss:     1.726570, Tokens per Sec:     8255, Lr: 0.000500
2021-12-19 07:45:12,581 - INFO - joeynmt.training - Epoch   4, Step:    74600, Batch Loss:     1.652326, Tokens per Sec:     8367, Lr: 0.000500
2021-12-19 07:46:14,959 - INFO - joeynmt.training - Epoch   4, Step:    74800, Batch Loss:     1.759112, Tokens per Sec:     8377, Lr: 0.000500
2021-12-19 07:47:17,554 - INFO - joeynmt.training - Epoch   4, Step:    75000, Batch Loss:     1.651593, Tokens per Sec:     8434, Lr: 0.000500
2021-12-19 07:49:43,949 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 07:49:43,950 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 07:49:48,635 - INFO - joeynmt.training - Example #0
2021-12-19 07:49:48,635 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 07:49:48,635 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'public@@', 'ised', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 07:49:48,636 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 07:49:48,636 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 07:49:48,636 - INFO - joeynmt.training - 	Hypothesis: A republicised strategy to counter the re-election of Obama
2021-12-19 07:49:48,636 - INFO - joeynmt.training - Example #1
2021-12-19 07:49:48,637 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 07:49:48,637 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 07:49:48,637 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 07:49:48,637 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 07:49:48,638 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 07:49:48,638 - INFO - joeynmt.training - Example #2
2021-12-19 07:49:48,638 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 07:49:48,638 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'last', 'year', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'in.']
2021-12-19 07:49:48,639 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 07:49:48,639 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 07:49:48,639 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this last year to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the rain.
2021-12-19 07:49:48,639 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    75000: bleu:  15.81, loss: 199906.5156, ppl:  12.9127, duration: 151.0847s
2021-12-19 07:50:50,489 - INFO - joeynmt.training - Epoch   4, Step:    75200, Batch Loss:     2.097689, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 07:51:52,006 - INFO - joeynmt.training - Epoch   4, Step:    75400, Batch Loss:     1.847724, Tokens per Sec:     8280, Lr: 0.000500
2021-12-19 07:52:53,885 - INFO - joeynmt.training - Epoch   4, Step:    75600, Batch Loss:     1.655391, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 07:53:56,452 - INFO - joeynmt.training - Epoch   4, Step:    75800, Batch Loss:     1.818451, Tokens per Sec:     8396, Lr: 0.000500
2021-12-19 07:54:58,627 - INFO - joeynmt.training - Epoch   4, Step:    76000, Batch Loss:     1.673662, Tokens per Sec:     8397, Lr: 0.000500
2021-12-19 07:56:00,758 - INFO - joeynmt.training - Epoch   4, Step:    76200, Batch Loss:     1.564824, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 07:57:02,101 - INFO - joeynmt.training - Epoch   4, Step:    76400, Batch Loss:     1.838658, Tokens per Sec:     8214, Lr: 0.000500
2021-12-19 07:58:04,631 - INFO - joeynmt.training - Epoch   4, Step:    76600, Batch Loss:     1.758927, Tokens per Sec:     8439, Lr: 0.000500
2021-12-19 07:59:06,654 - INFO - joeynmt.training - Epoch   4, Step:    76800, Batch Loss:     2.023816, Tokens per Sec:     8432, Lr: 0.000500
2021-12-19 08:00:08,856 - INFO - joeynmt.training - Epoch   4, Step:    77000, Batch Loss:     1.950227, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 08:01:11,453 - INFO - joeynmt.training - Epoch   4, Step:    77200, Batch Loss:     1.755957, Tokens per Sec:     8451, Lr: 0.000500
2021-12-19 08:02:13,514 - INFO - joeynmt.training - Epoch   4, Step:    77400, Batch Loss:     2.131810, Tokens per Sec:     8404, Lr: 0.000500
2021-12-19 08:03:16,112 - INFO - joeynmt.training - Epoch   4, Step:    77600, Batch Loss:     1.820080, Tokens per Sec:     8406, Lr: 0.000500
2021-12-19 08:04:18,721 - INFO - joeynmt.training - Epoch   4, Step:    77800, Batch Loss:     1.588583, Tokens per Sec:     8414, Lr: 0.000500
2021-12-19 08:05:20,699 - INFO - joeynmt.training - Epoch   4, Step:    78000, Batch Loss:     1.607984, Tokens per Sec:     8375, Lr: 0.000500
2021-12-19 08:06:22,450 - INFO - joeynmt.training - Epoch   4, Step:    78200, Batch Loss:     1.631459, Tokens per Sec:     8366, Lr: 0.000500
2021-12-19 08:07:24,664 - INFO - joeynmt.training - Epoch   4, Step:    78400, Batch Loss:     1.690922, Tokens per Sec:     8408, Lr: 0.000500
2021-12-19 08:08:26,541 - INFO - joeynmt.training - Epoch   4, Step:    78600, Batch Loss:     1.800443, Tokens per Sec:     8323, Lr: 0.000500
2021-12-19 08:09:29,134 - INFO - joeynmt.training - Epoch   4, Step:    78800, Batch Loss:     1.884095, Tokens per Sec:     8421, Lr: 0.000500
2021-12-19 08:10:31,625 - INFO - joeynmt.training - Epoch   4, Step:    79000, Batch Loss:     1.645295, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 08:11:33,647 - INFO - joeynmt.training - Epoch   4, Step:    79200, Batch Loss:     1.759870, Tokens per Sec:     8258, Lr: 0.000500
2021-12-19 08:12:35,817 - INFO - joeynmt.training - Epoch   4, Step:    79400, Batch Loss:     1.602783, Tokens per Sec:     8378, Lr: 0.000500
2021-12-19 08:13:37,757 - INFO - joeynmt.training - Epoch   4, Step:    79600, Batch Loss:     1.616187, Tokens per Sec:     8308, Lr: 0.000500
2021-12-19 08:14:39,955 - INFO - joeynmt.training - Epoch   4, Step:    79800, Batch Loss:     1.709748, Tokens per Sec:     8384, Lr: 0.000500
2021-12-19 08:15:42,189 - INFO - joeynmt.training - Epoch   4, Step:    80000, Batch Loss:     1.440127, Tokens per Sec:     8360, Lr: 0.000500
2021-12-19 08:18:12,783 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 08:18:12,783 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 08:18:15,192 - INFO - joeynmt.training - Example #0
2021-12-19 08:18:15,193 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 08:18:15,193 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'O@@', 'ba@@', '.']
2021-12-19 08:18:15,193 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 08:18:15,193 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 08:18:15,193 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the reelection of Oba.
2021-12-19 08:18:15,193 - INFO - joeynmt.training - Example #1
2021-12-19 08:18:15,193 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 08:18:15,194 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'fight', 'electoral', 'fraud.']
2021-12-19 08:18:15,194 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 08:18:15,194 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 08:18:15,194 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to fight electoral fraud.
2021-12-19 08:18:15,194 - INFO - joeynmt.training - Example #2
2021-12-19 08:18:15,194 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 08:18:15,194 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'is', 'now', 'looking', 'at', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'pe.']
2021-12-19 08:18:15,194 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 08:18:15,195 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 08:18:15,195 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre is now looking at this last year as a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the rape.
2021-12-19 08:18:15,195 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    80000: bleu:  16.25, loss: 197809.2031, ppl:  12.5708, duration: 153.0059s
2021-12-19 08:19:17,492 - INFO - joeynmt.training - Epoch   4, Step:    80200, Batch Loss:     1.715763, Tokens per Sec:     8329, Lr: 0.000500
2021-12-19 08:20:19,504 - INFO - joeynmt.training - Epoch   4, Step:    80400, Batch Loss:     1.551469, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 08:21:21,527 - INFO - joeynmt.training - Epoch   4, Step:    80600, Batch Loss:     1.535781, Tokens per Sec:     8380, Lr: 0.000500
2021-12-19 08:22:23,179 - INFO - joeynmt.training - Epoch   4, Step:    80800, Batch Loss:     1.491265, Tokens per Sec:     8328, Lr: 0.000500
2021-12-19 08:23:24,759 - INFO - joeynmt.training - Epoch   4, Step:    81000, Batch Loss:     1.658417, Tokens per Sec:     8266, Lr: 0.000500
2021-12-19 08:24:26,234 - INFO - joeynmt.training - Epoch   4, Step:    81200, Batch Loss:     1.826456, Tokens per Sec:     8396, Lr: 0.000500
2021-12-19 08:25:28,345 - INFO - joeynmt.training - Epoch   4, Step:    81400, Batch Loss:     1.602475, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 08:26:30,562 - INFO - joeynmt.training - Epoch   4, Step:    81600, Batch Loss:     1.432323, Tokens per Sec:     8310, Lr: 0.000500
2021-12-19 08:27:32,320 - INFO - joeynmt.training - Epoch   4, Step:    81800, Batch Loss:     1.614561, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 08:28:34,216 - INFO - joeynmt.training - Epoch   4, Step:    82000, Batch Loss:     1.762459, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 08:29:36,413 - INFO - joeynmt.training - Epoch   4, Step:    82200, Batch Loss:     1.758951, Tokens per Sec:     8428, Lr: 0.000500
2021-12-19 08:30:37,945 - INFO - joeynmt.training - Epoch   4, Step:    82400, Batch Loss:     1.797359, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 08:31:39,727 - INFO - joeynmt.training - Epoch   4, Step:    82600, Batch Loss:     1.880760, Tokens per Sec:     8263, Lr: 0.000500
2021-12-19 08:32:41,511 - INFO - joeynmt.training - Epoch   4, Step:    82800, Batch Loss:     1.676356, Tokens per Sec:     8308, Lr: 0.000500
2021-12-19 08:33:43,581 - INFO - joeynmt.training - Epoch   4, Step:    83000, Batch Loss:     1.734626, Tokens per Sec:     8444, Lr: 0.000500
2021-12-19 08:34:45,094 - INFO - joeynmt.training - Epoch   4, Step:    83200, Batch Loss:     1.725031, Tokens per Sec:     8276, Lr: 0.000500
2021-12-19 08:35:47,047 - INFO - joeynmt.training - Epoch   4, Step:    83400, Batch Loss:     1.663980, Tokens per Sec:     8418, Lr: 0.000500
2021-12-19 08:36:49,329 - INFO - joeynmt.training - Epoch   4, Step:    83600, Batch Loss:     1.735232, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 08:37:51,403 - INFO - joeynmt.training - Epoch   4, Step:    83800, Batch Loss:     1.686172, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 08:38:52,825 - INFO - joeynmt.training - Epoch   4, Step:    84000, Batch Loss:     1.798182, Tokens per Sec:     8307, Lr: 0.000500
2021-12-19 08:39:54,431 - INFO - joeynmt.training - Epoch   4, Step:    84200, Batch Loss:     1.786499, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 08:40:55,631 - INFO - joeynmt.training - Epoch   4, Step:    84400, Batch Loss:     1.790110, Tokens per Sec:     8316, Lr: 0.000500
2021-12-19 08:41:56,951 - INFO - joeynmt.training - Epoch   4, Step:    84600, Batch Loss:     2.091434, Tokens per Sec:     8348, Lr: 0.000500
2021-12-19 08:42:59,172 - INFO - joeynmt.training - Epoch   4, Step:    84800, Batch Loss:     1.502749, Tokens per Sec:     8331, Lr: 0.000500
2021-12-19 08:44:01,364 - INFO - joeynmt.training - Epoch   4, Step:    85000, Batch Loss:     1.698002, Tokens per Sec:     8326, Lr: 0.000500
2021-12-19 08:46:28,355 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 08:46:28,355 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 08:46:30,879 - INFO - joeynmt.training - Example #0
2021-12-19 08:46:30,880 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 08:46:30,880 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 08:46:30,880 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 08:46:30,880 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 08:46:30,880 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 08:46:30,880 - INFO - joeynmt.training - Example #1
2021-12-19 08:46:30,880 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 08:46:30,881 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'cl@@', 'y-@@', 'minded', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 08:46:30,881 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 08:46:30,881 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 08:46:30,881 - INFO - joeynmt.training - 	Hypothesis: Republicly-minded leaders justify their policy by the need to combat electoral fraud.
2021-12-19 08:46:30,881 - INFO - joeynmt.training - Example #2
2021-12-19 08:46:30,881 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 08:46:30,881 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'is', 'now', 'looking', 'at', 'this', 'last', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'in.']
2021-12-19 08:46:30,882 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 08:46:30,882 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 08:46:30,882 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre is now looking at this last as a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the rain.
2021-12-19 08:46:30,882 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step    85000: bleu:  16.17, loss: 195603.1094, ppl:  12.2208, duration: 149.5175s
2021-12-19 08:47:33,373 - INFO - joeynmt.training - Epoch   4, Step:    85200, Batch Loss:     1.851701, Tokens per Sec:     8322, Lr: 0.000500
2021-12-19 08:48:35,040 - INFO - joeynmt.training - Epoch   4, Step:    85400, Batch Loss:     1.581791, Tokens per Sec:     8272, Lr: 0.000500
2021-12-19 08:49:37,290 - INFO - joeynmt.training - Epoch   4, Step:    85600, Batch Loss:     1.752499, Tokens per Sec:     8433, Lr: 0.000500
2021-12-19 08:50:39,092 - INFO - joeynmt.training - Epoch   4, Step:    85800, Batch Loss:     1.642300, Tokens per Sec:     8270, Lr: 0.000500
2021-12-19 08:51:41,069 - INFO - joeynmt.training - Epoch   4, Step:    86000, Batch Loss:     1.739604, Tokens per Sec:     8405, Lr: 0.000500
2021-12-19 08:52:07,973 - INFO - joeynmt.training - Epoch   4: total training loss 37260.27
2021-12-19 08:52:07,974 - INFO - joeynmt.training - EPOCH 5
2021-12-19 08:52:46,665 - INFO - joeynmt.training - Epoch   5, Step:    86200, Batch Loss:     1.634169, Tokens per Sec:     7622, Lr: 0.000500
2021-12-19 08:53:48,854 - INFO - joeynmt.training - Epoch   5, Step:    86400, Batch Loss:     1.770560, Tokens per Sec:     8378, Lr: 0.000500
2021-12-19 08:54:51,153 - INFO - joeynmt.training - Epoch   5, Step:    86600, Batch Loss:     1.215778, Tokens per Sec:     8434, Lr: 0.000500
2021-12-19 08:55:53,053 - INFO - joeynmt.training - Epoch   5, Step:    86800, Batch Loss:     1.735292, Tokens per Sec:     8226, Lr: 0.000500
2021-12-19 08:56:55,236 - INFO - joeynmt.training - Epoch   5, Step:    87000, Batch Loss:     1.574712, Tokens per Sec:     8449, Lr: 0.000500
2021-12-19 08:57:57,668 - INFO - joeynmt.training - Epoch   5, Step:    87200, Batch Loss:     1.981856, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 08:58:58,924 - INFO - joeynmt.training - Epoch   5, Step:    87400, Batch Loss:     1.621359, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 09:00:01,203 - INFO - joeynmt.training - Epoch   5, Step:    87600, Batch Loss:     1.631311, Tokens per Sec:     8377, Lr: 0.000500
2021-12-19 09:01:03,334 - INFO - joeynmt.training - Epoch   5, Step:    87800, Batch Loss:     1.780395, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 09:02:05,490 - INFO - joeynmt.training - Epoch   5, Step:    88000, Batch Loss:     1.585558, Tokens per Sec:     8385, Lr: 0.000500
2021-12-19 09:03:07,530 - INFO - joeynmt.training - Epoch   5, Step:    88200, Batch Loss:     1.581947, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 09:04:09,782 - INFO - joeynmt.training - Epoch   5, Step:    88400, Batch Loss:     1.758609, Tokens per Sec:     8399, Lr: 0.000500
2021-12-19 09:05:12,104 - INFO - joeynmt.training - Epoch   5, Step:    88600, Batch Loss:     1.389910, Tokens per Sec:     8440, Lr: 0.000500
2021-12-19 09:06:13,722 - INFO - joeynmt.training - Epoch   5, Step:    88800, Batch Loss:     1.716150, Tokens per Sec:     8297, Lr: 0.000500
2021-12-19 09:07:15,821 - INFO - joeynmt.training - Epoch   5, Step:    89000, Batch Loss:     1.868432, Tokens per Sec:     8448, Lr: 0.000500
2021-12-19 09:08:17,999 - INFO - joeynmt.training - Epoch   5, Step:    89200, Batch Loss:     2.227185, Tokens per Sec:     8477, Lr: 0.000500
2021-12-19 09:09:20,340 - INFO - joeynmt.training - Epoch   5, Step:    89400, Batch Loss:     1.670038, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 09:10:22,015 - INFO - joeynmt.training - Epoch   5, Step:    89600, Batch Loss:     1.602413, Tokens per Sec:     8394, Lr: 0.000500
2021-12-19 09:11:24,610 - INFO - joeynmt.training - Epoch   5, Step:    89800, Batch Loss:     1.636544, Tokens per Sec:     8445, Lr: 0.000500
2021-12-19 09:12:26,654 - INFO - joeynmt.training - Epoch   5, Step:    90000, Batch Loss:     1.588032, Tokens per Sec:     8378, Lr: 0.000500
2021-12-19 09:14:52,300 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 09:14:52,301 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 09:14:54,910 - INFO - joeynmt.training - Example #0
2021-12-19 09:14:54,910 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 09:14:54,910 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter@@', 'act', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 09:14:54,910 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 09:14:54,911 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 09:14:54,911 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counteract the re-election of Obama
2021-12-19 09:14:54,911 - INFO - joeynmt.training - Example #1
2021-12-19 09:14:54,911 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 09:14:54,911 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 09:14:54,911 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 09:14:54,911 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 09:14:54,912 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 09:14:54,912 - INFO - joeynmt.training - Example #2
2021-12-19 09:14:54,912 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 09:14:54,912 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'is', 'now', 'looking', 'at', 'this', 'last', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'mass', 'fi@@', 're.']
2021-12-19 09:14:54,912 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 09:14:54,912 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 09:14:54,912 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre is now looking at this last as a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the mass fire.
2021-12-19 09:14:54,913 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    90000: bleu:  16.36, loss: 195148.7812, ppl:  12.1500, duration: 148.2585s
2021-12-19 09:15:57,214 - INFO - joeynmt.training - Epoch   5, Step:    90200, Batch Loss:     1.777706, Tokens per Sec:     8379, Lr: 0.000500
2021-12-19 09:16:59,392 - INFO - joeynmt.training - Epoch   5, Step:    90400, Batch Loss:     1.411338, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 09:18:01,209 - INFO - joeynmt.training - Epoch   5, Step:    90600, Batch Loss:     1.543443, Tokens per Sec:     8257, Lr: 0.000500
2021-12-19 09:19:02,985 - INFO - joeynmt.training - Epoch   5, Step:    90800, Batch Loss:     1.785548, Tokens per Sec:     8315, Lr: 0.000500
2021-12-19 09:20:04,899 - INFO - joeynmt.training - Epoch   5, Step:    91000, Batch Loss:     1.667200, Tokens per Sec:     8375, Lr: 0.000500
2021-12-19 09:21:06,681 - INFO - joeynmt.training - Epoch   5, Step:    91200, Batch Loss:     1.885104, Tokens per Sec:     8394, Lr: 0.000500
2021-12-19 09:22:08,715 - INFO - joeynmt.training - Epoch   5, Step:    91400, Batch Loss:     1.782578, Tokens per Sec:     8314, Lr: 0.000500
2021-12-19 09:23:11,095 - INFO - joeynmt.training - Epoch   5, Step:    91600, Batch Loss:     1.746445, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 09:24:13,136 - INFO - joeynmt.training - Epoch   5, Step:    91800, Batch Loss:     1.826026, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 09:25:15,565 - INFO - joeynmt.training - Epoch   5, Step:    92000, Batch Loss:     1.522630, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 09:26:17,526 - INFO - joeynmt.training - Epoch   5, Step:    92200, Batch Loss:     1.370920, Tokens per Sec:     8277, Lr: 0.000500
2021-12-19 09:27:19,732 - INFO - joeynmt.training - Epoch   5, Step:    92400, Batch Loss:     1.734039, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 09:28:21,044 - INFO - joeynmt.training - Epoch   5, Step:    92600, Batch Loss:     1.786892, Tokens per Sec:     8319, Lr: 0.000500
2021-12-19 09:29:22,672 - INFO - joeynmt.training - Epoch   5, Step:    92800, Batch Loss:     1.785370, Tokens per Sec:     8371, Lr: 0.000500
2021-12-19 09:30:24,711 - INFO - joeynmt.training - Epoch   5, Step:    93000, Batch Loss:     1.676071, Tokens per Sec:     8383, Lr: 0.000500
2021-12-19 09:31:26,610 - INFO - joeynmt.training - Epoch   5, Step:    93200, Batch Loss:     1.634046, Tokens per Sec:     8343, Lr: 0.000500
2021-12-19 09:32:28,774 - INFO - joeynmt.training - Epoch   5, Step:    93400, Batch Loss:     1.953307, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 09:33:30,422 - INFO - joeynmt.training - Epoch   5, Step:    93600, Batch Loss:     1.801804, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 09:34:32,505 - INFO - joeynmt.training - Epoch   5, Step:    93800, Batch Loss:     1.670009, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 09:35:35,104 - INFO - joeynmt.training - Epoch   5, Step:    94000, Batch Loss:     1.711314, Tokens per Sec:     8468, Lr: 0.000500
2021-12-19 09:36:37,062 - INFO - joeynmt.training - Epoch   5, Step:    94200, Batch Loss:     1.494190, Tokens per Sec:     8240, Lr: 0.000500
2021-12-19 09:37:39,124 - INFO - joeynmt.training - Epoch   5, Step:    94400, Batch Loss:     1.558405, Tokens per Sec:     8387, Lr: 0.000500
2021-12-19 09:38:40,966 - INFO - joeynmt.training - Epoch   5, Step:    94600, Batch Loss:     1.584031, Tokens per Sec:     8358, Lr: 0.000500
2021-12-19 09:39:42,754 - INFO - joeynmt.training - Epoch   5, Step:    94800, Batch Loss:     1.611767, Tokens per Sec:     8363, Lr: 0.000500
2021-12-19 09:40:44,118 - INFO - joeynmt.training - Epoch   5, Step:    95000, Batch Loss:     1.610636, Tokens per Sec:     8371, Lr: 0.000500
2021-12-19 09:43:09,873 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 09:43:09,873 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 09:43:12,477 - INFO - joeynmt.training - Example #0
2021-12-19 09:43:12,478 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 09:43:12,478 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 09:43:12,478 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 09:43:12,478 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 09:43:12,478 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 09:43:12,478 - INFO - joeynmt.training - Example #1
2021-12-19 09:43:12,479 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 09:43:12,479 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 're@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 09:43:12,479 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 09:43:12,479 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 09:43:12,479 - INFO - joeynmt.training - 	Hypothesis: Republican republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 09:43:12,479 - INFO - joeynmt.training - Example #2
2021-12-19 09:43:12,479 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 09:43:12,479 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'year', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ra@@', 'in.']
2021-12-19 09:43:12,480 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 09:43:12,480 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 09:43:12,480 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last year to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the rain.
2021-12-19 09:43:12,480 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step    95000: bleu:  16.44, loss: 193420.2500, ppl:  11.8842, duration: 148.3617s
2021-12-19 09:44:14,970 - INFO - joeynmt.training - Epoch   5, Step:    95200, Batch Loss:     1.677829, Tokens per Sec:     8309, Lr: 0.000500
2021-12-19 09:45:17,370 - INFO - joeynmt.training - Epoch   5, Step:    95400, Batch Loss:     1.728132, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 09:46:19,289 - INFO - joeynmt.training - Epoch   5, Step:    95600, Batch Loss:     1.716547, Tokens per Sec:     8285, Lr: 0.000500
2021-12-19 09:47:21,395 - INFO - joeynmt.training - Epoch   5, Step:    95800, Batch Loss:     1.728373, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 09:48:23,569 - INFO - joeynmt.training - Epoch   5, Step:    96000, Batch Loss:     1.462174, Tokens per Sec:     8285, Lr: 0.000500
2021-12-19 09:49:25,907 - INFO - joeynmt.training - Epoch   5, Step:    96200, Batch Loss:     1.691666, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 09:50:28,036 - INFO - joeynmt.training - Epoch   5, Step:    96400, Batch Loss:     1.733104, Tokens per Sec:     8306, Lr: 0.000500
2021-12-19 09:51:29,767 - INFO - joeynmt.training - Epoch   5, Step:    96600, Batch Loss:     1.775133, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 09:52:32,063 - INFO - joeynmt.training - Epoch   5, Step:    96800, Batch Loss:     1.904243, Tokens per Sec:     8404, Lr: 0.000500
2021-12-19 09:53:34,499 - INFO - joeynmt.training - Epoch   5, Step:    97000, Batch Loss:     1.882743, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 09:54:36,644 - INFO - joeynmt.training - Epoch   5, Step:    97200, Batch Loss:     1.662892, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 09:55:37,665 - INFO - joeynmt.training - Epoch   5, Step:    97400, Batch Loss:     2.182448, Tokens per Sec:     8323, Lr: 0.000500
2021-12-19 09:56:40,093 - INFO - joeynmt.training - Epoch   5, Step:    97600, Batch Loss:     1.667749, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 09:57:42,600 - INFO - joeynmt.training - Epoch   5, Step:    97800, Batch Loss:     1.645373, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 09:58:44,707 - INFO - joeynmt.training - Epoch   5, Step:    98000, Batch Loss:     1.646566, Tokens per Sec:     8304, Lr: 0.000500
2021-12-19 09:59:46,732 - INFO - joeynmt.training - Epoch   5, Step:    98200, Batch Loss:     1.771184, Tokens per Sec:     8341, Lr: 0.000500
2021-12-19 10:00:48,869 - INFO - joeynmt.training - Epoch   5, Step:    98400, Batch Loss:     1.719268, Tokens per Sec:     8283, Lr: 0.000500
2021-12-19 10:01:50,698 - INFO - joeynmt.training - Epoch   5, Step:    98600, Batch Loss:     1.584290, Tokens per Sec:     8367, Lr: 0.000500
2021-12-19 10:02:53,029 - INFO - joeynmt.training - Epoch   5, Step:    98800, Batch Loss:     1.654869, Tokens per Sec:     8378, Lr: 0.000500
2021-12-19 10:03:54,747 - INFO - joeynmt.training - Epoch   5, Step:    99000, Batch Loss:     1.637325, Tokens per Sec:     8338, Lr: 0.000500
2021-12-19 10:04:56,924 - INFO - joeynmt.training - Epoch   5, Step:    99200, Batch Loss:     1.977755, Tokens per Sec:     8324, Lr: 0.000500
2021-12-19 10:05:58,432 - INFO - joeynmt.training - Epoch   5, Step:    99400, Batch Loss:     1.732112, Tokens per Sec:     8357, Lr: 0.000500
2021-12-19 10:07:00,440 - INFO - joeynmt.training - Epoch   5, Step:    99600, Batch Loss:     1.756750, Tokens per Sec:     8380, Lr: 0.000500
2021-12-19 10:08:02,542 - INFO - joeynmt.training - Epoch   5, Step:    99800, Batch Loss:     1.570859, Tokens per Sec:     8379, Lr: 0.000500
2021-12-19 10:09:04,227 - INFO - joeynmt.training - Epoch   5, Step:   100000, Batch Loss:     1.511273, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 10:11:33,357 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 10:11:33,357 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 10:11:38,074 - INFO - joeynmt.training - Example #0
2021-12-19 10:11:38,074 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 10:11:38,074 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'O@@', 'ba@@', '.']
2021-12-19 10:11:38,075 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 10:11:38,075 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 10:11:38,075 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Oba.
2021-12-19 10:11:38,075 - INFO - joeynmt.training - Example #1
2021-12-19 10:11:38,075 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 10:11:38,075 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'fight', 'electoral', 'fraud.']
2021-12-19 10:11:38,075 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 10:11:38,076 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 10:11:38,076 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to fight electoral fraud.
2021-12-19 10:11:38,076 - INFO - joeynmt.training - Example #2
2021-12-19 10:11:38,076 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 10:11:38,076 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'stor@@', 'y.']
2021-12-19 10:11:38,076 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 10:11:38,076 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 10:11:38,077 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the story.
2021-12-19 10:11:38,077 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   100000: bleu:  16.82, loss: 192112.9375, ppl:  11.6870, duration: 153.8497s
2021-12-19 10:12:39,930 - INFO - joeynmt.training - Epoch   5, Step:   100200, Batch Loss:     2.010566, Tokens per Sec:     8279, Lr: 0.000500
2021-12-19 10:13:41,986 - INFO - joeynmt.training - Epoch   5, Step:   100400, Batch Loss:     1.386318, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 10:14:43,821 - INFO - joeynmt.training - Epoch   5, Step:   100600, Batch Loss:     1.607344, Tokens per Sec:     8296, Lr: 0.000500
2021-12-19 10:15:45,853 - INFO - joeynmt.training - Epoch   5, Step:   100800, Batch Loss:     1.753971, Tokens per Sec:     8264, Lr: 0.000500
2021-12-19 10:16:47,672 - INFO - joeynmt.training - Epoch   5, Step:   101000, Batch Loss:     1.756725, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 10:17:50,265 - INFO - joeynmt.training - Epoch   5, Step:   101200, Batch Loss:     1.719037, Tokens per Sec:     8457, Lr: 0.000500
2021-12-19 10:18:52,137 - INFO - joeynmt.training - Epoch   5, Step:   101400, Batch Loss:     1.644857, Tokens per Sec:     8420, Lr: 0.000500
2021-12-19 10:19:54,001 - INFO - joeynmt.training - Epoch   5, Step:   101600, Batch Loss:     1.624666, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 10:20:56,507 - INFO - joeynmt.training - Epoch   5, Step:   101800, Batch Loss:     1.773568, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 10:21:57,659 - INFO - joeynmt.training - Epoch   5, Step:   102000, Batch Loss:     1.774368, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 10:22:59,547 - INFO - joeynmt.training - Epoch   5, Step:   102200, Batch Loss:     1.681760, Tokens per Sec:     8380, Lr: 0.000500
2021-12-19 10:24:01,284 - INFO - joeynmt.training - Epoch   5, Step:   102400, Batch Loss:     1.737466, Tokens per Sec:     8417, Lr: 0.000500
2021-12-19 10:25:02,873 - INFO - joeynmt.training - Epoch   5, Step:   102600, Batch Loss:     1.809087, Tokens per Sec:     8398, Lr: 0.000500
2021-12-19 10:26:05,101 - INFO - joeynmt.training - Epoch   5, Step:   102800, Batch Loss:     1.651244, Tokens per Sec:     8380, Lr: 0.000500
2021-12-19 10:27:07,112 - INFO - joeynmt.training - Epoch   5, Step:   103000, Batch Loss:     1.570567, Tokens per Sec:     8304, Lr: 0.000500
2021-12-19 10:28:08,999 - INFO - joeynmt.training - Epoch   5, Step:   103200, Batch Loss:     1.541828, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 10:29:10,868 - INFO - joeynmt.training - Epoch   5, Step:   103400, Batch Loss:     1.711822, Tokens per Sec:     8287, Lr: 0.000500
2021-12-19 10:30:12,689 - INFO - joeynmt.training - Epoch   5, Step:   103600, Batch Loss:     1.892922, Tokens per Sec:     8249, Lr: 0.000500
2021-12-19 10:31:14,775 - INFO - joeynmt.training - Epoch   5, Step:   103800, Batch Loss:     1.796648, Tokens per Sec:     8435, Lr: 0.000500
2021-12-19 10:32:17,328 - INFO - joeynmt.training - Epoch   5, Step:   104000, Batch Loss:     1.667114, Tokens per Sec:     8453, Lr: 0.000500
2021-12-19 10:33:19,551 - INFO - joeynmt.training - Epoch   5, Step:   104200, Batch Loss:     1.801204, Tokens per Sec:     8363, Lr: 0.000500
2021-12-19 10:34:20,999 - INFO - joeynmt.training - Epoch   5, Step:   104400, Batch Loss:     1.509075, Tokens per Sec:     8232, Lr: 0.000500
2021-12-19 10:35:23,361 - INFO - joeynmt.training - Epoch   5, Step:   104600, Batch Loss:     1.536596, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 10:36:25,405 - INFO - joeynmt.training - Epoch   5, Step:   104800, Batch Loss:     1.585998, Tokens per Sec:     8432, Lr: 0.000500
2021-12-19 10:37:28,016 - INFO - joeynmt.training - Epoch   5, Step:   105000, Batch Loss:     1.860361, Tokens per Sec:     8441, Lr: 0.000500
2021-12-19 10:39:56,840 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 10:39:56,840 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 10:40:01,433 - INFO - joeynmt.training - Example #0
2021-12-19 10:40:01,433 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 10:40:01,433 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 10:40:01,433 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 10:40:01,434 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 10:40:01,434 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 10:40:01,434 - INFO - joeynmt.training - Example #1
2021-12-19 10:40:01,434 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 10:40:01,435 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 10:40:01,435 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 10:40:01,435 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 10:40:01,435 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 10:40:01,435 - INFO - joeynmt.training - Example #2
2021-12-19 10:40:01,436 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 10:40:01,436 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'year', 'as', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 10:40:01,436 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 10:40:01,436 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 10:40:01,437 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last year as a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 10:40:01,437 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step   105000: bleu:  16.98, loss: 190616.9375, ppl:  11.4654, duration: 153.4207s
2021-12-19 10:41:03,952 - INFO - joeynmt.training - Epoch   5, Step:   105200, Batch Loss:     1.862298, Tokens per Sec:     8399, Lr: 0.000500
2021-12-19 10:42:06,168 - INFO - joeynmt.training - Epoch   5, Step:   105400, Batch Loss:     1.643568, Tokens per Sec:     8407, Lr: 0.000500
2021-12-19 10:43:08,070 - INFO - joeynmt.training - Epoch   5, Step:   105600, Batch Loss:     1.750503, Tokens per Sec:     8391, Lr: 0.000500
2021-12-19 10:44:10,202 - INFO - joeynmt.training - Epoch   5, Step:   105800, Batch Loss:     1.630799, Tokens per Sec:     8426, Lr: 0.000500
2021-12-19 10:45:12,980 - INFO - joeynmt.training - Epoch   5, Step:   106000, Batch Loss:     1.896143, Tokens per Sec:     8427, Lr: 0.000500
2021-12-19 10:46:15,496 - INFO - joeynmt.training - Epoch   5, Step:   106200, Batch Loss:     1.661153, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 10:47:16,990 - INFO - joeynmt.training - Epoch   5, Step:   106400, Batch Loss:     1.581714, Tokens per Sec:     8272, Lr: 0.000500
2021-12-19 10:48:19,082 - INFO - joeynmt.training - Epoch   5, Step:   106600, Batch Loss:     1.619477, Tokens per Sec:     8386, Lr: 0.000500
2021-12-19 10:49:21,100 - INFO - joeynmt.training - Epoch   5, Step:   106800, Batch Loss:     1.614046, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 10:50:22,886 - INFO - joeynmt.training - Epoch   5, Step:   107000, Batch Loss:     1.561473, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 10:51:24,538 - INFO - joeynmt.training - Epoch   5, Step:   107200, Batch Loss:     1.924153, Tokens per Sec:     8226, Lr: 0.000500
2021-12-19 10:52:26,817 - INFO - joeynmt.training - Epoch   5, Step:   107400, Batch Loss:     1.911316, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 10:53:25,103 - INFO - joeynmt.training - Epoch   5: total training loss 35825.82
2021-12-19 10:53:25,103 - INFO - joeynmt.training - EPOCH 6
2021-12-19 10:53:32,246 - INFO - joeynmt.training - Epoch   6, Step:   107600, Batch Loss:     1.678412, Tokens per Sec:     4497, Lr: 0.000500
2021-12-19 10:54:34,072 - INFO - joeynmt.training - Epoch   6, Step:   107800, Batch Loss:     1.607361, Tokens per Sec:     8363, Lr: 0.000500
2021-12-19 10:55:35,944 - INFO - joeynmt.training - Epoch   6, Step:   108000, Batch Loss:     1.625449, Tokens per Sec:     8425, Lr: 0.000500
2021-12-19 10:56:38,227 - INFO - joeynmt.training - Epoch   6, Step:   108200, Batch Loss:     1.420234, Tokens per Sec:     8309, Lr: 0.000500
2021-12-19 10:57:39,783 - INFO - joeynmt.training - Epoch   6, Step:   108400, Batch Loss:     1.356852, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 10:58:41,287 - INFO - joeynmt.training - Epoch   6, Step:   108600, Batch Loss:     1.491297, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 10:59:43,425 - INFO - joeynmt.training - Epoch   6, Step:   108800, Batch Loss:     1.812718, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 11:00:45,085 - INFO - joeynmt.training - Epoch   6, Step:   109000, Batch Loss:     1.823455, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 11:01:47,001 - INFO - joeynmt.training - Epoch   6, Step:   109200, Batch Loss:     1.798544, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 11:02:48,687 - INFO - joeynmt.training - Epoch   6, Step:   109400, Batch Loss:     1.771517, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 11:03:50,283 - INFO - joeynmt.training - Epoch   6, Step:   109600, Batch Loss:     1.337405, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 11:04:52,592 - INFO - joeynmt.training - Epoch   6, Step:   109800, Batch Loss:     1.608814, Tokens per Sec:     8482, Lr: 0.000500
2021-12-19 11:05:54,769 - INFO - joeynmt.training - Epoch   6, Step:   110000, Batch Loss:     1.710883, Tokens per Sec:     8508, Lr: 0.000500
2021-12-19 11:08:19,851 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 11:08:19,851 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 11:08:22,339 - INFO - joeynmt.training - Example #0
2021-12-19 11:08:22,340 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 11:08:22,340 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 11:08:22,340 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 11:08:22,340 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 11:08:22,340 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 11:08:22,340 - INFO - joeynmt.training - Example #1
2021-12-19 11:08:22,341 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 11:08:22,341 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 11:08:22,341 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 11:08:22,341 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 11:08:22,341 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 11:08:22,341 - INFO - joeynmt.training - Example #2
2021-12-19 11:08:22,341 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 11:08:22,342 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'latter', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'fi@@', 're.']
2021-12-19 11:08:22,342 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 11:08:22,342 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 11:08:22,342 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this latter to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the fire.
2021-12-19 11:08:22,342 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   110000: bleu:  17.01, loss: 189695.0781, ppl:  11.3309, duration: 147.5721s
2021-12-19 11:09:24,092 - INFO - joeynmt.training - Epoch   6, Step:   110200, Batch Loss:     1.845394, Tokens per Sec:     8264, Lr: 0.000500
2021-12-19 11:10:26,152 - INFO - joeynmt.training - Epoch   6, Step:   110400, Batch Loss:     1.816674, Tokens per Sec:     8463, Lr: 0.000500
2021-12-19 11:11:27,958 - INFO - joeynmt.training - Epoch   6, Step:   110600, Batch Loss:     1.575677, Tokens per Sec:     8309, Lr: 0.000500
2021-12-19 11:12:30,031 - INFO - joeynmt.training - Epoch   6, Step:   110800, Batch Loss:     1.497600, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 11:13:31,822 - INFO - joeynmt.training - Epoch   6, Step:   111000, Batch Loss:     1.770485, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 11:14:33,455 - INFO - joeynmt.training - Epoch   6, Step:   111200, Batch Loss:     1.638480, Tokens per Sec:     8346, Lr: 0.000500
2021-12-19 11:15:35,449 - INFO - joeynmt.training - Epoch   6, Step:   111400, Batch Loss:     1.487437, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 11:16:36,916 - INFO - joeynmt.training - Epoch   6, Step:   111600, Batch Loss:     1.402078, Tokens per Sec:     8287, Lr: 0.000500
2021-12-19 11:17:38,855 - INFO - joeynmt.training - Epoch   6, Step:   111800, Batch Loss:     1.490567, Tokens per Sec:     8335, Lr: 0.000500
2021-12-19 11:18:41,001 - INFO - joeynmt.training - Epoch   6, Step:   112000, Batch Loss:     1.682113, Tokens per Sec:     8451, Lr: 0.000500
2021-12-19 11:19:43,416 - INFO - joeynmt.training - Epoch   6, Step:   112200, Batch Loss:     1.658635, Tokens per Sec:     8432, Lr: 0.000500
2021-12-19 11:20:45,592 - INFO - joeynmt.training - Epoch   6, Step:   112400, Batch Loss:     1.585580, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 11:21:48,084 - INFO - joeynmt.training - Epoch   6, Step:   112600, Batch Loss:     1.702700, Tokens per Sec:     8389, Lr: 0.000500
2021-12-19 11:22:50,532 - INFO - joeynmt.training - Epoch   6, Step:   112800, Batch Loss:     1.638924, Tokens per Sec:     8398, Lr: 0.000500
2021-12-19 11:23:52,718 - INFO - joeynmt.training - Epoch   6, Step:   113000, Batch Loss:     1.450670, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 11:24:54,752 - INFO - joeynmt.training - Epoch   6, Step:   113200, Batch Loss:     1.515433, Tokens per Sec:     8431, Lr: 0.000500
2021-12-19 11:25:56,573 - INFO - joeynmt.training - Epoch   6, Step:   113400, Batch Loss:     1.810717, Tokens per Sec:     8288, Lr: 0.000500
2021-12-19 11:26:58,585 - INFO - joeynmt.training - Epoch   6, Step:   113600, Batch Loss:     1.920982, Tokens per Sec:     8272, Lr: 0.000500
2021-12-19 11:28:00,421 - INFO - joeynmt.training - Epoch   6, Step:   113800, Batch Loss:     1.739709, Tokens per Sec:     8359, Lr: 0.000500
2021-12-19 11:29:02,643 - INFO - joeynmt.training - Epoch   6, Step:   114000, Batch Loss:     1.521816, Tokens per Sec:     8387, Lr: 0.000500
2021-12-19 11:30:04,716 - INFO - joeynmt.training - Epoch   6, Step:   114200, Batch Loss:     1.635750, Tokens per Sec:     8260, Lr: 0.000500
2021-12-19 11:31:06,555 - INFO - joeynmt.training - Epoch   6, Step:   114400, Batch Loss:     1.043427, Tokens per Sec:     8326, Lr: 0.000500
2021-12-19 11:32:08,213 - INFO - joeynmt.training - Epoch   6, Step:   114600, Batch Loss:     1.647226, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 11:33:10,341 - INFO - joeynmt.training - Epoch   6, Step:   114800, Batch Loss:     1.811830, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 11:34:12,293 - INFO - joeynmt.training - Epoch   6, Step:   115000, Batch Loss:     1.801034, Tokens per Sec:     8419, Lr: 0.000500
2021-12-19 11:36:38,104 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 11:36:38,104 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 11:36:40,516 - INFO - joeynmt.training - Example #0
2021-12-19 11:36:40,517 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 11:36:40,517 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'O@@', 'ba@@', '.']
2021-12-19 11:36:40,517 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 11:36:40,517 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 11:36:40,518 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Oba.
2021-12-19 11:36:40,518 - INFO - joeynmt.training - Example #1
2021-12-19 11:36:40,518 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 11:36:40,518 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 11:36:40,518 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 11:36:40,519 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 11:36:40,519 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 11:36:40,519 - INFO - joeynmt.training - Example #2
2021-12-19 11:36:40,519 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 11:36:40,520 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'stor@@', 'y.']
2021-12-19 11:36:40,520 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 11:36:40,520 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 11:36:40,520 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the story.
2021-12-19 11:36:40,521 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   115000: bleu:  17.21, loss: 188182.0781, ppl:  11.1137, duration: 148.2266s
2021-12-19 11:37:42,090 - INFO - joeynmt.training - Epoch   6, Step:   115200, Batch Loss:     1.598992, Tokens per Sec:     8279, Lr: 0.000500
2021-12-19 11:38:43,883 - INFO - joeynmt.training - Epoch   6, Step:   115400, Batch Loss:     1.630686, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 11:39:45,930 - INFO - joeynmt.training - Epoch   6, Step:   115600, Batch Loss:     1.415334, Tokens per Sec:     8310, Lr: 0.000500
2021-12-19 11:40:47,721 - INFO - joeynmt.training - Epoch   6, Step:   115800, Batch Loss:     1.650803, Tokens per Sec:     8317, Lr: 0.000500
2021-12-19 11:41:50,004 - INFO - joeynmt.training - Epoch   6, Step:   116000, Batch Loss:     1.467999, Tokens per Sec:     8310, Lr: 0.000500
2021-12-19 11:42:52,407 - INFO - joeynmt.training - Epoch   6, Step:   116200, Batch Loss:     1.547204, Tokens per Sec:     8429, Lr: 0.000500
2021-12-19 11:43:54,501 - INFO - joeynmt.training - Epoch   6, Step:   116400, Batch Loss:     1.763407, Tokens per Sec:     8309, Lr: 0.000500
2021-12-19 11:44:56,493 - INFO - joeynmt.training - Epoch   6, Step:   116600, Batch Loss:     1.679949, Tokens per Sec:     8438, Lr: 0.000500
2021-12-19 11:45:58,874 - INFO - joeynmt.training - Epoch   6, Step:   116800, Batch Loss:     1.738145, Tokens per Sec:     8397, Lr: 0.000500
2021-12-19 11:47:01,041 - INFO - joeynmt.training - Epoch   6, Step:   117000, Batch Loss:     1.888175, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 11:48:03,031 - INFO - joeynmt.training - Epoch   6, Step:   117200, Batch Loss:     1.621914, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 11:49:05,284 - INFO - joeynmt.training - Epoch   6, Step:   117400, Batch Loss:     1.744774, Tokens per Sec:     8453, Lr: 0.000500
2021-12-19 11:50:07,592 - INFO - joeynmt.training - Epoch   6, Step:   117600, Batch Loss:     1.538807, Tokens per Sec:     8398, Lr: 0.000500
2021-12-19 11:51:09,527 - INFO - joeynmt.training - Epoch   6, Step:   117800, Batch Loss:     1.708075, Tokens per Sec:     8412, Lr: 0.000500
2021-12-19 11:52:11,413 - INFO - joeynmt.training - Epoch   6, Step:   118000, Batch Loss:     1.464214, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 11:53:13,678 - INFO - joeynmt.training - Epoch   6, Step:   118200, Batch Loss:     1.637732, Tokens per Sec:     8346, Lr: 0.000500
2021-12-19 11:54:15,928 - INFO - joeynmt.training - Epoch   6, Step:   118400, Batch Loss:     1.489259, Tokens per Sec:     8434, Lr: 0.000500
2021-12-19 11:55:18,337 - INFO - joeynmt.training - Epoch   6, Step:   118600, Batch Loss:     1.599809, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 11:56:20,325 - INFO - joeynmt.training - Epoch   6, Step:   118800, Batch Loss:     1.657159, Tokens per Sec:     8396, Lr: 0.000500
2021-12-19 11:57:22,632 - INFO - joeynmt.training - Epoch   6, Step:   119000, Batch Loss:     1.544849, Tokens per Sec:     8319, Lr: 0.000500
2021-12-19 11:58:24,668 - INFO - joeynmt.training - Epoch   6, Step:   119200, Batch Loss:     1.554434, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 11:59:26,832 - INFO - joeynmt.training - Epoch   6, Step:   119400, Batch Loss:     1.461232, Tokens per Sec:     8424, Lr: 0.000500
2021-12-19 12:00:28,725 - INFO - joeynmt.training - Epoch   6, Step:   119600, Batch Loss:     1.801142, Tokens per Sec:     8342, Lr: 0.000500
2021-12-19 12:01:30,792 - INFO - joeynmt.training - Epoch   6, Step:   119800, Batch Loss:     1.596244, Tokens per Sec:     8427, Lr: 0.000500
2021-12-19 12:02:33,223 - INFO - joeynmt.training - Epoch   6, Step:   120000, Batch Loss:     1.672463, Tokens per Sec:     8510, Lr: 0.000500
2021-12-19 12:05:02,608 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 12:05:02,609 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 12:05:05,103 - INFO - joeynmt.training - Example #0
2021-12-19 12:05:05,104 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 12:05:05,104 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'O@@', 'ba@@', '.']
2021-12-19 12:05:05,104 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 12:05:05,104 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 12:05:05,104 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Oba.
2021-12-19 12:05:05,104 - INFO - joeynmt.training - Example #1
2021-12-19 12:05:05,104 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 12:05:05,105 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 12:05:05,105 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 12:05:05,105 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 12:05:05,105 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 12:05:05,105 - INFO - joeynmt.training - Example #2
2021-12-19 12:05:05,105 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 12:05:05,105 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'pl@@', 'under@@', '.']
2021-12-19 12:05:05,106 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 12:05:05,106 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 12:05:05,106 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the plunder.
2021-12-19 12:05:05,106 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   120000: bleu:  17.25, loss: 187400.1719, ppl:  11.0030, duration: 151.8823s
2021-12-19 12:06:07,304 - INFO - joeynmt.training - Epoch   6, Step:   120200, Batch Loss:     1.476737, Tokens per Sec:     8297, Lr: 0.000500
2021-12-19 12:07:09,214 - INFO - joeynmt.training - Epoch   6, Step:   120400, Batch Loss:     1.531386, Tokens per Sec:     8391, Lr: 0.000500
2021-12-19 12:08:11,397 - INFO - joeynmt.training - Epoch   6, Step:   120600, Batch Loss:     1.700020, Tokens per Sec:     8345, Lr: 0.000500
2021-12-19 12:09:12,697 - INFO - joeynmt.training - Epoch   6, Step:   120800, Batch Loss:     1.602134, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 12:10:14,701 - INFO - joeynmt.training - Epoch   6, Step:   121000, Batch Loss:     1.584269, Tokens per Sec:     8296, Lr: 0.000500
2021-12-19 12:11:15,941 - INFO - joeynmt.training - Epoch   6, Step:   121200, Batch Loss:     1.425174, Tokens per Sec:     8213, Lr: 0.000500
2021-12-19 12:12:18,044 - INFO - joeynmt.training - Epoch   6, Step:   121400, Batch Loss:     1.502146, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 12:13:19,885 - INFO - joeynmt.training - Epoch   6, Step:   121600, Batch Loss:     1.580957, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 12:14:22,420 - INFO - joeynmt.training - Epoch   6, Step:   121800, Batch Loss:     1.659938, Tokens per Sec:     8463, Lr: 0.000500
2021-12-19 12:15:24,088 - INFO - joeynmt.training - Epoch   6, Step:   122000, Batch Loss:     1.631754, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 12:16:26,289 - INFO - joeynmt.training - Epoch   6, Step:   122200, Batch Loss:     1.552206, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 12:17:28,339 - INFO - joeynmt.training - Epoch   6, Step:   122400, Batch Loss:     1.823913, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 12:18:30,120 - INFO - joeynmt.training - Epoch   6, Step:   122600, Batch Loss:     1.586303, Tokens per Sec:     8278, Lr: 0.000500
2021-12-19 12:19:32,349 - INFO - joeynmt.training - Epoch   6, Step:   122800, Batch Loss:     1.385823, Tokens per Sec:     8315, Lr: 0.000500
2021-12-19 12:20:34,668 - INFO - joeynmt.training - Epoch   6, Step:   123000, Batch Loss:     1.595516, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 12:21:36,539 - INFO - joeynmt.training - Epoch   6, Step:   123200, Batch Loss:     1.463289, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 12:22:38,336 - INFO - joeynmt.training - Epoch   6, Step:   123400, Batch Loss:     1.689313, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 12:23:40,647 - INFO - joeynmt.training - Epoch   6, Step:   123600, Batch Loss:     1.641654, Tokens per Sec:     8290, Lr: 0.000500
2021-12-19 12:24:42,734 - INFO - joeynmt.training - Epoch   6, Step:   123800, Batch Loss:     1.541902, Tokens per Sec:     8331, Lr: 0.000500
2021-12-19 12:25:45,104 - INFO - joeynmt.training - Epoch   6, Step:   124000, Batch Loss:     1.668873, Tokens per Sec:     8508, Lr: 0.000500
2021-12-19 12:26:47,008 - INFO - joeynmt.training - Epoch   6, Step:   124200, Batch Loss:     1.964169, Tokens per Sec:     8369, Lr: 0.000500
2021-12-19 12:27:49,026 - INFO - joeynmt.training - Epoch   6, Step:   124400, Batch Loss:     1.514830, Tokens per Sec:     8416, Lr: 0.000500
2021-12-19 12:28:50,994 - INFO - joeynmt.training - Epoch   6, Step:   124600, Batch Loss:     1.128556, Tokens per Sec:     8366, Lr: 0.000500
2021-12-19 12:29:53,390 - INFO - joeynmt.training - Epoch   6, Step:   124800, Batch Loss:     1.677621, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 12:30:55,394 - INFO - joeynmt.training - Epoch   6, Step:   125000, Batch Loss:     1.706032, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 12:33:12,850 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 12:33:12,850 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 12:33:15,366 - INFO - joeynmt.training - Example #0
2021-12-19 12:33:15,366 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 12:33:15,367 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'ci@@', 'ous', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 12:33:15,367 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 12:33:15,367 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 12:33:15,367 - INFO - joeynmt.training - 	Hypothesis: A republicious strategy to counter the re-election of Obama
2021-12-19 12:33:15,367 - INFO - joeynmt.training - Example #1
2021-12-19 12:33:15,367 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 12:33:15,367 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 12:33:15,368 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 12:33:15,368 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 12:33:15,368 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 12:33:15,368 - INFO - joeynmt.training - Example #2
2021-12-19 12:33:15,368 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 12:33:15,368 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'pl@@', 'ant.']
2021-12-19 12:33:15,369 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 12:33:15,369 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 12:33:15,369 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the plant.
2021-12-19 12:33:15,369 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step   125000: bleu:  17.59, loss: 186212.5312, ppl:  10.8371, duration: 139.9750s
2021-12-19 12:34:17,629 - INFO - joeynmt.training - Epoch   6, Step:   125200, Batch Loss:     1.542539, Tokens per Sec:     8291, Lr: 0.000500
2021-12-19 12:35:19,854 - INFO - joeynmt.training - Epoch   6, Step:   125400, Batch Loss:     1.783647, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 12:36:21,727 - INFO - joeynmt.training - Epoch   6, Step:   125600, Batch Loss:     1.674647, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 12:37:23,771 - INFO - joeynmt.training - Epoch   6, Step:   125800, Batch Loss:     1.626381, Tokens per Sec:     8274, Lr: 0.000500
2021-12-19 12:38:25,635 - INFO - joeynmt.training - Epoch   6, Step:   126000, Batch Loss:     1.497189, Tokens per Sec:     8402, Lr: 0.000500
2021-12-19 12:39:27,793 - INFO - joeynmt.training - Epoch   6, Step:   126200, Batch Loss:     1.544941, Tokens per Sec:     8418, Lr: 0.000500
2021-12-19 12:40:29,720 - INFO - joeynmt.training - Epoch   6, Step:   126400, Batch Loss:     1.734747, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 12:41:31,856 - INFO - joeynmt.training - Epoch   6, Step:   126600, Batch Loss:     1.601386, Tokens per Sec:     8293, Lr: 0.000500
2021-12-19 12:42:33,884 - INFO - joeynmt.training - Epoch   6, Step:   126800, Batch Loss:     1.597042, Tokens per Sec:     8405, Lr: 0.000500
2021-12-19 12:43:36,367 - INFO - joeynmt.training - Epoch   6, Step:   127000, Batch Loss:     1.607587, Tokens per Sec:     8437, Lr: 0.000500
2021-12-19 12:44:37,715 - INFO - joeynmt.training - Epoch   6, Step:   127200, Batch Loss:     1.468196, Tokens per Sec:     8296, Lr: 0.000500
2021-12-19 12:45:39,545 - INFO - joeynmt.training - Epoch   6, Step:   127400, Batch Loss:     1.416227, Tokens per Sec:     8261, Lr: 0.000500
2021-12-19 12:46:41,826 - INFO - joeynmt.training - Epoch   6, Step:   127600, Batch Loss:     1.445183, Tokens per Sec:     8317, Lr: 0.000500
2021-12-19 12:47:43,610 - INFO - joeynmt.training - Epoch   6, Step:   127800, Batch Loss:     1.530858, Tokens per Sec:     8408, Lr: 0.000500
2021-12-19 12:48:46,115 - INFO - joeynmt.training - Epoch   6, Step:   128000, Batch Loss:     1.438785, Tokens per Sec:     8427, Lr: 0.000500
2021-12-19 12:49:48,332 - INFO - joeynmt.training - Epoch   6, Step:   128200, Batch Loss:     1.476964, Tokens per Sec:     8420, Lr: 0.000500
2021-12-19 12:50:50,409 - INFO - joeynmt.training - Epoch   6, Step:   128400, Batch Loss:     1.162869, Tokens per Sec:     8307, Lr: 0.000500
2021-12-19 12:51:52,366 - INFO - joeynmt.training - Epoch   6, Step:   128600, Batch Loss:     1.653664, Tokens per Sec:     8398, Lr: 0.000500
2021-12-19 12:52:54,324 - INFO - joeynmt.training - Epoch   6, Step:   128800, Batch Loss:     1.455047, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 12:53:56,566 - INFO - joeynmt.training - Epoch   6, Step:   129000, Batch Loss:     1.812849, Tokens per Sec:     8405, Lr: 0.000500
2021-12-19 12:54:24,274 - INFO - joeynmt.training - Epoch   6: total training loss 34829.71
2021-12-19 12:54:24,274 - INFO - joeynmt.training - EPOCH 7
2021-12-19 12:55:02,164 - INFO - joeynmt.training - Epoch   7, Step:   129200, Batch Loss:     1.432829, Tokens per Sec:     7601, Lr: 0.000500
2021-12-19 12:56:04,994 - INFO - joeynmt.training - Epoch   7, Step:   129400, Batch Loss:     1.608304, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 12:57:07,080 - INFO - joeynmt.training - Epoch   7, Step:   129600, Batch Loss:     1.743840, Tokens per Sec:     8345, Lr: 0.000500
2021-12-19 12:58:09,264 - INFO - joeynmt.training - Epoch   7, Step:   129800, Batch Loss:     1.865690, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 12:59:11,373 - INFO - joeynmt.training - Epoch   7, Step:   130000, Batch Loss:     1.564122, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 13:01:37,451 - INFO - joeynmt.training - Example #0
2021-12-19 13:01:37,452 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 13:01:37,452 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 13:01:37,452 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 13:01:37,452 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 13:01:37,452 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 13:01:37,452 - INFO - joeynmt.training - Example #1
2021-12-19 13:01:37,453 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 13:01:37,453 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 13:01:37,453 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 13:01:37,453 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 13:01:37,453 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 13:01:37,453 - INFO - joeynmt.training - Example #2
2021-12-19 13:01:37,453 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 13:01:37,453 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'last', 'year', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 13:01:37,454 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 13:01:37,454 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 13:01:37,454 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this last year to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 13:01:37,454 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   130000: bleu:  17.41, loss: 186326.1250, ppl:  10.8528, duration: 146.0802s
2021-12-19 13:02:39,546 - INFO - joeynmt.training - Epoch   7, Step:   130200, Batch Loss:     1.497951, Tokens per Sec:     8260, Lr: 0.000500
2021-12-19 13:03:41,442 - INFO - joeynmt.training - Epoch   7, Step:   130400, Batch Loss:     1.405664, Tokens per Sec:     8409, Lr: 0.000500
2021-12-19 13:04:43,155 - INFO - joeynmt.training - Epoch   7, Step:   130600, Batch Loss:     1.464475, Tokens per Sec:     8330, Lr: 0.000500
2021-12-19 13:05:44,996 - INFO - joeynmt.training - Epoch   7, Step:   130800, Batch Loss:     1.586785, Tokens per Sec:     8288, Lr: 0.000500
2021-12-19 13:06:47,200 - INFO - joeynmt.training - Epoch   7, Step:   131000, Batch Loss:     1.554754, Tokens per Sec:     8315, Lr: 0.000500
2021-12-19 13:07:49,448 - INFO - joeynmt.training - Epoch   7, Step:   131200, Batch Loss:     1.809090, Tokens per Sec:     8278, Lr: 0.000500
2021-12-19 13:08:51,834 - INFO - joeynmt.training - Epoch   7, Step:   131400, Batch Loss:     1.516502, Tokens per Sec:     8289, Lr: 0.000500
2021-12-19 13:09:53,496 - INFO - joeynmt.training - Epoch   7, Step:   131600, Batch Loss:     1.587938, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 13:10:55,355 - INFO - joeynmt.training - Epoch   7, Step:   131800, Batch Loss:     1.572576, Tokens per Sec:     8369, Lr: 0.000500
2021-12-19 13:11:57,166 - INFO - joeynmt.training - Epoch   7, Step:   132000, Batch Loss:     1.728706, Tokens per Sec:     8236, Lr: 0.000500
2021-12-19 13:12:59,831 - INFO - joeynmt.training - Epoch   7, Step:   132200, Batch Loss:     1.806625, Tokens per Sec:     8409, Lr: 0.000500
2021-12-19 13:14:02,070 - INFO - joeynmt.training - Epoch   7, Step:   132400, Batch Loss:     1.554564, Tokens per Sec:     8422, Lr: 0.000500
2021-12-19 13:15:03,650 - INFO - joeynmt.training - Epoch   7, Step:   132600, Batch Loss:     1.588527, Tokens per Sec:     8317, Lr: 0.000500
2021-12-19 13:16:06,353 - INFO - joeynmt.training - Epoch   7, Step:   132800, Batch Loss:     1.651848, Tokens per Sec:     8424, Lr: 0.000500
2021-12-19 13:17:07,955 - INFO - joeynmt.training - Epoch   7, Step:   133000, Batch Loss:     1.578058, Tokens per Sec:     8333, Lr: 0.000500
2021-12-19 13:18:10,032 - INFO - joeynmt.training - Epoch   7, Step:   133200, Batch Loss:     1.306469, Tokens per Sec:     8405, Lr: 0.000500
2021-12-19 13:19:12,453 - INFO - joeynmt.training - Epoch   7, Step:   133400, Batch Loss:     1.599196, Tokens per Sec:     8386, Lr: 0.000500
2021-12-19 13:20:14,892 - INFO - joeynmt.training - Epoch   7, Step:   133600, Batch Loss:     1.476048, Tokens per Sec:     8447, Lr: 0.000500
2021-12-19 13:21:17,340 - INFO - joeynmt.training - Epoch   7, Step:   133800, Batch Loss:     1.784592, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 13:22:19,112 - INFO - joeynmt.training - Epoch   7, Step:   134000, Batch Loss:     2.124938, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 13:23:21,315 - INFO - joeynmt.training - Epoch   7, Step:   134200, Batch Loss:     1.411877, Tokens per Sec:     8391, Lr: 0.000500
2021-12-19 13:24:23,458 - INFO - joeynmt.training - Epoch   7, Step:   134400, Batch Loss:     1.441367, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 13:25:25,010 - INFO - joeynmt.training - Epoch   7, Step:   134600, Batch Loss:     1.724819, Tokens per Sec:     8248, Lr: 0.000500
2021-12-19 13:26:26,883 - INFO - joeynmt.training - Epoch   7, Step:   134800, Batch Loss:     1.630935, Tokens per Sec:     8371, Lr: 0.000500
2021-12-19 13:27:29,217 - INFO - joeynmt.training - Epoch   7, Step:   135000, Batch Loss:     1.464574, Tokens per Sec:     8332, Lr: 0.000500
2021-12-19 13:29:55,624 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 13:29:55,624 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 13:29:58,235 - INFO - joeynmt.training - Example #0
2021-12-19 13:29:58,235 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 13:29:58,235 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 13:29:58,236 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 13:29:58,236 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 13:29:58,236 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 13:29:58,236 - INFO - joeynmt.training - Example #1
2021-12-19 13:29:58,236 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 13:29:58,236 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 13:29:58,236 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 13:29:58,236 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 13:29:58,237 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 13:29:58,237 - INFO - joeynmt.training - Example #2
2021-12-19 13:29:58,237 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 13:29:58,237 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'claiming', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'ho@@', 't@@', 'le.']
2021-12-19 13:29:58,237 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 13:29:58,238 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 13:29:58,239 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, claiming that electoral fraud is more rare in the United States than the number of people killed by the hotle.
2021-12-19 13:29:58,239 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   135000: bleu:  17.76, loss: 184534.7969, ppl:  10.6069, duration: 149.0215s
2021-12-19 13:31:00,799 - INFO - joeynmt.training - Epoch   7, Step:   135200, Batch Loss:     1.366592, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 13:32:02,551 - INFO - joeynmt.training - Epoch   7, Step:   135400, Batch Loss:     1.421014, Tokens per Sec:     8264, Lr: 0.000500
2021-12-19 13:33:04,604 - INFO - joeynmt.training - Epoch   7, Step:   135600, Batch Loss:     1.587383, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 13:34:06,731 - INFO - joeynmt.training - Epoch   7, Step:   135800, Batch Loss:     1.553456, Tokens per Sec:     8392, Lr: 0.000500
2021-12-19 13:35:08,248 - INFO - joeynmt.training - Epoch   7, Step:   136000, Batch Loss:     1.587098, Tokens per Sec:     8304, Lr: 0.000500
2021-12-19 13:36:10,193 - INFO - joeynmt.training - Epoch   7, Step:   136200, Batch Loss:     1.353245, Tokens per Sec:     8383, Lr: 0.000500
2021-12-19 13:37:11,789 - INFO - joeynmt.training - Epoch   7, Step:   136400, Batch Loss:     1.504213, Tokens per Sec:     8306, Lr: 0.000500
2021-12-19 13:38:13,867 - INFO - joeynmt.training - Epoch   7, Step:   136600, Batch Loss:     1.518441, Tokens per Sec:     8289, Lr: 0.000500
2021-12-19 13:39:15,906 - INFO - joeynmt.training - Epoch   7, Step:   136800, Batch Loss:     1.744628, Tokens per Sec:     8423, Lr: 0.000500
2021-12-19 13:40:18,517 - INFO - joeynmt.training - Epoch   7, Step:   137000, Batch Loss:     1.521250, Tokens per Sec:     8439, Lr: 0.000500
2021-12-19 13:41:20,572 - INFO - joeynmt.training - Epoch   7, Step:   137200, Batch Loss:     1.689265, Tokens per Sec:     8261, Lr: 0.000500
2021-12-19 13:42:22,456 - INFO - joeynmt.training - Epoch   7, Step:   137400, Batch Loss:     1.676294, Tokens per Sec:     8335, Lr: 0.000500
2021-12-19 13:43:24,089 - INFO - joeynmt.training - Epoch   7, Step:   137600, Batch Loss:     1.967148, Tokens per Sec:     8272, Lr: 0.000500
2021-12-19 13:44:26,206 - INFO - joeynmt.training - Epoch   7, Step:   137800, Batch Loss:     1.565631, Tokens per Sec:     8379, Lr: 0.000500
2021-12-19 13:45:27,870 - INFO - joeynmt.training - Epoch   7, Step:   138000, Batch Loss:     1.417806, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 13:46:29,947 - INFO - joeynmt.training - Epoch   7, Step:   138200, Batch Loss:     1.891201, Tokens per Sec:     8414, Lr: 0.000500
2021-12-19 13:47:31,456 - INFO - joeynmt.training - Epoch   7, Step:   138400, Batch Loss:     1.581564, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 13:48:33,792 - INFO - joeynmt.training - Epoch   7, Step:   138600, Batch Loss:     1.664039, Tokens per Sec:     8293, Lr: 0.000500
2021-12-19 13:49:35,906 - INFO - joeynmt.training - Epoch   7, Step:   138800, Batch Loss:     1.711274, Tokens per Sec:     8252, Lr: 0.000500
2021-12-19 13:50:37,839 - INFO - joeynmt.training - Epoch   7, Step:   139000, Batch Loss:     1.475152, Tokens per Sec:     8384, Lr: 0.000500
2021-12-19 13:51:40,110 - INFO - joeynmt.training - Epoch   7, Step:   139200, Batch Loss:     1.480456, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 13:52:42,410 - INFO - joeynmt.training - Epoch   7, Step:   139400, Batch Loss:     1.567719, Tokens per Sec:     8289, Lr: 0.000500
2021-12-19 13:53:44,260 - INFO - joeynmt.training - Epoch   7, Step:   139600, Batch Loss:     1.625047, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 13:54:46,313 - INFO - joeynmt.training - Epoch   7, Step:   139800, Batch Loss:     1.461544, Tokens per Sec:     8302, Lr: 0.000500
2021-12-19 13:55:47,982 - INFO - joeynmt.training - Epoch   7, Step:   140000, Batch Loss:     1.516660, Tokens per Sec:     8281, Lr: 0.000500
2021-12-19 13:58:10,380 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 13:58:10,380 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 13:58:12,812 - INFO - joeynmt.training - Example #0
2021-12-19 13:58:12,812 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 13:58:12,812 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 13:58:12,813 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 13:58:12,813 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 13:58:12,813 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 13:58:12,813 - INFO - joeynmt.training - Example #1
2021-12-19 13:58:12,813 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 13:58:12,813 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 13:58:12,813 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 13:58:12,814 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 13:58:12,814 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 13:58:12,814 - INFO - joeynmt.training - Example #2
2021-12-19 13:58:12,814 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 13:58:12,814 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 13:58:12,814 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 13:58:12,814 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 13:58:12,815 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 13:58:12,815 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   140000: bleu:  17.88, loss: 183801.2812, ppl:  10.5078, duration: 144.8322s
2021-12-19 13:59:15,804 - INFO - joeynmt.training - Epoch   7, Step:   140200, Batch Loss:     1.643939, Tokens per Sec:     8398, Lr: 0.000500
2021-12-19 14:00:17,527 - INFO - joeynmt.training - Epoch   7, Step:   140400, Batch Loss:     1.590530, Tokens per Sec:     8308, Lr: 0.000500
2021-12-19 14:01:19,666 - INFO - joeynmt.training - Epoch   7, Step:   140600, Batch Loss:     1.740798, Tokens per Sec:     8358, Lr: 0.000500
2021-12-19 14:02:21,708 - INFO - joeynmt.training - Epoch   7, Step:   140800, Batch Loss:     1.642782, Tokens per Sec:     8238, Lr: 0.000500
2021-12-19 14:03:24,112 - INFO - joeynmt.training - Epoch   7, Step:   141000, Batch Loss:     1.689215, Tokens per Sec:     8416, Lr: 0.000500
2021-12-19 14:04:25,800 - INFO - joeynmt.training - Epoch   7, Step:   141200, Batch Loss:     1.771215, Tokens per Sec:     8428, Lr: 0.000500
2021-12-19 14:05:28,072 - INFO - joeynmt.training - Epoch   7, Step:   141400, Batch Loss:     1.593239, Tokens per Sec:     8366, Lr: 0.000500
2021-12-19 14:06:30,665 - INFO - joeynmt.training - Epoch   7, Step:   141600, Batch Loss:     1.626425, Tokens per Sec:     8464, Lr: 0.000500
2021-12-19 14:07:33,025 - INFO - joeynmt.training - Epoch   7, Step:   141800, Batch Loss:     1.842813, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 14:08:35,209 - INFO - joeynmt.training - Epoch   7, Step:   142000, Batch Loss:     1.575935, Tokens per Sec:     8411, Lr: 0.000500
2021-12-19 14:09:37,541 - INFO - joeynmt.training - Epoch   7, Step:   142200, Batch Loss:     1.726754, Tokens per Sec:     8411, Lr: 0.000500
2021-12-19 14:10:39,426 - INFO - joeynmt.training - Epoch   7, Step:   142400, Batch Loss:     1.661930, Tokens per Sec:     8242, Lr: 0.000500
2021-12-19 14:11:41,656 - INFO - joeynmt.training - Epoch   7, Step:   142600, Batch Loss:     1.692964, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 14:12:44,433 - INFO - joeynmt.training - Epoch   7, Step:   142800, Batch Loss:     1.569998, Tokens per Sec:     8384, Lr: 0.000500
2021-12-19 14:13:46,773 - INFO - joeynmt.training - Epoch   7, Step:   143000, Batch Loss:     1.641771, Tokens per Sec:     8343, Lr: 0.000500
2021-12-19 14:14:48,704 - INFO - joeynmt.training - Epoch   7, Step:   143200, Batch Loss:     1.609010, Tokens per Sec:     8331, Lr: 0.000500
2021-12-19 14:15:50,814 - INFO - joeynmt.training - Epoch   7, Step:   143400, Batch Loss:     1.578690, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 14:16:53,355 - INFO - joeynmt.training - Epoch   7, Step:   143600, Batch Loss:     1.522352, Tokens per Sec:     8403, Lr: 0.000500
2021-12-19 14:17:55,708 - INFO - joeynmt.training - Epoch   7, Step:   143800, Batch Loss:     1.415693, Tokens per Sec:     8455, Lr: 0.000500
2021-12-19 14:18:57,732 - INFO - joeynmt.training - Epoch   7, Step:   144000, Batch Loss:     1.520222, Tokens per Sec:     8320, Lr: 0.000500
2021-12-19 14:19:59,740 - INFO - joeynmt.training - Epoch   7, Step:   144200, Batch Loss:     1.324842, Tokens per Sec:     8265, Lr: 0.000500
2021-12-19 14:21:01,665 - INFO - joeynmt.training - Epoch   7, Step:   144400, Batch Loss:     1.435659, Tokens per Sec:     8330, Lr: 0.000500
2021-12-19 14:22:03,115 - INFO - joeynmt.training - Epoch   7, Step:   144600, Batch Loss:     1.451960, Tokens per Sec:     8264, Lr: 0.000500
2021-12-19 14:23:05,555 - INFO - joeynmt.training - Epoch   7, Step:   144800, Batch Loss:     1.583117, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 14:24:06,895 - INFO - joeynmt.training - Epoch   7, Step:   145000, Batch Loss:     1.649153, Tokens per Sec:     8261, Lr: 0.000500
2021-12-19 14:26:29,984 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 14:26:29,984 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 14:26:32,496 - INFO - joeynmt.training - Example #0
2021-12-19 14:26:32,497 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 14:26:32,497 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'ci@@', 'zed', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 14:26:32,497 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 14:26:32,497 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 14:26:32,497 - INFO - joeynmt.training - 	Hypothesis: A republicized strategy to counter the re-election of Obama
2021-12-19 14:26:32,498 - INFO - joeynmt.training - Example #1
2021-12-19 14:26:32,498 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 14:26:32,498 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 14:26:32,498 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 14:26:32,498 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 14:26:32,498 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 14:26:32,498 - INFO - joeynmt.training - Example #2
2021-12-19 14:26:32,498 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 14:26:32,499 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'unusual', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'head@@', '.']
2021-12-19 14:26:32,499 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 14:26:32,499 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 14:26:32,499 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, stating that electoral fraud is more unusual in the United States than the number of people killed by the head.
2021-12-19 14:26:32,499 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   145000: bleu:  17.85, loss: 182650.0469, ppl:  10.3541, duration: 145.6038s
2021-12-19 14:27:35,080 - INFO - joeynmt.training - Epoch   7, Step:   145200, Batch Loss:     1.561354, Tokens per Sec:     8359, Lr: 0.000500
2021-12-19 14:28:36,456 - INFO - joeynmt.training - Epoch   7, Step:   145400, Batch Loss:     1.528634, Tokens per Sec:     8248, Lr: 0.000500
2021-12-19 14:29:38,760 - INFO - joeynmt.training - Epoch   7, Step:   145600, Batch Loss:     1.559233, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 14:30:40,713 - INFO - joeynmt.training - Epoch   7, Step:   145800, Batch Loss:     1.509382, Tokens per Sec:     8254, Lr: 0.000500
2021-12-19 14:31:43,125 - INFO - joeynmt.training - Epoch   7, Step:   146000, Batch Loss:     1.463974, Tokens per Sec:     8488, Lr: 0.000500
2021-12-19 14:32:45,337 - INFO - joeynmt.training - Epoch   7, Step:   146200, Batch Loss:     1.552648, Tokens per Sec:     8404, Lr: 0.000500
2021-12-19 14:33:47,787 - INFO - joeynmt.training - Epoch   7, Step:   146400, Batch Loss:     1.576405, Tokens per Sec:     8402, Lr: 0.000500
2021-12-19 14:34:49,486 - INFO - joeynmt.training - Epoch   7, Step:   146600, Batch Loss:     1.538035, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 14:35:51,864 - INFO - joeynmt.training - Epoch   7, Step:   146800, Batch Loss:     1.487978, Tokens per Sec:     8335, Lr: 0.000500
2021-12-19 14:36:54,203 - INFO - joeynmt.training - Epoch   7, Step:   147000, Batch Loss:     1.497291, Tokens per Sec:     8431, Lr: 0.000500
2021-12-19 14:37:56,075 - INFO - joeynmt.training - Epoch   7, Step:   147200, Batch Loss:     1.516786, Tokens per Sec:     8379, Lr: 0.000500
2021-12-19 14:38:58,230 - INFO - joeynmt.training - Epoch   7, Step:   147400, Batch Loss:     1.730402, Tokens per Sec:     8385, Lr: 0.000500
2021-12-19 14:40:00,104 - INFO - joeynmt.training - Epoch   7, Step:   147600, Batch Loss:     1.572488, Tokens per Sec:     8260, Lr: 0.000500
2021-12-19 14:41:02,113 - INFO - joeynmt.training - Epoch   7, Step:   147800, Batch Loss:     1.666988, Tokens per Sec:     8445, Lr: 0.000500
2021-12-19 14:42:04,021 - INFO - joeynmt.training - Epoch   7, Step:   148000, Batch Loss:     1.675943, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 14:43:05,135 - INFO - joeynmt.training - Epoch   7, Step:   148200, Batch Loss:     1.379180, Tokens per Sec:     8176, Lr: 0.000500
2021-12-19 14:44:07,300 - INFO - joeynmt.training - Epoch   7, Step:   148400, Batch Loss:     1.500408, Tokens per Sec:     8269, Lr: 0.000500
2021-12-19 14:45:09,382 - INFO - joeynmt.training - Epoch   7, Step:   148600, Batch Loss:     1.670883, Tokens per Sec:     8398, Lr: 0.000500
2021-12-19 14:46:11,945 - INFO - joeynmt.training - Epoch   7, Step:   148800, Batch Loss:     1.351762, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 14:47:13,795 - INFO - joeynmt.training - Epoch   7, Step:   149000, Batch Loss:     1.720216, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 14:48:15,916 - INFO - joeynmt.training - Epoch   7, Step:   149200, Batch Loss:     1.718308, Tokens per Sec:     8474, Lr: 0.000500
2021-12-19 14:49:17,887 - INFO - joeynmt.training - Epoch   7, Step:   149400, Batch Loss:     1.673684, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 14:50:20,170 - INFO - joeynmt.training - Epoch   7, Step:   149600, Batch Loss:     1.576877, Tokens per Sec:     8354, Lr: 0.000500
2021-12-19 14:51:22,648 - INFO - joeynmt.training - Epoch   7, Step:   149800, Batch Loss:     1.517510, Tokens per Sec:     8419, Lr: 0.000500
2021-12-19 14:52:24,734 - INFO - joeynmt.training - Epoch   7, Step:   150000, Batch Loss:     1.676263, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 14:54:50,431 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 14:54:50,431 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 14:54:52,836 - INFO - joeynmt.training - Example #0
2021-12-19 14:54:52,836 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 14:54:52,836 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 14:54:52,837 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 14:54:52,837 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 14:54:52,837 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 14:54:52,837 - INFO - joeynmt.training - Example #1
2021-12-19 14:54:52,837 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 14:54:52,837 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 14:54:52,837 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 14:54:52,837 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 14:54:52,838 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 14:54:52,838 - INFO - joeynmt.training - Example #2
2021-12-19 14:54:52,838 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 14:54:52,838 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'sees', 'this', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'pl@@', 'under@@', '.']
2021-12-19 14:54:52,838 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 14:54:52,838 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 14:54:52,838 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre sees this as a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the plunder.
2021-12-19 14:54:52,839 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step   150000: bleu:  17.99, loss: 181556.5469, ppl:  10.2102, duration: 148.1045s
2021-12-19 14:55:54,880 - INFO - joeynmt.training - Epoch   7, Step:   150200, Batch Loss:     1.691037, Tokens per Sec:     8256, Lr: 0.000500
2021-12-19 14:56:56,473 - INFO - joeynmt.training - Epoch   7, Step:   150400, Batch Loss:     2.096653, Tokens per Sec:     8449, Lr: 0.000500
2021-12-19 14:57:58,377 - INFO - joeynmt.training - Epoch   7: total training loss 34063.68
2021-12-19 14:57:58,378 - INFO - joeynmt.training - EPOCH 8
2021-12-19 14:58:02,068 - INFO - joeynmt.training - Epoch   8, Step:   150600, Batch Loss:     1.563708, Tokens per Sec:      716, Lr: 0.000500
2021-12-19 14:59:04,144 - INFO - joeynmt.training - Epoch   8, Step:   150800, Batch Loss:     1.422513, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 15:00:05,785 - INFO - joeynmt.training - Epoch   8, Step:   151000, Batch Loss:     1.435852, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 15:01:07,582 - INFO - joeynmt.training - Epoch   8, Step:   151200, Batch Loss:     1.551864, Tokens per Sec:     8302, Lr: 0.000500
2021-12-19 15:02:10,004 - INFO - joeynmt.training - Epoch   8, Step:   151400, Batch Loss:     1.590674, Tokens per Sec:     8405, Lr: 0.000500
2021-12-19 15:03:11,698 - INFO - joeynmt.training - Epoch   8, Step:   151600, Batch Loss:     1.442502, Tokens per Sec:     8383, Lr: 0.000500
2021-12-19 15:04:13,835 - INFO - joeynmt.training - Epoch   8, Step:   151800, Batch Loss:     1.457876, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 15:05:15,730 - INFO - joeynmt.training - Epoch   8, Step:   152000, Batch Loss:     1.396453, Tokens per Sec:     8320, Lr: 0.000500
2021-12-19 15:06:17,805 - INFO - joeynmt.training - Epoch   8, Step:   152200, Batch Loss:     1.449936, Tokens per Sec:     8345, Lr: 0.000500
2021-12-19 15:07:19,808 - INFO - joeynmt.training - Epoch   8, Step:   152400, Batch Loss:     1.525114, Tokens per Sec:     8429, Lr: 0.000500
2021-12-19 15:08:22,174 - INFO - joeynmt.training - Epoch   8, Step:   152600, Batch Loss:     1.587103, Tokens per Sec:     8402, Lr: 0.000500
2021-12-19 15:09:24,383 - INFO - joeynmt.training - Epoch   8, Step:   152800, Batch Loss:     1.384843, Tokens per Sec:     8318, Lr: 0.000500
2021-12-19 15:10:26,932 - INFO - joeynmt.training - Epoch   8, Step:   153000, Batch Loss:     1.368503, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 15:11:28,612 - INFO - joeynmt.training - Epoch   8, Step:   153200, Batch Loss:     1.464351, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 15:12:30,254 - INFO - joeynmt.training - Epoch   8, Step:   153400, Batch Loss:     1.612661, Tokens per Sec:     8285, Lr: 0.000500
2021-12-19 15:13:32,501 - INFO - joeynmt.training - Epoch   8, Step:   153600, Batch Loss:     1.601005, Tokens per Sec:     8441, Lr: 0.000500
2021-12-19 15:14:34,381 - INFO - joeynmt.training - Epoch   8, Step:   153800, Batch Loss:     1.487156, Tokens per Sec:     8441, Lr: 0.000500
2021-12-19 15:15:36,630 - INFO - joeynmt.training - Epoch   8, Step:   154000, Batch Loss:     1.580752, Tokens per Sec:     8304, Lr: 0.000500
2021-12-19 15:16:38,073 - INFO - joeynmt.training - Epoch   8, Step:   154200, Batch Loss:     1.533284, Tokens per Sec:     8307, Lr: 0.000500
2021-12-19 15:17:40,155 - INFO - joeynmt.training - Epoch   8, Step:   154400, Batch Loss:     1.467741, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 15:18:42,130 - INFO - joeynmt.training - Epoch   8, Step:   154600, Batch Loss:     1.694753, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 15:19:44,563 - INFO - joeynmt.training - Epoch   8, Step:   154800, Batch Loss:     1.656269, Tokens per Sec:     8358, Lr: 0.000500
2021-12-19 15:20:45,731 - INFO - joeynmt.training - Epoch   8, Step:   155000, Batch Loss:     1.723113, Tokens per Sec:     8200, Lr: 0.000500
2021-12-19 15:23:16,499 - INFO - joeynmt.training - Example #0
2021-12-19 15:23:16,499 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 15:23:16,500 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 15:23:16,500 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 15:23:16,500 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 15:23:16,500 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 15:23:16,500 - INFO - joeynmt.training - Example #1
2021-12-19 15:23:16,500 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 15:23:16,500 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 15:23:16,500 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 15:23:16,501 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 15:23:16,501 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 15:23:16,501 - INFO - joeynmt.training - Example #2
2021-12-19 15:23:16,501 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 15:23:16,501 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'plu@@', 'd@@', 'der.']
2021-12-19 15:23:16,501 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 15:23:16,501 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 15:23:16,501 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this to be a myth, saying that electoral fraud is more rare in the United States than the number of people killed by the pludder.
2021-12-19 15:23:16,502 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   155000: bleu:  18.01, loss: 182259.2969, ppl:  10.3025, duration: 150.7700s
2021-12-19 15:24:19,010 - INFO - joeynmt.training - Epoch   8, Step:   155200, Batch Loss:     1.592159, Tokens per Sec:     8264, Lr: 0.000500
2021-12-19 15:25:21,174 - INFO - joeynmt.training - Epoch   8, Step:   155400, Batch Loss:     1.687986, Tokens per Sec:     8439, Lr: 0.000500
2021-12-19 15:26:23,539 - INFO - joeynmt.training - Epoch   8, Step:   155600, Batch Loss:     1.457141, Tokens per Sec:     8386, Lr: 0.000500
2021-12-19 15:27:26,088 - INFO - joeynmt.training - Epoch   8, Step:   155800, Batch Loss:     1.577992, Tokens per Sec:     8474, Lr: 0.000500
2021-12-19 15:28:27,881 - INFO - joeynmt.training - Epoch   8, Step:   156000, Batch Loss:     1.592395, Tokens per Sec:     8219, Lr: 0.000500
2021-12-19 15:29:30,010 - INFO - joeynmt.training - Epoch   8, Step:   156200, Batch Loss:     1.566137, Tokens per Sec:     8423, Lr: 0.000500
2021-12-19 15:30:31,890 - INFO - joeynmt.training - Epoch   8, Step:   156400, Batch Loss:     1.382114, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 15:31:33,383 - INFO - joeynmt.training - Epoch   8, Step:   156600, Batch Loss:     1.175959, Tokens per Sec:     8237, Lr: 0.000500
2021-12-19 15:32:35,534 - INFO - joeynmt.training - Epoch   8, Step:   156800, Batch Loss:     1.781584, Tokens per Sec:     8379, Lr: 0.000500
2021-12-19 15:33:38,049 - INFO - joeynmt.training - Epoch   8, Step:   157000, Batch Loss:     1.503761, Tokens per Sec:     8431, Lr: 0.000500
2021-12-19 15:34:40,072 - INFO - joeynmt.training - Epoch   8, Step:   157200, Batch Loss:     1.619876, Tokens per Sec:     8384, Lr: 0.000500
2021-12-19 15:35:41,814 - INFO - joeynmt.training - Epoch   8, Step:   157400, Batch Loss:     1.530212, Tokens per Sec:     8323, Lr: 0.000500
2021-12-19 15:36:43,389 - INFO - joeynmt.training - Epoch   8, Step:   157600, Batch Loss:     1.309651, Tokens per Sec:     8322, Lr: 0.000500
2021-12-19 15:37:44,993 - INFO - joeynmt.training - Epoch   8, Step:   157800, Batch Loss:     1.514274, Tokens per Sec:     8275, Lr: 0.000500
2021-12-19 15:38:47,077 - INFO - joeynmt.training - Epoch   8, Step:   158000, Batch Loss:     1.605619, Tokens per Sec:     8238, Lr: 0.000500
2021-12-19 15:39:49,438 - INFO - joeynmt.training - Epoch   8, Step:   158200, Batch Loss:     1.735642, Tokens per Sec:     8412, Lr: 0.000500
2021-12-19 15:40:51,618 - INFO - joeynmt.training - Epoch   8, Step:   158400, Batch Loss:     1.740245, Tokens per Sec:     8292, Lr: 0.000500
2021-12-19 15:41:53,661 - INFO - joeynmt.training - Epoch   8, Step:   158600, Batch Loss:     1.444621, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 15:42:56,241 - INFO - joeynmt.training - Epoch   8, Step:   158800, Batch Loss:     1.396079, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 15:43:58,380 - INFO - joeynmt.training - Epoch   8, Step:   159000, Batch Loss:     1.440764, Tokens per Sec:     8478, Lr: 0.000500
2021-12-19 15:45:00,512 - INFO - joeynmt.training - Epoch   8, Step:   159200, Batch Loss:     1.606371, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 15:46:02,713 - INFO - joeynmt.training - Epoch   8, Step:   159400, Batch Loss:     1.591830, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 15:47:04,596 - INFO - joeynmt.training - Epoch   8, Step:   159600, Batch Loss:     1.542755, Tokens per Sec:     8431, Lr: 0.000500
2021-12-19 15:48:06,247 - INFO - joeynmt.training - Epoch   8, Step:   159800, Batch Loss:     1.609832, Tokens per Sec:     8331, Lr: 0.000500
2021-12-19 15:49:07,943 - INFO - joeynmt.training - Epoch   8, Step:   160000, Batch Loss:     1.721461, Tokens per Sec:     8279, Lr: 0.000500
2021-12-19 15:51:33,476 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 15:51:33,477 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 15:51:36,101 - INFO - joeynmt.training - Example #0
2021-12-19 15:51:36,101 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 15:51:36,101 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 'election', 'of', 'Obama']
2021-12-19 15:51:36,101 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 15:51:36,101 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 15:51:36,102 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the election of Obama
2021-12-19 15:51:36,102 - INFO - joeynmt.training - Example #1
2021-12-19 15:51:36,102 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 15:51:36,102 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 15:51:36,102 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 15:51:36,102 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 15:51:36,102 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 15:51:36,103 - INFO - joeynmt.training - Example #2
2021-12-19 15:51:36,103 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 15:51:36,103 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'pl@@', 'ant.']
2021-12-19 15:51:36,103 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 15:51:36,103 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 15:51:36,103 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the plant.
2021-12-19 15:51:36,103 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   160000: bleu:  18.12, loss: 180872.4375, ppl:  10.1212, duration: 148.1601s
2021-12-19 15:52:38,016 - INFO - joeynmt.training - Epoch   8, Step:   160200, Batch Loss:     1.863851, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 15:53:39,370 - INFO - joeynmt.training - Epoch   8, Step:   160400, Batch Loss:     1.507012, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 15:54:41,130 - INFO - joeynmt.training - Epoch   8, Step:   160600, Batch Loss:     1.446441, Tokens per Sec:     8333, Lr: 0.000500
2021-12-19 15:55:43,133 - INFO - joeynmt.training - Epoch   8, Step:   160800, Batch Loss:     1.613712, Tokens per Sec:     8399, Lr: 0.000500
2021-12-19 15:56:45,344 - INFO - joeynmt.training - Epoch   8, Step:   161000, Batch Loss:     1.449425, Tokens per Sec:     8348, Lr: 0.000500
2021-12-19 15:57:46,798 - INFO - joeynmt.training - Epoch   8, Step:   161200, Batch Loss:     1.516943, Tokens per Sec:     8292, Lr: 0.000500
2021-12-19 15:58:49,083 - INFO - joeynmt.training - Epoch   8, Step:   161400, Batch Loss:     1.658833, Tokens per Sec:     8433, Lr: 0.000500
2021-12-19 15:59:51,519 - INFO - joeynmt.training - Epoch   8, Step:   161600, Batch Loss:     1.650500, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 16:00:53,450 - INFO - joeynmt.training - Epoch   8, Step:   161800, Batch Loss:     1.156281, Tokens per Sec:     8337, Lr: 0.000500
2021-12-19 16:01:55,810 - INFO - joeynmt.training - Epoch   8, Step:   162000, Batch Loss:     1.347571, Tokens per Sec:     8358, Lr: 0.000500
2021-12-19 16:02:57,740 - INFO - joeynmt.training - Epoch   8, Step:   162200, Batch Loss:     1.712143, Tokens per Sec:     8264, Lr: 0.000500
2021-12-19 16:04:00,116 - INFO - joeynmt.training - Epoch   8, Step:   162400, Batch Loss:     1.676483, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 16:05:02,289 - INFO - joeynmt.training - Epoch   8, Step:   162600, Batch Loss:     1.573472, Tokens per Sec:     8285, Lr: 0.000500
2021-12-19 16:06:04,150 - INFO - joeynmt.training - Epoch   8, Step:   162800, Batch Loss:     1.660262, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 16:07:06,579 - INFO - joeynmt.training - Epoch   8, Step:   163000, Batch Loss:     1.559933, Tokens per Sec:     8440, Lr: 0.000500
2021-12-19 16:08:08,587 - INFO - joeynmt.training - Epoch   8, Step:   163200, Batch Loss:     1.537727, Tokens per Sec:     8384, Lr: 0.000500
2021-12-19 16:09:10,382 - INFO - joeynmt.training - Epoch   8, Step:   163400, Batch Loss:     1.344593, Tokens per Sec:     8335, Lr: 0.000500
2021-12-19 16:10:12,405 - INFO - joeynmt.training - Epoch   8, Step:   163600, Batch Loss:     1.500136, Tokens per Sec:     8409, Lr: 0.000500
2021-12-19 16:11:14,445 - INFO - joeynmt.training - Epoch   8, Step:   163800, Batch Loss:     1.551390, Tokens per Sec:     8375, Lr: 0.000500
2021-12-19 16:12:15,945 - INFO - joeynmt.training - Epoch   8, Step:   164000, Batch Loss:     1.577369, Tokens per Sec:     8342, Lr: 0.000500
2021-12-19 16:13:18,004 - INFO - joeynmt.training - Epoch   8, Step:   164200, Batch Loss:     1.398535, Tokens per Sec:     8451, Lr: 0.000500
2021-12-19 16:14:20,702 - INFO - joeynmt.training - Epoch   8, Step:   164400, Batch Loss:     1.633652, Tokens per Sec:     8454, Lr: 0.000500
2021-12-19 16:15:22,462 - INFO - joeynmt.training - Epoch   8, Step:   164600, Batch Loss:     1.396614, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 16:16:24,358 - INFO - joeynmt.training - Epoch   8, Step:   164800, Batch Loss:     1.564286, Tokens per Sec:     8341, Lr: 0.000500
2021-12-19 16:17:26,343 - INFO - joeynmt.training - Epoch   8, Step:   165000, Batch Loss:     0.987422, Tokens per Sec:     8335, Lr: 0.000500
2021-12-19 16:19:51,993 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 16:19:51,994 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 16:19:56,612 - INFO - joeynmt.training - Example #0
2021-12-19 16:19:56,613 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 16:19:56,613 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 16:19:56,613 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 16:19:56,613 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 16:19:56,614 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 16:19:56,614 - INFO - joeynmt.training - Example #1
2021-12-19 16:19:56,614 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 16:19:56,614 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 16:19:56,614 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 16:19:56,614 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 16:19:56,614 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 16:19:56,614 - INFO - joeynmt.training - Example #2
2021-12-19 16:19:56,615 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 16:19:56,615 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'head@@', '.']
2021-12-19 16:19:56,615 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 16:19:56,615 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 16:19:56,615 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the head.
2021-12-19 16:19:56,615 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   165000: bleu:  18.25, loss: 180032.0938, ppl:  10.0130, duration: 150.2721s
2021-12-19 16:20:58,960 - INFO - joeynmt.training - Epoch   8, Step:   165200, Batch Loss:     1.615859, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 16:22:00,918 - INFO - joeynmt.training - Epoch   8, Step:   165400, Batch Loss:     1.659800, Tokens per Sec:     8219, Lr: 0.000500
2021-12-19 16:23:03,451 - INFO - joeynmt.training - Epoch   8, Step:   165600, Batch Loss:     1.605684, Tokens per Sec:     8336, Lr: 0.000500
2021-12-19 16:24:05,477 - INFO - joeynmt.training - Epoch   8, Step:   165800, Batch Loss:     1.567472, Tokens per Sec:     8441, Lr: 0.000500
2021-12-19 16:25:06,951 - INFO - joeynmt.training - Epoch   8, Step:   166000, Batch Loss:     1.630140, Tokens per Sec:     8294, Lr: 0.000500
2021-12-19 16:26:09,013 - INFO - joeynmt.training - Epoch   8, Step:   166200, Batch Loss:     1.794389, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 16:27:10,757 - INFO - joeynmt.training - Epoch   8, Step:   166400, Batch Loss:     1.503266, Tokens per Sec:     8225, Lr: 0.000500
2021-12-19 16:28:12,551 - INFO - joeynmt.training - Epoch   8, Step:   166600, Batch Loss:     2.062928, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 16:29:14,658 - INFO - joeynmt.training - Epoch   8, Step:   166800, Batch Loss:     1.595659, Tokens per Sec:     8423, Lr: 0.000500
2021-12-19 16:30:16,639 - INFO - joeynmt.training - Epoch   8, Step:   167000, Batch Loss:     1.545191, Tokens per Sec:     8414, Lr: 0.000500
2021-12-19 16:31:19,245 - INFO - joeynmt.training - Epoch   8, Step:   167200, Batch Loss:     1.607554, Tokens per Sec:     8443, Lr: 0.000500
2021-12-19 16:32:21,661 - INFO - joeynmt.training - Epoch   8, Step:   167400, Batch Loss:     1.721338, Tokens per Sec:     8443, Lr: 0.000500
2021-12-19 16:33:23,513 - INFO - joeynmt.training - Epoch   8, Step:   167600, Batch Loss:     1.705580, Tokens per Sec:     8389, Lr: 0.000500
2021-12-19 16:34:24,928 - INFO - joeynmt.training - Epoch   8, Step:   167800, Batch Loss:     1.695424, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 16:35:27,795 - INFO - joeynmt.training - Epoch   8, Step:   168000, Batch Loss:     1.566433, Tokens per Sec:     8425, Lr: 0.000500
2021-12-19 16:36:29,192 - INFO - joeynmt.training - Epoch   8, Step:   168200, Batch Loss:     1.511230, Tokens per Sec:     8265, Lr: 0.000500
2021-12-19 16:37:31,365 - INFO - joeynmt.training - Epoch   8, Step:   168400, Batch Loss:     1.580564, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 16:38:33,422 - INFO - joeynmt.training - Epoch   8, Step:   168600, Batch Loss:     1.554861, Tokens per Sec:     8348, Lr: 0.000500
2021-12-19 16:39:34,942 - INFO - joeynmt.training - Epoch   8, Step:   168800, Batch Loss:     1.523325, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 16:40:37,224 - INFO - joeynmt.training - Epoch   8, Step:   169000, Batch Loss:     1.904328, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 16:41:39,221 - INFO - joeynmt.training - Epoch   8, Step:   169200, Batch Loss:     1.514260, Tokens per Sec:     8456, Lr: 0.000500
2021-12-19 16:42:40,819 - INFO - joeynmt.training - Epoch   8, Step:   169400, Batch Loss:     1.654292, Tokens per Sec:     8331, Lr: 0.000500
2021-12-19 16:43:42,860 - INFO - joeynmt.training - Epoch   8, Step:   169600, Batch Loss:     1.582438, Tokens per Sec:     8451, Lr: 0.000500
2021-12-19 16:44:44,549 - INFO - joeynmt.training - Epoch   8, Step:   169800, Batch Loss:     1.811100, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 16:45:47,182 - INFO - joeynmt.training - Epoch   8, Step:   170000, Batch Loss:     1.519477, Tokens per Sec:     8482, Lr: 0.000500
2021-12-19 16:48:11,475 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 16:48:11,475 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 16:48:13,872 - INFO - joeynmt.training - Example #0
2021-12-19 16:48:13,872 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 16:48:13,872 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'ci@@', 'zed', 'strategy', 'to', 'counter', 'the', 'Obama', 're@@', 'election']
2021-12-19 16:48:13,873 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 16:48:13,873 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 16:48:13,873 - INFO - joeynmt.training - 	Hypothesis: A republicized strategy to counter the Obama reelection
2021-12-19 16:48:13,873 - INFO - joeynmt.training - Example #1
2021-12-19 16:48:13,873 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 16:48:13,873 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 16:48:13,873 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 16:48:13,874 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 16:48:13,874 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 16:48:13,874 - INFO - joeynmt.training - Example #2
2021-12-19 16:48:13,874 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 16:48:13,874 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 16:48:13,874 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 16:48:13,874 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 16:48:13,874 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 16:48:13,875 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step   170000: bleu:  18.38, loss: 179065.3594, ppl:   9.8898, duration: 146.6918s
2021-12-19 16:49:18,404 - INFO - joeynmt.training - Epoch   8, Step:   170200, Batch Loss:     1.596457, Tokens per Sec:     8173, Lr: 0.000500
2021-12-19 16:50:20,414 - INFO - joeynmt.training - Epoch   8, Step:   170400, Batch Loss:     1.540165, Tokens per Sec:     8392, Lr: 0.000500
2021-12-19 16:51:22,569 - INFO - joeynmt.training - Epoch   8, Step:   170600, Batch Loss:     1.437140, Tokens per Sec:     8436, Lr: 0.000500
2021-12-19 16:52:24,909 - INFO - joeynmt.training - Epoch   8, Step:   170800, Batch Loss:     1.400003, Tokens per Sec:     8297, Lr: 0.000500
2021-12-19 16:53:26,976 - INFO - joeynmt.training - Epoch   8, Step:   171000, Batch Loss:     1.647758, Tokens per Sec:     8322, Lr: 0.000500
2021-12-19 16:54:28,926 - INFO - joeynmt.training - Epoch   8, Step:   171200, Batch Loss:     1.562166, Tokens per Sec:     8338, Lr: 0.000500
2021-12-19 16:55:30,770 - INFO - joeynmt.training - Epoch   8, Step:   171400, Batch Loss:     1.762091, Tokens per Sec:     8378, Lr: 0.000500
2021-12-19 16:56:32,364 - INFO - joeynmt.training - Epoch   8, Step:   171600, Batch Loss:     1.629918, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 16:57:33,895 - INFO - joeynmt.training - Epoch   8, Step:   171800, Batch Loss:     1.500751, Tokens per Sec:     8397, Lr: 0.000500
2021-12-19 16:58:36,235 - INFO - joeynmt.training - Epoch   8, Step:   172000, Batch Loss:     1.537530, Tokens per Sec:     8384, Lr: 0.000500
2021-12-19 16:59:09,719 - INFO - joeynmt.training - Epoch   8: total training loss 33428.84
2021-12-19 16:59:09,719 - INFO - joeynmt.training - EPOCH 9
2021-12-19 16:59:41,126 - INFO - joeynmt.training - Epoch   9, Step:   172200, Batch Loss:     1.415817, Tokens per Sec:     7397, Lr: 0.000500
2021-12-19 17:00:43,277 - INFO - joeynmt.training - Epoch   9, Step:   172400, Batch Loss:     1.738966, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 17:01:45,433 - INFO - joeynmt.training - Epoch   9, Step:   172600, Batch Loss:     1.486367, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 17:02:47,220 - INFO - joeynmt.training - Epoch   9, Step:   172800, Batch Loss:     1.480241, Tokens per Sec:     8243, Lr: 0.000500
2021-12-19 17:03:49,823 - INFO - joeynmt.training - Epoch   9, Step:   173000, Batch Loss:     1.653026, Tokens per Sec:     8446, Lr: 0.000500
2021-12-19 17:04:51,690 - INFO - joeynmt.training - Epoch   9, Step:   173200, Batch Loss:     1.608029, Tokens per Sec:     8394, Lr: 0.000500
2021-12-19 17:05:54,421 - INFO - joeynmt.training - Epoch   9, Step:   173400, Batch Loss:     1.391157, Tokens per Sec:     8459, Lr: 0.000500
2021-12-19 17:06:56,607 - INFO - joeynmt.training - Epoch   9, Step:   173600, Batch Loss:     1.786708, Tokens per Sec:     8305, Lr: 0.000500
2021-12-19 17:07:58,724 - INFO - joeynmt.training - Epoch   9, Step:   173800, Batch Loss:     1.448194, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 17:09:00,411 - INFO - joeynmt.training - Epoch   9, Step:   174000, Batch Loss:     1.783887, Tokens per Sec:     8330, Lr: 0.000500
2021-12-19 17:10:02,425 - INFO - joeynmt.training - Epoch   9, Step:   174200, Batch Loss:     1.514216, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 17:11:04,282 - INFO - joeynmt.training - Epoch   9, Step:   174400, Batch Loss:     1.376972, Tokens per Sec:     8267, Lr: 0.000500
2021-12-19 17:12:05,724 - INFO - joeynmt.training - Epoch   9, Step:   174600, Batch Loss:     1.659794, Tokens per Sec:     8259, Lr: 0.000500
2021-12-19 17:13:07,519 - INFO - joeynmt.training - Epoch   9, Step:   174800, Batch Loss:     1.667382, Tokens per Sec:     8262, Lr: 0.000500
2021-12-19 17:14:09,734 - INFO - joeynmt.training - Epoch   9, Step:   175000, Batch Loss:     1.777080, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 17:16:36,078 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 17:16:36,078 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 17:16:38,705 - INFO - joeynmt.training - Example #0
2021-12-19 17:16:38,706 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 17:16:38,706 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 17:16:38,706 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 17:16:38,706 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 17:16:38,707 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 17:16:38,707 - INFO - joeynmt.training - Example #1
2021-12-19 17:16:38,707 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 17:16:38,707 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 17:16:38,707 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 17:16:38,707 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 17:16:38,707 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 17:16:38,708 - INFO - joeynmt.training - Example #2
2021-12-19 17:16:38,708 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 17:16:38,708 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'mass', 'fi@@', 're.']
2021-12-19 17:16:38,708 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 17:16:38,708 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 17:16:38,708 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the mass fire.
2021-12-19 17:16:38,709 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   175000: bleu:  18.28, loss: 178857.0781, ppl:   9.8635, duration: 148.9744s
2021-12-19 17:17:40,525 - INFO - joeynmt.training - Epoch   9, Step:   175200, Batch Loss:     1.763809, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 17:18:42,728 - INFO - joeynmt.training - Epoch   9, Step:   175400, Batch Loss:     1.497746, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 17:19:44,542 - INFO - joeynmt.training - Epoch   9, Step:   175600, Batch Loss:     1.645459, Tokens per Sec:     8321, Lr: 0.000500
2021-12-19 17:20:46,508 - INFO - joeynmt.training - Epoch   9, Step:   175800, Batch Loss:     1.553084, Tokens per Sec:     8456, Lr: 0.000500
2021-12-19 17:21:48,930 - INFO - joeynmt.training - Epoch   9, Step:   176000, Batch Loss:     1.729499, Tokens per Sec:     8460, Lr: 0.000500
2021-12-19 17:22:50,350 - INFO - joeynmt.training - Epoch   9, Step:   176200, Batch Loss:     1.557449, Tokens per Sec:     8328, Lr: 0.000500
2021-12-19 17:23:52,223 - INFO - joeynmt.training - Epoch   9, Step:   176400, Batch Loss:     1.563383, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 17:24:54,675 - INFO - joeynmt.training - Epoch   9, Step:   176600, Batch Loss:     1.424369, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 17:25:56,624 - INFO - joeynmt.training - Epoch   9, Step:   176800, Batch Loss:     1.604586, Tokens per Sec:     8409, Lr: 0.000500
2021-12-19 17:26:58,773 - INFO - joeynmt.training - Epoch   9, Step:   177000, Batch Loss:     1.702541, Tokens per Sec:     8343, Lr: 0.000500
2021-12-19 17:28:00,676 - INFO - joeynmt.training - Epoch   9, Step:   177200, Batch Loss:     1.347048, Tokens per Sec:     8311, Lr: 0.000500
2021-12-19 17:29:02,751 - INFO - joeynmt.training - Epoch   9, Step:   177400, Batch Loss:     1.741135, Tokens per Sec:     8475, Lr: 0.000500
2021-12-19 17:30:04,786 - INFO - joeynmt.training - Epoch   9, Step:   177600, Batch Loss:     1.582135, Tokens per Sec:     8320, Lr: 0.000500
2021-12-19 17:31:06,945 - INFO - joeynmt.training - Epoch   9, Step:   177800, Batch Loss:     1.200738, Tokens per Sec:     8303, Lr: 0.000500
2021-12-19 17:32:09,086 - INFO - joeynmt.training - Epoch   9, Step:   178000, Batch Loss:     1.559724, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 17:33:10,950 - INFO - joeynmt.training - Epoch   9, Step:   178200, Batch Loss:     1.581664, Tokens per Sec:     8342, Lr: 0.000500
2021-12-19 17:34:12,663 - INFO - joeynmt.training - Epoch   9, Step:   178400, Batch Loss:     1.676223, Tokens per Sec:     8257, Lr: 0.000500
2021-12-19 17:35:15,002 - INFO - joeynmt.training - Epoch   9, Step:   178600, Batch Loss:     1.572787, Tokens per Sec:     8408, Lr: 0.000500
2021-12-19 17:36:16,811 - INFO - joeynmt.training - Epoch   9, Step:   178800, Batch Loss:     1.506229, Tokens per Sec:     8389, Lr: 0.000500
2021-12-19 17:37:18,413 - INFO - joeynmt.training - Epoch   9, Step:   179000, Batch Loss:     1.596420, Tokens per Sec:     8360, Lr: 0.000500
2021-12-19 17:38:20,101 - INFO - joeynmt.training - Epoch   9, Step:   179200, Batch Loss:     1.679243, Tokens per Sec:     8393, Lr: 0.000500
2021-12-19 17:39:21,823 - INFO - joeynmt.training - Epoch   9, Step:   179400, Batch Loss:     1.399942, Tokens per Sec:     8310, Lr: 0.000500
2021-12-19 17:40:23,482 - INFO - joeynmt.training - Epoch   9, Step:   179600, Batch Loss:     1.545718, Tokens per Sec:     8345, Lr: 0.000500
2021-12-19 17:41:25,657 - INFO - joeynmt.training - Epoch   9, Step:   179800, Batch Loss:     1.435691, Tokens per Sec:     8420, Lr: 0.000500
2021-12-19 17:42:27,386 - INFO - joeynmt.training - Epoch   9, Step:   180000, Batch Loss:     1.642649, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 17:44:51,785 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 17:44:51,785 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 17:44:54,299 - INFO - joeynmt.training - Example #0
2021-12-19 17:44:54,299 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 17:44:54,300 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 17:44:54,300 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 17:44:54,300 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 17:44:54,300 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 17:44:54,300 - INFO - joeynmt.training - Example #1
2021-12-19 17:44:54,300 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 17:44:54,301 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 17:44:54,301 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 17:44:54,301 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 17:44:54,301 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 17:44:54,301 - INFO - joeynmt.training - Example #2
2021-12-19 17:44:54,302 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 17:44:54,302 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'pl@@', 'ate.']
2021-12-19 17:44:54,302 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 17:44:54,302 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 17:44:54,302 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the plate.
2021-12-19 17:44:54,302 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   180000: bleu:  18.41, loss: 178294.2344, ppl:   9.7927, duration: 146.9159s
2021-12-19 17:45:56,391 - INFO - joeynmt.training - Epoch   9, Step:   180200, Batch Loss:     1.516907, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 17:46:57,976 - INFO - joeynmt.training - Epoch   9, Step:   180400, Batch Loss:     1.480824, Tokens per Sec:     8321, Lr: 0.000500
2021-12-19 17:48:00,208 - INFO - joeynmt.training - Epoch   9, Step:   180600, Batch Loss:     1.416071, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 17:49:01,956 - INFO - joeynmt.training - Epoch   9, Step:   180800, Batch Loss:     1.633515, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 17:50:04,074 - INFO - joeynmt.training - Epoch   9, Step:   181000, Batch Loss:     1.525283, Tokens per Sec:     8369, Lr: 0.000500
2021-12-19 17:51:06,084 - INFO - joeynmt.training - Epoch   9, Step:   181200, Batch Loss:     1.532018, Tokens per Sec:     8356, Lr: 0.000500
2021-12-19 17:52:08,129 - INFO - joeynmt.training - Epoch   9, Step:   181400, Batch Loss:     1.447364, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 17:53:10,182 - INFO - joeynmt.training - Epoch   9, Step:   181600, Batch Loss:     1.556998, Tokens per Sec:     8397, Lr: 0.000500
2021-12-19 17:54:12,178 - INFO - joeynmt.training - Epoch   9, Step:   181800, Batch Loss:     2.161865, Tokens per Sec:     8418, Lr: 0.000500
2021-12-19 17:55:14,490 - INFO - joeynmt.training - Epoch   9, Step:   182000, Batch Loss:     1.513901, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 17:56:16,378 - INFO - joeynmt.training - Epoch   9, Step:   182200, Batch Loss:     1.327996, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 17:57:18,453 - INFO - joeynmt.training - Epoch   9, Step:   182400, Batch Loss:     1.585233, Tokens per Sec:     8383, Lr: 0.000500
2021-12-19 17:58:20,363 - INFO - joeynmt.training - Epoch   9, Step:   182600, Batch Loss:     1.556968, Tokens per Sec:     8295, Lr: 0.000500
2021-12-19 17:59:22,222 - INFO - joeynmt.training - Epoch   9, Step:   182800, Batch Loss:     1.454055, Tokens per Sec:     8220, Lr: 0.000500
2021-12-19 18:00:24,282 - INFO - joeynmt.training - Epoch   9, Step:   183000, Batch Loss:     1.380549, Tokens per Sec:     8419, Lr: 0.000500
2021-12-19 18:01:25,942 - INFO - joeynmt.training - Epoch   9, Step:   183200, Batch Loss:     1.485491, Tokens per Sec:     8291, Lr: 0.000500
2021-12-19 18:02:28,127 - INFO - joeynmt.training - Epoch   9, Step:   183400, Batch Loss:     1.616825, Tokens per Sec:     8394, Lr: 0.000500
2021-12-19 18:03:30,182 - INFO - joeynmt.training - Epoch   9, Step:   183600, Batch Loss:     1.476058, Tokens per Sec:     8332, Lr: 0.000500
2021-12-19 18:04:32,186 - INFO - joeynmt.training - Epoch   9, Step:   183800, Batch Loss:     1.383722, Tokens per Sec:     8345, Lr: 0.000500
2021-12-19 18:05:34,451 - INFO - joeynmt.training - Epoch   9, Step:   184000, Batch Loss:     1.518371, Tokens per Sec:     8416, Lr: 0.000500
2021-12-19 18:06:36,138 - INFO - joeynmt.training - Epoch   9, Step:   184200, Batch Loss:     1.494926, Tokens per Sec:     8255, Lr: 0.000500
2021-12-19 18:07:37,886 - INFO - joeynmt.training - Epoch   9, Step:   184400, Batch Loss:     1.641344, Tokens per Sec:     8283, Lr: 0.000500
2021-12-19 18:08:39,882 - INFO - joeynmt.training - Epoch   9, Step:   184600, Batch Loss:     1.622177, Tokens per Sec:     8307, Lr: 0.000500
2021-12-19 18:09:41,771 - INFO - joeynmt.training - Epoch   9, Step:   184800, Batch Loss:     1.717726, Tokens per Sec:     8321, Lr: 0.000500
2021-12-19 18:10:43,376 - INFO - joeynmt.training - Epoch   9, Step:   185000, Batch Loss:     1.637294, Tokens per Sec:     8320, Lr: 0.000500
2021-12-19 18:13:09,310 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 18:13:09,310 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 18:13:13,264 - INFO - joeynmt.training - Example #0
2021-12-19 18:13:13,264 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 18:13:13,264 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'Obama']
2021-12-19 18:13:13,264 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 18:13:13,264 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 18:13:13,265 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the reelection of Obama
2021-12-19 18:13:13,265 - INFO - joeynmt.training - Example #1
2021-12-19 18:13:13,265 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 18:13:13,265 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 18:13:13,265 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 18:13:13,265 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 18:13:13,265 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 18:13:13,265 - INFO - joeynmt.training - Example #2
2021-12-19 18:13:13,266 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 18:13:13,266 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'sees', 'this', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 18:13:13,266 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 18:13:13,266 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 18:13:13,266 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre sees this as a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 18:13:13,266 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   185000: bleu:  18.51, loss: 177479.3281, ppl:   9.6911, duration: 149.8899s
2021-12-19 18:14:15,731 - INFO - joeynmt.training - Epoch   9, Step:   185200, Batch Loss:     1.379182, Tokens per Sec:     8298, Lr: 0.000500
2021-12-19 18:15:17,824 - INFO - joeynmt.training - Epoch   9, Step:   185400, Batch Loss:     1.603526, Tokens per Sec:     8427, Lr: 0.000500
2021-12-19 18:16:19,754 - INFO - joeynmt.training - Epoch   9, Step:   185600, Batch Loss:     1.647931, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 18:17:22,025 - INFO - joeynmt.training - Epoch   9, Step:   185800, Batch Loss:     1.383943, Tokens per Sec:     8369, Lr: 0.000500
2021-12-19 18:18:24,002 - INFO - joeynmt.training - Epoch   9, Step:   186000, Batch Loss:     1.556864, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 18:19:25,641 - INFO - joeynmt.training - Epoch   9, Step:   186200, Batch Loss:     1.550244, Tokens per Sec:     8348, Lr: 0.000500
2021-12-19 18:20:27,291 - INFO - joeynmt.training - Epoch   9, Step:   186400, Batch Loss:     1.316621, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 18:21:29,697 - INFO - joeynmt.training - Epoch   9, Step:   186600, Batch Loss:     1.128660, Tokens per Sec:     8417, Lr: 0.000500
2021-12-19 18:22:31,728 - INFO - joeynmt.training - Epoch   9, Step:   186800, Batch Loss:     1.689726, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 18:23:33,628 - INFO - joeynmt.training - Epoch   9, Step:   187000, Batch Loss:     1.548162, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 18:24:35,903 - INFO - joeynmt.training - Epoch   9, Step:   187200, Batch Loss:     1.404786, Tokens per Sec:     8417, Lr: 0.000500
2021-12-19 18:25:37,894 - INFO - joeynmt.training - Epoch   9, Step:   187400, Batch Loss:     1.579813, Tokens per Sec:     8445, Lr: 0.000500
2021-12-19 18:26:39,812 - INFO - joeynmt.training - Epoch   9, Step:   187600, Batch Loss:     1.594127, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 18:27:41,832 - INFO - joeynmt.training - Epoch   9, Step:   187800, Batch Loss:     1.603547, Tokens per Sec:     8288, Lr: 0.000500
2021-12-19 18:28:43,722 - INFO - joeynmt.training - Epoch   9, Step:   188000, Batch Loss:     1.495119, Tokens per Sec:     8435, Lr: 0.000500
2021-12-19 18:29:46,437 - INFO - joeynmt.training - Epoch   9, Step:   188200, Batch Loss:     1.420370, Tokens per Sec:     8417, Lr: 0.000500
2021-12-19 18:30:48,412 - INFO - joeynmt.training - Epoch   9, Step:   188400, Batch Loss:     1.704967, Tokens per Sec:     8275, Lr: 0.000500
2021-12-19 18:31:50,571 - INFO - joeynmt.training - Epoch   9, Step:   188600, Batch Loss:     1.571628, Tokens per Sec:     8375, Lr: 0.000500
2021-12-19 18:32:52,259 - INFO - joeynmt.training - Epoch   9, Step:   188800, Batch Loss:     1.474185, Tokens per Sec:     8242, Lr: 0.000500
2021-12-19 18:33:54,245 - INFO - joeynmt.training - Epoch   9, Step:   189000, Batch Loss:     1.438955, Tokens per Sec:     8315, Lr: 0.000500
2021-12-19 18:34:56,713 - INFO - joeynmt.training - Epoch   9, Step:   189200, Batch Loss:     1.545817, Tokens per Sec:     8393, Lr: 0.000500
2021-12-19 18:35:58,467 - INFO - joeynmt.training - Epoch   9, Step:   189400, Batch Loss:     1.787773, Tokens per Sec:     8338, Lr: 0.000500
2021-12-19 18:37:00,396 - INFO - joeynmt.training - Epoch   9, Step:   189600, Batch Loss:     1.568367, Tokens per Sec:     8394, Lr: 0.000500
2021-12-19 18:38:02,025 - INFO - joeynmt.training - Epoch   9, Step:   189800, Batch Loss:     1.557343, Tokens per Sec:     8371, Lr: 0.000500
2021-12-19 18:39:04,466 - INFO - joeynmt.training - Epoch   9, Step:   190000, Batch Loss:     1.614609, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 18:41:29,125 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 18:41:29,126 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 18:41:33,900 - INFO - joeynmt.training - Example #0
2021-12-19 18:41:33,900 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 18:41:33,901 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 18:41:33,901 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 18:41:33,901 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 18:41:33,901 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 18:41:33,901 - INFO - joeynmt.training - Example #1
2021-12-19 18:41:33,901 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 18:41:33,901 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 18:41:33,902 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 18:41:33,902 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 18:41:33,902 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 18:41:33,902 - INFO - joeynmt.training - Example #2
2021-12-19 18:41:33,902 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 18:41:33,902 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'shorter', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'mas@@', 'ter.']
2021-12-19 18:41:33,902 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 18:41:33,902 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 18:41:33,903 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this to be a mythy, saying that electoral fraud is shorter in the United States than the number of people killed by the master.
2021-12-19 18:41:33,903 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step   190000: bleu:  18.74, loss: 176743.6406, ppl:   9.6003, duration: 149.4362s
2021-12-19 18:42:35,914 - INFO - joeynmt.training - Epoch   9, Step:   190200, Batch Loss:     1.387285, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 18:43:37,731 - INFO - joeynmt.training - Epoch   9, Step:   190400, Batch Loss:     1.410085, Tokens per Sec:     8368, Lr: 0.000500
2021-12-19 18:44:39,651 - INFO - joeynmt.training - Epoch   9, Step:   190600, Batch Loss:     1.630978, Tokens per Sec:     8319, Lr: 0.000500
2021-12-19 18:45:41,527 - INFO - joeynmt.training - Epoch   9, Step:   190800, Batch Loss:     1.382598, Tokens per Sec:     8322, Lr: 0.000500
2021-12-19 18:46:43,594 - INFO - joeynmt.training - Epoch   9, Step:   191000, Batch Loss:     1.528948, Tokens per Sec:     8410, Lr: 0.000500
2021-12-19 18:47:45,635 - INFO - joeynmt.training - Epoch   9, Step:   191200, Batch Loss:     1.458499, Tokens per Sec:     8341, Lr: 0.000500
2021-12-19 18:48:47,667 - INFO - joeynmt.training - Epoch   9, Step:   191400, Batch Loss:     1.745448, Tokens per Sec:     8367, Lr: 0.000500
2021-12-19 18:49:49,591 - INFO - joeynmt.training - Epoch   9, Step:   191600, Batch Loss:     1.617119, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 18:50:50,669 - INFO - joeynmt.training - Epoch   9, Step:   191800, Batch Loss:     1.464530, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 18:51:52,960 - INFO - joeynmt.training - Epoch   9, Step:   192000, Batch Loss:     1.461257, Tokens per Sec:     8415, Lr: 0.000500
2021-12-19 18:52:55,364 - INFO - joeynmt.training - Epoch   9, Step:   192200, Batch Loss:     1.570577, Tokens per Sec:     8418, Lr: 0.000500
2021-12-19 18:53:57,517 - INFO - joeynmt.training - Epoch   9, Step:   192400, Batch Loss:     1.416911, Tokens per Sec:     8455, Lr: 0.000500
2021-12-19 18:54:59,857 - INFO - joeynmt.training - Epoch   9, Step:   192600, Batch Loss:     1.386551, Tokens per Sec:     8389, Lr: 0.000500
2021-12-19 18:56:02,016 - INFO - joeynmt.training - Epoch   9, Step:   192800, Batch Loss:     1.652041, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 18:57:04,257 - INFO - joeynmt.training - Epoch   9, Step:   193000, Batch Loss:     1.749018, Tokens per Sec:     8426, Lr: 0.000500
2021-12-19 18:58:06,247 - INFO - joeynmt.training - Epoch   9, Step:   193200, Batch Loss:     1.634718, Tokens per Sec:     8297, Lr: 0.000500
2021-12-19 18:59:07,930 - INFO - joeynmt.training - Epoch   9, Step:   193400, Batch Loss:     1.492130, Tokens per Sec:     8324, Lr: 0.000500
2021-12-19 19:00:09,785 - INFO - joeynmt.training - Epoch   9, Step:   193600, Batch Loss:     1.431810, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 19:00:18,221 - INFO - joeynmt.training - Epoch   9: total training loss 32893.60
2021-12-19 19:00:18,222 - INFO - joeynmt.training - EPOCH 10
2021-12-19 19:01:15,199 - INFO - joeynmt.training - Epoch  10, Step:   193800, Batch Loss:     1.489777, Tokens per Sec:     7893, Lr: 0.000500
2021-12-19 19:02:17,584 - INFO - joeynmt.training - Epoch  10, Step:   194000, Batch Loss:     1.582565, Tokens per Sec:     8292, Lr: 0.000500
2021-12-19 19:03:20,054 - INFO - joeynmt.training - Epoch  10, Step:   194200, Batch Loss:     1.890392, Tokens per Sec:     8476, Lr: 0.000500
2021-12-19 19:04:22,355 - INFO - joeynmt.training - Epoch  10, Step:   194400, Batch Loss:     1.641723, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 19:05:24,237 - INFO - joeynmt.training - Epoch  10, Step:   194600, Batch Loss:     1.406121, Tokens per Sec:     8287, Lr: 0.000500
2021-12-19 19:06:26,538 - INFO - joeynmt.training - Epoch  10, Step:   194800, Batch Loss:     1.751812, Tokens per Sec:     8467, Lr: 0.000500
2021-12-19 19:07:28,729 - INFO - joeynmt.training - Epoch  10, Step:   195000, Batch Loss:     1.480291, Tokens per Sec:     8473, Lr: 0.000500
2021-12-19 19:09:58,040 - INFO - joeynmt.training - Example #0
2021-12-19 19:09:58,041 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 19:09:58,041 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 19:09:58,041 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 19:09:58,041 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 19:09:58,041 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 19:09:58,042 - INFO - joeynmt.training - Example #1
2021-12-19 19:09:58,042 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 19:09:58,042 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 19:09:58,042 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 19:09:58,043 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 19:09:58,043 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 19:09:58,043 - INFO - joeynmt.training - Example #2
2021-12-19 19:09:58,043 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 19:09:58,043 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'claiming', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 19:09:58,044 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 19:09:58,044 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 19:09:58,044 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, claiming that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 19:09:58,044 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   195000: bleu:  18.63, loss: 176796.1094, ppl:   9.6068, duration: 149.3150s
2021-12-19 19:11:02,010 - INFO - joeynmt.training - Epoch  10, Step:   195200, Batch Loss:     1.389339, Tokens per Sec:     8040, Lr: 0.000500
2021-12-19 19:12:04,685 - INFO - joeynmt.training - Epoch  10, Step:   195400, Batch Loss:     1.545252, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 19:13:07,151 - INFO - joeynmt.training - Epoch  10, Step:   195600, Batch Loss:     1.225276, Tokens per Sec:     8441, Lr: 0.000500
2021-12-19 19:14:09,398 - INFO - joeynmt.training - Epoch  10, Step:   195800, Batch Loss:     1.350482, Tokens per Sec:     8343, Lr: 0.000500
2021-12-19 19:15:11,532 - INFO - joeynmt.training - Epoch  10, Step:   196000, Batch Loss:     1.539150, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 19:16:13,315 - INFO - joeynmt.training - Epoch  10, Step:   196200, Batch Loss:     1.290504, Tokens per Sec:     8269, Lr: 0.000500
2021-12-19 19:17:15,083 - INFO - joeynmt.training - Epoch  10, Step:   196400, Batch Loss:     1.497675, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 19:18:16,614 - INFO - joeynmt.training - Epoch  10, Step:   196600, Batch Loss:     1.584487, Tokens per Sec:     8284, Lr: 0.000500
2021-12-19 19:19:18,867 - INFO - joeynmt.training - Epoch  10, Step:   196800, Batch Loss:     1.447165, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 19:20:21,049 - INFO - joeynmt.training - Epoch  10, Step:   197000, Batch Loss:     1.555392, Tokens per Sec:     8397, Lr: 0.000500
2021-12-19 19:21:23,172 - INFO - joeynmt.training - Epoch  10, Step:   197200, Batch Loss:     1.325185, Tokens per Sec:     8356, Lr: 0.000500
2021-12-19 19:22:24,960 - INFO - joeynmt.training - Epoch  10, Step:   197400, Batch Loss:     1.019375, Tokens per Sec:     8393, Lr: 0.000500
2021-12-19 19:23:27,035 - INFO - joeynmt.training - Epoch  10, Step:   197600, Batch Loss:     1.532014, Tokens per Sec:     8360, Lr: 0.000500
2021-12-19 19:24:29,331 - INFO - joeynmt.training - Epoch  10, Step:   197800, Batch Loss:     1.517905, Tokens per Sec:     8321, Lr: 0.000500
2021-12-19 19:25:31,532 - INFO - joeynmt.training - Epoch  10, Step:   198000, Batch Loss:     1.374026, Tokens per Sec:     8328, Lr: 0.000500
2021-12-19 19:26:33,453 - INFO - joeynmt.training - Epoch  10, Step:   198200, Batch Loss:     1.404377, Tokens per Sec:     8332, Lr: 0.000500
2021-12-19 19:27:35,684 - INFO - joeynmt.training - Epoch  10, Step:   198400, Batch Loss:     1.393278, Tokens per Sec:     8397, Lr: 0.000500
2021-12-19 19:28:37,458 - INFO - joeynmt.training - Epoch  10, Step:   198600, Batch Loss:     1.584575, Tokens per Sec:     8314, Lr: 0.000500
2021-12-19 19:29:39,462 - INFO - joeynmt.training - Epoch  10, Step:   198800, Batch Loss:     1.467877, Tokens per Sec:     8289, Lr: 0.000500
2021-12-19 19:30:41,738 - INFO - joeynmt.training - Epoch  10, Step:   199000, Batch Loss:     1.392075, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 19:31:43,557 - INFO - joeynmt.training - Epoch  10, Step:   199200, Batch Loss:     1.324698, Tokens per Sec:     8332, Lr: 0.000500
2021-12-19 19:32:45,592 - INFO - joeynmt.training - Epoch  10, Step:   199400, Batch Loss:     1.455962, Tokens per Sec:     8426, Lr: 0.000500
2021-12-19 19:33:47,020 - INFO - joeynmt.training - Epoch  10, Step:   199600, Batch Loss:     1.502848, Tokens per Sec:     8255, Lr: 0.000500
2021-12-19 19:34:49,285 - INFO - joeynmt.training - Epoch  10, Step:   199800, Batch Loss:     1.628184, Tokens per Sec:     8429, Lr: 0.000500
2021-12-19 19:35:51,354 - INFO - joeynmt.training - Epoch  10, Step:   200000, Batch Loss:     1.459186, Tokens per Sec:     8392, Lr: 0.000500
2021-12-19 19:38:15,705 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 19:38:15,706 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 19:38:18,295 - INFO - joeynmt.training - Example #0
2021-12-19 19:38:18,296 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 19:38:18,296 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 19:38:18,296 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 19:38:18,296 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 19:38:18,297 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 19:38:18,297 - INFO - joeynmt.training - Example #1
2021-12-19 19:38:18,297 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 19:38:18,297 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 19:38:18,297 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 19:38:18,297 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 19:38:18,298 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 19:38:18,298 - INFO - joeynmt.training - Example #2
2021-12-19 19:38:18,298 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 19:38:18,298 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 19:38:18,298 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 19:38:18,298 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 19:38:18,298 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 19:38:18,299 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   200000: bleu:  18.75, loss: 176528.4062, ppl:   9.5739, duration: 146.9437s
2021-12-19 19:39:20,211 - INFO - joeynmt.training - Epoch  10, Step:   200200, Batch Loss:     1.512451, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 19:40:22,217 - INFO - joeynmt.training - Epoch  10, Step:   200400, Batch Loss:     1.342239, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 19:41:24,800 - INFO - joeynmt.training - Epoch  10, Step:   200600, Batch Loss:     1.399481, Tokens per Sec:     8385, Lr: 0.000500
2021-12-19 19:42:27,001 - INFO - joeynmt.training - Epoch  10, Step:   200800, Batch Loss:     1.499523, Tokens per Sec:     8357, Lr: 0.000500
2021-12-19 19:43:29,249 - INFO - joeynmt.training - Epoch  10, Step:   201000, Batch Loss:     1.641147, Tokens per Sec:     8378, Lr: 0.000500
2021-12-19 19:44:31,402 - INFO - joeynmt.training - Epoch  10, Step:   201200, Batch Loss:     1.563617, Tokens per Sec:     8394, Lr: 0.000500
2021-12-19 19:45:33,712 - INFO - joeynmt.training - Epoch  10, Step:   201400, Batch Loss:     1.760715, Tokens per Sec:     8443, Lr: 0.000500
2021-12-19 19:46:35,811 - INFO - joeynmt.training - Epoch  10, Step:   201600, Batch Loss:     1.597056, Tokens per Sec:     8359, Lr: 0.000500
2021-12-19 19:47:38,067 - INFO - joeynmt.training - Epoch  10, Step:   201800, Batch Loss:     1.343540, Tokens per Sec:     8346, Lr: 0.000500
2021-12-19 19:48:40,395 - INFO - joeynmt.training - Epoch  10, Step:   202000, Batch Loss:     1.516256, Tokens per Sec:     8361, Lr: 0.000500
2021-12-19 19:49:42,228 - INFO - joeynmt.training - Epoch  10, Step:   202200, Batch Loss:     1.558225, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 19:50:44,537 - INFO - joeynmt.training - Epoch  10, Step:   202400, Batch Loss:     1.669231, Tokens per Sec:     8437, Lr: 0.000500
2021-12-19 19:51:46,951 - INFO - joeynmt.training - Epoch  10, Step:   202600, Batch Loss:     1.777059, Tokens per Sec:     8412, Lr: 0.000500
2021-12-19 19:52:48,395 - INFO - joeynmt.training - Epoch  10, Step:   202800, Batch Loss:     1.503718, Tokens per Sec:     8273, Lr: 0.000500
2021-12-19 19:53:50,184 - INFO - joeynmt.training - Epoch  10, Step:   203000, Batch Loss:     1.485287, Tokens per Sec:     8285, Lr: 0.000500
2021-12-19 19:54:52,364 - INFO - joeynmt.training - Epoch  10, Step:   203200, Batch Loss:     1.589257, Tokens per Sec:     8398, Lr: 0.000500
2021-12-19 19:55:54,452 - INFO - joeynmt.training - Epoch  10, Step:   203400, Batch Loss:     1.508317, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 19:56:56,841 - INFO - joeynmt.training - Epoch  10, Step:   203600, Batch Loss:     1.395495, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 19:57:58,618 - INFO - joeynmt.training - Epoch  10, Step:   203800, Batch Loss:     1.547455, Tokens per Sec:     8407, Lr: 0.000500
2021-12-19 19:59:00,810 - INFO - joeynmt.training - Epoch  10, Step:   204000, Batch Loss:     1.598728, Tokens per Sec:     8369, Lr: 0.000500
2021-12-19 20:00:02,660 - INFO - joeynmt.training - Epoch  10, Step:   204200, Batch Loss:     1.564088, Tokens per Sec:     8437, Lr: 0.000500
2021-12-19 20:01:04,630 - INFO - joeynmt.training - Epoch  10, Step:   204400, Batch Loss:     1.606152, Tokens per Sec:     8309, Lr: 0.000500
2021-12-19 20:02:06,081 - INFO - joeynmt.training - Epoch  10, Step:   204600, Batch Loss:     1.678073, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 20:03:08,005 - INFO - joeynmt.training - Epoch  10, Step:   204800, Batch Loss:     1.621124, Tokens per Sec:     8227, Lr: 0.000500
2021-12-19 20:04:09,841 - INFO - joeynmt.training - Epoch  10, Step:   205000, Batch Loss:     1.652361, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 20:06:32,974 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 20:06:32,975 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 20:06:35,477 - INFO - joeynmt.training - Example #0
2021-12-19 20:06:35,478 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 20:06:35,478 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 20:06:35,478 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 20:06:35,478 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 20:06:35,478 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 20:06:35,479 - INFO - joeynmt.training - Example #1
2021-12-19 20:06:35,479 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 20:06:35,479 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 20:06:35,479 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 20:06:35,479 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 20:06:35,479 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 20:06:35,479 - INFO - joeynmt.training - Example #2
2021-12-19 20:06:35,480 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 20:06:35,480 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'fi@@', 're.']
2021-12-19 20:06:35,480 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 20:06:35,480 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 20:06:35,480 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the fire.
2021-12-19 20:06:35,480 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   205000: bleu:  18.80, loss: 175229.2500, ppl:   9.4161, duration: 145.6383s
2021-12-19 20:07:38,226 - INFO - joeynmt.training - Epoch  10, Step:   205200, Batch Loss:     1.691601, Tokens per Sec:     8360, Lr: 0.000500
2021-12-19 20:08:40,344 - INFO - joeynmt.training - Epoch  10, Step:   205400, Batch Loss:     1.662923, Tokens per Sec:     8333, Lr: 0.000500
2021-12-19 20:09:42,143 - INFO - joeynmt.training - Epoch  10, Step:   205600, Batch Loss:     1.390834, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 20:10:43,949 - INFO - joeynmt.training - Epoch  10, Step:   205800, Batch Loss:     1.336802, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 20:11:45,729 - INFO - joeynmt.training - Epoch  10, Step:   206000, Batch Loss:     1.518986, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 20:12:47,677 - INFO - joeynmt.training - Epoch  10, Step:   206200, Batch Loss:     1.492498, Tokens per Sec:     8311, Lr: 0.000500
2021-12-19 20:13:49,802 - INFO - joeynmt.training - Epoch  10, Step:   206400, Batch Loss:     1.478022, Tokens per Sec:     8317, Lr: 0.000500
2021-12-19 20:14:51,747 - INFO - joeynmt.training - Epoch  10, Step:   206600, Batch Loss:     1.628744, Tokens per Sec:     8425, Lr: 0.000500
2021-12-19 20:15:53,631 - INFO - joeynmt.training - Epoch  10, Step:   206800, Batch Loss:     1.574983, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 20:16:55,577 - INFO - joeynmt.training - Epoch  10, Step:   207000, Batch Loss:     1.356148, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 20:17:57,489 - INFO - joeynmt.training - Epoch  10, Step:   207200, Batch Loss:     1.665172, Tokens per Sec:     8406, Lr: 0.000500
2021-12-19 20:18:59,120 - INFO - joeynmt.training - Epoch  10, Step:   207400, Batch Loss:     1.494941, Tokens per Sec:     8353, Lr: 0.000500
2021-12-19 20:20:01,157 - INFO - joeynmt.training - Epoch  10, Step:   207600, Batch Loss:     1.562933, Tokens per Sec:     8434, Lr: 0.000500
2021-12-19 20:21:03,112 - INFO - joeynmt.training - Epoch  10, Step:   207800, Batch Loss:     1.439437, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 20:22:04,969 - INFO - joeynmt.training - Epoch  10, Step:   208000, Batch Loss:     1.422897, Tokens per Sec:     8273, Lr: 0.000500
2021-12-19 20:23:06,971 - INFO - joeynmt.training - Epoch  10, Step:   208200, Batch Loss:     1.657073, Tokens per Sec:     8361, Lr: 0.000500
2021-12-19 20:24:08,628 - INFO - joeynmt.training - Epoch  10, Step:   208400, Batch Loss:     1.447345, Tokens per Sec:     8265, Lr: 0.000500
2021-12-19 20:25:10,567 - INFO - joeynmt.training - Epoch  10, Step:   208600, Batch Loss:     1.489293, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 20:26:12,880 - INFO - joeynmt.training - Epoch  10, Step:   208800, Batch Loss:     1.506909, Tokens per Sec:     8484, Lr: 0.000500
2021-12-19 20:27:14,452 - INFO - joeynmt.training - Epoch  10, Step:   209000, Batch Loss:     1.486608, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 20:28:16,073 - INFO - joeynmt.training - Epoch  10, Step:   209200, Batch Loss:     1.732200, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 20:29:18,213 - INFO - joeynmt.training - Epoch  10, Step:   209400, Batch Loss:     1.663147, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 20:30:19,762 - INFO - joeynmt.training - Epoch  10, Step:   209600, Batch Loss:     1.447020, Tokens per Sec:     8309, Lr: 0.000500
2021-12-19 20:31:22,011 - INFO - joeynmt.training - Epoch  10, Step:   209800, Batch Loss:     1.651160, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 20:32:24,007 - INFO - joeynmt.training - Epoch  10, Step:   210000, Batch Loss:     1.534740, Tokens per Sec:     8362, Lr: 0.000500
2021-12-19 20:34:57,206 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 20:34:57,206 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 20:34:59,651 - INFO - joeynmt.training - Example #0
2021-12-19 20:34:59,652 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 20:34:59,652 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 'O@@', 'ba@@', 'ma@@', "'s", 're-@@', 'election']
2021-12-19 20:34:59,652 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 20:34:59,652 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 20:34:59,653 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the Obama's re-election
2021-12-19 20:34:59,653 - INFO - joeynmt.training - Example #1
2021-12-19 20:34:59,653 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 20:34:59,653 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 20:34:59,653 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 20:34:59,653 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 20:34:59,653 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 20:34:59,653 - INFO - joeynmt.training - Example #2
2021-12-19 20:34:59,654 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 20:34:59,654 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 20:34:59,654 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 20:34:59,654 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 20:34:59,654 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 20:34:59,654 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   210000: bleu:  18.96, loss: 174952.4062, ppl:   9.3828, duration: 155.6469s
2021-12-19 20:36:01,968 - INFO - joeynmt.training - Epoch  10, Step:   210200, Batch Loss:     1.556334, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 20:37:03,998 - INFO - joeynmt.training - Epoch  10, Step:   210400, Batch Loss:     1.101981, Tokens per Sec:     8306, Lr: 0.000500
2021-12-19 20:38:05,628 - INFO - joeynmt.training - Epoch  10, Step:   210600, Batch Loss:     1.675572, Tokens per Sec:     8283, Lr: 0.000500
2021-12-19 20:39:07,873 - INFO - joeynmt.training - Epoch  10, Step:   210800, Batch Loss:     1.622997, Tokens per Sec:     8339, Lr: 0.000500
2021-12-19 20:40:09,803 - INFO - joeynmt.training - Epoch  10, Step:   211000, Batch Loss:     1.348811, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 20:41:11,909 - INFO - joeynmt.training - Epoch  10, Step:   211200, Batch Loss:     1.234433, Tokens per Sec:     8404, Lr: 0.000500
2021-12-19 20:42:14,492 - INFO - joeynmt.training - Epoch  10, Step:   211400, Batch Loss:     1.482054, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 20:43:16,907 - INFO - joeynmt.training - Epoch  10, Step:   211600, Batch Loss:     1.673422, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 20:44:19,174 - INFO - joeynmt.training - Epoch  10, Step:   211800, Batch Loss:     1.417439, Tokens per Sec:     8357, Lr: 0.000500
2021-12-19 20:45:21,949 - INFO - joeynmt.training - Epoch  10, Step:   212000, Batch Loss:     1.611196, Tokens per Sec:     8444, Lr: 0.000500
2021-12-19 20:46:23,539 - INFO - joeynmt.training - Epoch  10, Step:   212200, Batch Loss:     1.479624, Tokens per Sec:     8259, Lr: 0.000500
2021-12-19 20:47:25,652 - INFO - joeynmt.training - Epoch  10, Step:   212400, Batch Loss:     1.481960, Tokens per Sec:     8401, Lr: 0.000500
2021-12-19 20:48:27,698 - INFO - joeynmt.training - Epoch  10, Step:   212600, Batch Loss:     1.547089, Tokens per Sec:     8344, Lr: 0.000500
2021-12-19 20:49:29,880 - INFO - joeynmt.training - Epoch  10, Step:   212800, Batch Loss:     1.505757, Tokens per Sec:     8413, Lr: 0.000500
2021-12-19 20:50:31,582 - INFO - joeynmt.training - Epoch  10, Step:   213000, Batch Loss:     1.457357, Tokens per Sec:     8367, Lr: 0.000500
2021-12-19 20:51:33,784 - INFO - joeynmt.training - Epoch  10, Step:   213200, Batch Loss:     1.478398, Tokens per Sec:     8457, Lr: 0.000500
2021-12-19 20:52:34,824 - INFO - joeynmt.training - Epoch  10, Step:   213400, Batch Loss:     1.636189, Tokens per Sec:     8173, Lr: 0.000500
2021-12-19 20:53:36,929 - INFO - joeynmt.training - Epoch  10, Step:   213600, Batch Loss:     1.243111, Tokens per Sec:     8443, Lr: 0.000500
2021-12-19 20:54:39,156 - INFO - joeynmt.training - Epoch  10, Step:   213800, Batch Loss:     1.399618, Tokens per Sec:     8360, Lr: 0.000500
2021-12-19 20:55:41,418 - INFO - joeynmt.training - Epoch  10, Step:   214000, Batch Loss:     1.381484, Tokens per Sec:     8355, Lr: 0.000500
2021-12-19 20:56:43,431 - INFO - joeynmt.training - Epoch  10, Step:   214200, Batch Loss:     1.379777, Tokens per Sec:     8387, Lr: 0.000500
2021-12-19 20:57:45,762 - INFO - joeynmt.training - Epoch  10, Step:   214400, Batch Loss:     1.433853, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 20:58:47,803 - INFO - joeynmt.training - Epoch  10, Step:   214600, Batch Loss:     0.865507, Tokens per Sec:     8328, Lr: 0.000500
2021-12-19 20:59:50,067 - INFO - joeynmt.training - Epoch  10, Step:   214800, Batch Loss:     1.605583, Tokens per Sec:     8359, Lr: 0.000500
2021-12-19 21:00:51,852 - INFO - joeynmt.training - Epoch  10, Step:   215000, Batch Loss:     1.465051, Tokens per Sec:     8218, Lr: 0.000500
2021-12-19 21:03:15,331 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 21:03:15,331 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 21:03:17,798 - INFO - joeynmt.training - Example #0
2021-12-19 21:03:17,798 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 21:03:17,798 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're@@', 'election', 'of', 'Obama']
2021-12-19 21:03:17,799 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 21:03:17,799 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 21:03:17,799 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the reelection of Obama
2021-12-19 21:03:17,799 - INFO - joeynmt.training - Example #1
2021-12-19 21:03:17,800 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 21:03:17,800 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 21:03:17,800 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 21:03:17,800 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 21:03:17,800 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 21:03:17,801 - INFO - joeynmt.training - Example #2
2021-12-19 21:03:17,801 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 21:03:17,801 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 21:03:17,801 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 21:03:17,802 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 21:03:17,802 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 21:03:17,802 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step   215000: bleu:  18.70, loss: 174438.3750, ppl:   9.3213, duration: 145.9497s
2021-12-19 21:03:58,314 - INFO - joeynmt.training - Epoch  10: total training loss 32383.46
2021-12-19 21:03:58,315 - INFO - joeynmt.training - EPOCH 11
2021-12-19 21:04:23,880 - INFO - joeynmt.training - Epoch  11, Step:   215200, Batch Loss:     1.652833, Tokens per Sec:     7451, Lr: 0.000500
2021-12-19 21:05:25,973 - INFO - joeynmt.training - Epoch  11, Step:   215400, Batch Loss:     1.813552, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 21:06:28,016 - INFO - joeynmt.training - Epoch  11, Step:   215600, Batch Loss:     1.391222, Tokens per Sec:     8331, Lr: 0.000500
2021-12-19 21:07:29,815 - INFO - joeynmt.training - Epoch  11, Step:   215800, Batch Loss:     1.331085, Tokens per Sec:     8392, Lr: 0.000500
2021-12-19 21:08:31,901 - INFO - joeynmt.training - Epoch  11, Step:   216000, Batch Loss:     1.627140, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 21:09:33,969 - INFO - joeynmt.training - Epoch  11, Step:   216200, Batch Loss:     1.462575, Tokens per Sec:     8436, Lr: 0.000500
2021-12-19 21:10:36,319 - INFO - joeynmt.training - Epoch  11, Step:   216400, Batch Loss:     1.660975, Tokens per Sec:     8308, Lr: 0.000500
2021-12-19 21:11:38,213 - INFO - joeynmt.training - Epoch  11, Step:   216600, Batch Loss:     1.431515, Tokens per Sec:     8395, Lr: 0.000500
2021-12-19 21:12:39,575 - INFO - joeynmt.training - Epoch  11, Step:   216800, Batch Loss:     1.740851, Tokens per Sec:     8229, Lr: 0.000500
2021-12-19 21:13:41,793 - INFO - joeynmt.training - Epoch  11, Step:   217000, Batch Loss:     1.558810, Tokens per Sec:     8354, Lr: 0.000500
2021-12-19 21:14:43,471 - INFO - joeynmt.training - Epoch  11, Step:   217200, Batch Loss:     1.368056, Tokens per Sec:     8310, Lr: 0.000500
2021-12-19 21:15:45,735 - INFO - joeynmt.training - Epoch  11, Step:   217400, Batch Loss:     1.721829, Tokens per Sec:     8377, Lr: 0.000500
2021-12-19 21:16:48,032 - INFO - joeynmt.training - Epoch  11, Step:   217600, Batch Loss:     1.642292, Tokens per Sec:     8419, Lr: 0.000500
2021-12-19 21:17:49,610 - INFO - joeynmt.training - Epoch  11, Step:   217800, Batch Loss:     1.604352, Tokens per Sec:     8310, Lr: 0.000500
2021-12-19 21:18:51,769 - INFO - joeynmt.training - Epoch  11, Step:   218000, Batch Loss:     1.530863, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 21:19:54,028 - INFO - joeynmt.training - Epoch  11, Step:   218200, Batch Loss:     1.513081, Tokens per Sec:     8414, Lr: 0.000500
2021-12-19 21:20:55,933 - INFO - joeynmt.training - Epoch  11, Step:   218400, Batch Loss:     1.686527, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 21:21:58,110 - INFO - joeynmt.training - Epoch  11, Step:   218600, Batch Loss:     1.365843, Tokens per Sec:     8320, Lr: 0.000500
2021-12-19 21:23:00,084 - INFO - joeynmt.training - Epoch  11, Step:   218800, Batch Loss:     1.414136, Tokens per Sec:     8336, Lr: 0.000500
2021-12-19 21:24:02,553 - INFO - joeynmt.training - Epoch  11, Step:   219000, Batch Loss:     1.552577, Tokens per Sec:     8419, Lr: 0.000500
2021-12-19 21:25:05,077 - INFO - joeynmt.training - Epoch  11, Step:   219200, Batch Loss:     1.192300, Tokens per Sec:     8485, Lr: 0.000500
2021-12-19 21:26:06,945 - INFO - joeynmt.training - Epoch  11, Step:   219400, Batch Loss:     1.514658, Tokens per Sec:     8382, Lr: 0.000500
2021-12-19 21:27:09,455 - INFO - joeynmt.training - Epoch  11, Step:   219600, Batch Loss:     1.511179, Tokens per Sec:     8453, Lr: 0.000500
2021-12-19 21:28:11,728 - INFO - joeynmt.training - Epoch  11, Step:   219800, Batch Loss:     1.619418, Tokens per Sec:     8348, Lr: 0.000500
2021-12-19 21:29:13,524 - INFO - joeynmt.training - Epoch  11, Step:   220000, Batch Loss:     1.522653, Tokens per Sec:     8328, Lr: 0.000500
2021-12-19 21:31:38,824 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 21:31:38,825 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 21:31:41,414 - INFO - joeynmt.training - Example #0
2021-12-19 21:31:41,414 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 21:31:41,414 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 21:31:41,414 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 21:31:41,415 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 21:31:41,415 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 21:31:41,415 - INFO - joeynmt.training - Example #1
2021-12-19 21:31:41,415 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 21:31:41,415 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 21:31:41,415 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 21:31:41,420 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 21:31:41,421 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 21:31:41,421 - INFO - joeynmt.training - Example #2
2021-12-19 21:31:41,421 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 21:31:41,421 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'claiming', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'pl@@', 'ant.']
2021-12-19 21:31:41,421 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 21:31:41,421 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 21:31:41,421 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, claiming that electoral fraud is more rare in the United States than the number of people killed by the plant.
2021-12-19 21:31:41,422 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   220000: bleu:  18.87, loss: 174046.2344, ppl:   9.2746, duration: 147.8969s
2021-12-19 21:32:44,119 - INFO - joeynmt.training - Epoch  11, Step:   220200, Batch Loss:     1.350196, Tokens per Sec:     8376, Lr: 0.000500
2021-12-19 21:33:46,154 - INFO - joeynmt.training - Epoch  11, Step:   220400, Batch Loss:     1.629821, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 21:34:48,215 - INFO - joeynmt.training - Epoch  11, Step:   220600, Batch Loss:     1.505736, Tokens per Sec:     8384, Lr: 0.000500
2021-12-19 21:35:49,965 - INFO - joeynmt.training - Epoch  11, Step:   220800, Batch Loss:     1.532483, Tokens per Sec:     8345, Lr: 0.000500
2021-12-19 21:36:52,230 - INFO - joeynmt.training - Epoch  11, Step:   221000, Batch Loss:     1.578541, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 21:37:54,357 - INFO - joeynmt.training - Epoch  11, Step:   221200, Batch Loss:     1.567705, Tokens per Sec:     8313, Lr: 0.000500
2021-12-19 21:38:56,698 - INFO - joeynmt.training - Epoch  11, Step:   221400, Batch Loss:     1.667275, Tokens per Sec:     8375, Lr: 0.000500
2021-12-19 21:39:58,500 - INFO - joeynmt.training - Epoch  11, Step:   221600, Batch Loss:     1.646319, Tokens per Sec:     8373, Lr: 0.000500
2021-12-19 21:41:00,582 - INFO - joeynmt.training - Epoch  11, Step:   221800, Batch Loss:     1.417944, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 21:42:02,404 - INFO - joeynmt.training - Epoch  11, Step:   222000, Batch Loss:     1.436882, Tokens per Sec:     8318, Lr: 0.000500
2021-12-19 21:43:04,519 - INFO - joeynmt.training - Epoch  11, Step:   222200, Batch Loss:     1.494383, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 21:44:06,723 - INFO - joeynmt.training - Epoch  11, Step:   222400, Batch Loss:     1.515891, Tokens per Sec:     8384, Lr: 0.000500
2021-12-19 21:45:08,317 - INFO - joeynmt.training - Epoch  11, Step:   222600, Batch Loss:     1.438574, Tokens per Sec:     8366, Lr: 0.000500
2021-12-19 21:46:10,344 - INFO - joeynmt.training - Epoch  11, Step:   222800, Batch Loss:     1.537194, Tokens per Sec:     8447, Lr: 0.000500
2021-12-19 21:47:12,334 - INFO - joeynmt.training - Epoch  11, Step:   223000, Batch Loss:     1.605474, Tokens per Sec:     8318, Lr: 0.000500
2021-12-19 21:48:14,255 - INFO - joeynmt.training - Epoch  11, Step:   223200, Batch Loss:     1.582774, Tokens per Sec:     8428, Lr: 0.000500
2021-12-19 21:49:16,317 - INFO - joeynmt.training - Epoch  11, Step:   223400, Batch Loss:     1.612366, Tokens per Sec:     8357, Lr: 0.000500
2021-12-19 21:50:17,728 - INFO - joeynmt.training - Epoch  11, Step:   223600, Batch Loss:     1.649459, Tokens per Sec:     8255, Lr: 0.000500
2021-12-19 21:51:19,834 - INFO - joeynmt.training - Epoch  11, Step:   223800, Batch Loss:     1.680224, Tokens per Sec:     8280, Lr: 0.000500
2021-12-19 21:52:21,809 - INFO - joeynmt.training - Epoch  11, Step:   224000, Batch Loss:     1.440635, Tokens per Sec:     8440, Lr: 0.000500
2021-12-19 21:53:24,407 - INFO - joeynmt.training - Epoch  11, Step:   224200, Batch Loss:     1.543539, Tokens per Sec:     8440, Lr: 0.000500
2021-12-19 21:54:26,166 - INFO - joeynmt.training - Epoch  11, Step:   224400, Batch Loss:     1.635512, Tokens per Sec:     8305, Lr: 0.000500
2021-12-19 21:55:28,538 - INFO - joeynmt.training - Epoch  11, Step:   224600, Batch Loss:     1.517591, Tokens per Sec:     8402, Lr: 0.000500
2021-12-19 21:56:29,895 - INFO - joeynmt.training - Epoch  11, Step:   224800, Batch Loss:     1.473642, Tokens per Sec:     8269, Lr: 0.000500
2021-12-19 21:57:31,697 - INFO - joeynmt.training - Epoch  11, Step:   225000, Batch Loss:     1.573662, Tokens per Sec:     8374, Lr: 0.000500
2021-12-19 21:59:54,977 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 21:59:54,977 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 21:59:59,587 - INFO - joeynmt.training - Example #0
2021-12-19 21:59:59,587 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 21:59:59,588 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 21:59:59,588 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 21:59:59,588 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 21:59:59,588 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 21:59:59,589 - INFO - joeynmt.training - Example #1
2021-12-19 21:59:59,589 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 21:59:59,589 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 21:59:59,589 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 21:59:59,589 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 21:59:59,590 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 21:59:59,590 - INFO - joeynmt.training - Example #2
2021-12-19 21:59:59,590 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 21:59:59,590 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'ner', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'stating', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'pl@@', 'ant.']
2021-12-19 21:59:59,590 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 21:59:59,590 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 21:59:59,590 - INFO - joeynmt.training - 	Hypothesis: The Brenner Centre considers this to be a mythy, stating that electoral fraud is more rare in the United States than the number of people killed by the plant.
2021-12-19 21:59:59,591 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   225000: bleu:  18.99, loss: 173116.0000, ppl:   9.1648, duration: 147.8930s
2021-12-19 22:01:01,617 - INFO - joeynmt.training - Epoch  11, Step:   225200, Batch Loss:     1.412401, Tokens per Sec:     8314, Lr: 0.000500
2021-12-19 22:02:03,600 - INFO - joeynmt.training - Epoch  11, Step:   225400, Batch Loss:     1.509033, Tokens per Sec:     8351, Lr: 0.000500
2021-12-19 22:03:05,507 - INFO - joeynmt.training - Epoch  11, Step:   225600, Batch Loss:     1.424114, Tokens per Sec:     8328, Lr: 0.000500
2021-12-19 22:04:07,708 - INFO - joeynmt.training - Epoch  11, Step:   225800, Batch Loss:     1.641473, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 22:05:09,543 - INFO - joeynmt.training - Epoch  11, Step:   226000, Batch Loss:     1.469960, Tokens per Sec:     8340, Lr: 0.000500
2021-12-19 22:06:10,962 - INFO - joeynmt.training - Epoch  11, Step:   226200, Batch Loss:     0.829946, Tokens per Sec:     8317, Lr: 0.000500
2021-12-19 22:07:12,782 - INFO - joeynmt.training - Epoch  11, Step:   226400, Batch Loss:     1.212915, Tokens per Sec:     8255, Lr: 0.000500
2021-12-19 22:08:14,646 - INFO - joeynmt.training - Epoch  11, Step:   226600, Batch Loss:     1.387154, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 22:09:17,198 - INFO - joeynmt.training - Epoch  11, Step:   226800, Batch Loss:     1.558691, Tokens per Sec:     8418, Lr: 0.000500
2021-12-19 22:10:19,576 - INFO - joeynmt.training - Epoch  11, Step:   227000, Batch Loss:     1.449156, Tokens per Sec:     8517, Lr: 0.000500
2021-12-19 22:11:21,864 - INFO - joeynmt.training - Epoch  11, Step:   227200, Batch Loss:     1.559668, Tokens per Sec:     8318, Lr: 0.000500
2021-12-19 22:12:23,418 - INFO - joeynmt.training - Epoch  11, Step:   227400, Batch Loss:     1.544121, Tokens per Sec:     8286, Lr: 0.000500
2021-12-19 22:13:25,376 - INFO - joeynmt.training - Epoch  11, Step:   227600, Batch Loss:     1.807275, Tokens per Sec:     8417, Lr: 0.000500
2021-12-19 22:14:27,592 - INFO - joeynmt.training - Epoch  11, Step:   227800, Batch Loss:     1.456194, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 22:15:29,052 - INFO - joeynmt.training - Epoch  11, Step:   228000, Batch Loss:     1.615990, Tokens per Sec:     8294, Lr: 0.000500
2021-12-19 22:16:31,525 - INFO - joeynmt.training - Epoch  11, Step:   228200, Batch Loss:     1.444201, Tokens per Sec:     8392, Lr: 0.000500
2021-12-19 22:17:33,438 - INFO - joeynmt.training - Epoch  11, Step:   228400, Batch Loss:     1.595274, Tokens per Sec:     8358, Lr: 0.000500
2021-12-19 22:18:34,929 - INFO - joeynmt.training - Epoch  11, Step:   228600, Batch Loss:     1.457805, Tokens per Sec:     8267, Lr: 0.000500
2021-12-19 22:19:36,568 - INFO - joeynmt.training - Epoch  11, Step:   228800, Batch Loss:     1.451037, Tokens per Sec:     8393, Lr: 0.000500
2021-12-19 22:20:38,571 - INFO - joeynmt.training - Epoch  11, Step:   229000, Batch Loss:     1.524178, Tokens per Sec:     8350, Lr: 0.000500
2021-12-19 22:21:40,668 - INFO - joeynmt.training - Epoch  11, Step:   229200, Batch Loss:     1.081461, Tokens per Sec:     8305, Lr: 0.000500
2021-12-19 22:22:42,540 - INFO - joeynmt.training - Epoch  11, Step:   229400, Batch Loss:     1.571822, Tokens per Sec:     8377, Lr: 0.000500
2021-12-19 22:23:45,045 - INFO - joeynmt.training - Epoch  11, Step:   229600, Batch Loss:     1.612392, Tokens per Sec:     8361, Lr: 0.000500
2021-12-19 22:24:47,399 - INFO - joeynmt.training - Epoch  11, Step:   229800, Batch Loss:     1.694779, Tokens per Sec:     8336, Lr: 0.000500
2021-12-19 22:25:49,826 - INFO - joeynmt.training - Epoch  11, Step:   230000, Batch Loss:     1.388360, Tokens per Sec:     8388, Lr: 0.000500
2021-12-19 22:28:11,113 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 22:28:11,114 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 22:28:15,722 - INFO - joeynmt.training - Example #0
2021-12-19 22:28:15,723 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 22:28:15,723 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 22:28:15,723 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 22:28:15,723 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 22:28:15,724 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 22:28:15,724 - INFO - joeynmt.training - Example #1
2021-12-19 22:28:15,724 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 22:28:15,724 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 22:28:15,724 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 22:28:15,724 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 22:28:15,724 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 22:28:15,724 - INFO - joeynmt.training - Example #2
2021-12-19 22:28:15,725 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 22:28:15,725 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'sees', 'this', 'as', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 22:28:15,725 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 22:28:15,725 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 22:28:15,725 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre sees this as a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 22:28:15,725 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   230000: bleu:  18.95, loss: 172502.4219, ppl:   9.0932, duration: 145.8991s
2021-12-19 22:29:17,419 - INFO - joeynmt.training - Epoch  11, Step:   230200, Batch Loss:     1.598698, Tokens per Sec:     8254, Lr: 0.000500
2021-12-19 22:30:19,225 - INFO - joeynmt.training - Epoch  11, Step:   230400, Batch Loss:     1.601669, Tokens per Sec:     8227, Lr: 0.000500
2021-12-19 22:31:21,471 - INFO - joeynmt.training - Epoch  11, Step:   230600, Batch Loss:     1.574503, Tokens per Sec:     8445, Lr: 0.000500
2021-12-19 22:32:23,296 - INFO - joeynmt.training - Epoch  11, Step:   230800, Batch Loss:     1.563947, Tokens per Sec:     8311, Lr: 0.000500
2021-12-19 22:33:25,199 - INFO - joeynmt.training - Epoch  11, Step:   231000, Batch Loss:     1.509898, Tokens per Sec:     8270, Lr: 0.000500
2021-12-19 22:34:27,486 - INFO - joeynmt.training - Epoch  11, Step:   231200, Batch Loss:     1.721629, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 22:35:29,817 - INFO - joeynmt.training - Epoch  11, Step:   231400, Batch Loss:     1.436800, Tokens per Sec:     8379, Lr: 0.000500
2021-12-19 22:36:31,991 - INFO - joeynmt.training - Epoch  11, Step:   231600, Batch Loss:     1.566870, Tokens per Sec:     8315, Lr: 0.000500
2021-12-19 22:37:33,601 - INFO - joeynmt.training - Epoch  11, Step:   231800, Batch Loss:     1.478296, Tokens per Sec:     8290, Lr: 0.000500
2021-12-19 22:38:34,954 - INFO - joeynmt.training - Epoch  11, Step:   232000, Batch Loss:     1.421444, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 22:39:36,610 - INFO - joeynmt.training - Epoch  11, Step:   232200, Batch Loss:     1.549926, Tokens per Sec:     8291, Lr: 0.000500
2021-12-19 22:40:38,646 - INFO - joeynmt.training - Epoch  11, Step:   232400, Batch Loss:     1.471848, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 22:41:40,409 - INFO - joeynmt.training - Epoch  11, Step:   232600, Batch Loss:     1.583952, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 22:42:41,937 - INFO - joeynmt.training - Epoch  11, Step:   232800, Batch Loss:     1.565166, Tokens per Sec:     8312, Lr: 0.000500
2021-12-19 22:43:43,766 - INFO - joeynmt.training - Epoch  11, Step:   233000, Batch Loss:     1.587025, Tokens per Sec:     8347, Lr: 0.000500
2021-12-19 22:44:45,783 - INFO - joeynmt.training - Epoch  11, Step:   233200, Batch Loss:     1.408728, Tokens per Sec:     8317, Lr: 0.000500
2021-12-19 22:45:47,899 - INFO - joeynmt.training - Epoch  11, Step:   233400, Batch Loss:     1.398952, Tokens per Sec:     8329, Lr: 0.000500
2021-12-19 22:46:49,938 - INFO - joeynmt.training - Epoch  11, Step:   233600, Batch Loss:     1.202900, Tokens per Sec:     8381, Lr: 0.000500
2021-12-19 22:47:52,166 - INFO - joeynmt.training - Epoch  11, Step:   233800, Batch Loss:     1.438476, Tokens per Sec:     8405, Lr: 0.000500
2021-12-19 22:48:54,098 - INFO - joeynmt.training - Epoch  11, Step:   234000, Batch Loss:     1.543089, Tokens per Sec:     8412, Lr: 0.000500
2021-12-19 22:49:56,007 - INFO - joeynmt.training - Epoch  11, Step:   234200, Batch Loss:     1.648876, Tokens per Sec:     8335, Lr: 0.000500
2021-12-19 22:50:58,069 - INFO - joeynmt.training - Epoch  11, Step:   234400, Batch Loss:     1.426505, Tokens per Sec:     8440, Lr: 0.000500
2021-12-19 22:52:00,379 - INFO - joeynmt.training - Epoch  11, Step:   234600, Batch Loss:     1.685219, Tokens per Sec:     8325, Lr: 0.000500
2021-12-19 22:53:02,516 - INFO - joeynmt.training - Epoch  11, Step:   234800, Batch Loss:     1.349946, Tokens per Sec:     8305, Lr: 0.000500
2021-12-19 22:54:04,112 - INFO - joeynmt.training - Epoch  11, Step:   235000, Batch Loss:     1.520543, Tokens per Sec:     8261, Lr: 0.000500
2021-12-19 22:56:31,324 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 22:56:31,325 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 22:56:33,844 - INFO - joeynmt.training - Example #0
2021-12-19 22:56:33,844 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 22:56:33,844 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 22:56:33,845 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 22:56:33,845 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 22:56:33,845 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 22:56:33,845 - INFO - joeynmt.training - Example #1
2021-12-19 22:56:33,845 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 22:56:33,845 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'cl@@', 'y-@@', 're@@', 'public', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 22:56:33,846 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 22:56:33,846 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 22:56:33,846 - INFO - joeynmt.training - 	Hypothesis: Republicly-republic leaders justify their policy by the need to combat electoral fraud.
2021-12-19 22:56:33,846 - INFO - joeynmt.training - Example #2
2021-12-19 22:56:33,846 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 22:56:33,846 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'claiming', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'crow@@', 'd.']
2021-12-19 22:56:33,847 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 22:56:33,847 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 22:56:33,847 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, claiming that electoral fraud is more rare in the United States than the number of people killed by the crowd.
2021-12-19 22:56:33,847 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step   235000: bleu:  19.19, loss: 172315.6094, ppl:   9.0714, duration: 149.7344s
2021-12-19 22:57:36,256 - INFO - joeynmt.training - Epoch  11, Step:   235200, Batch Loss:     1.561569, Tokens per Sec:     8264, Lr: 0.000500
2021-12-19 22:58:38,260 - INFO - joeynmt.training - Epoch  11, Step:   235400, Batch Loss:     1.463508, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 22:59:40,741 - INFO - joeynmt.training - Epoch  11, Step:   235600, Batch Loss:     1.329052, Tokens per Sec:     8436, Lr: 0.000500
2021-12-19 23:00:42,894 - INFO - joeynmt.training - Epoch  11, Step:   235800, Batch Loss:     1.410434, Tokens per Sec:     8426, Lr: 0.000500
2021-12-19 23:01:45,086 - INFO - joeynmt.training - Epoch  11, Step:   236000, Batch Loss:     1.673701, Tokens per Sec:     8440, Lr: 0.000500
2021-12-19 23:02:47,456 - INFO - joeynmt.training - Epoch  11, Step:   236200, Batch Loss:     1.436329, Tokens per Sec:     8445, Lr: 0.000500
2021-12-19 23:03:49,495 - INFO - joeynmt.training - Epoch  11, Step:   236400, Batch Loss:     1.521531, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 23:04:51,752 - INFO - joeynmt.training - Epoch  11, Step:   236600, Batch Loss:     1.407832, Tokens per Sec:     8367, Lr: 0.000500
2021-12-19 23:05:06,339 - INFO - joeynmt.training - Epoch  11: total training loss 32008.29
2021-12-19 23:05:06,339 - INFO - joeynmt.training - EPOCH 12
2021-12-19 23:05:56,886 - INFO - joeynmt.training - Epoch  12, Step:   236800, Batch Loss:     1.518416, Tokens per Sec:     7701, Lr: 0.000500
2021-12-19 23:06:59,332 - INFO - joeynmt.training - Epoch  12, Step:   237000, Batch Loss:     1.833393, Tokens per Sec:     8349, Lr: 0.000500
2021-12-19 23:08:01,942 - INFO - joeynmt.training - Epoch  12, Step:   237200, Batch Loss:     1.545060, Tokens per Sec:     8457, Lr: 0.000500
2021-12-19 23:09:03,357 - INFO - joeynmt.training - Epoch  12, Step:   237400, Batch Loss:     1.514992, Tokens per Sec:     8280, Lr: 0.000500
2021-12-19 23:10:05,486 - INFO - joeynmt.training - Epoch  12, Step:   237600, Batch Loss:     1.143069, Tokens per Sec:     8370, Lr: 0.000500
2021-12-19 23:11:08,027 - INFO - joeynmt.training - Epoch  12, Step:   237800, Batch Loss:     1.581962, Tokens per Sec:     8487, Lr: 0.000500
2021-12-19 23:12:10,337 - INFO - joeynmt.training - Epoch  12, Step:   238000, Batch Loss:     1.709031, Tokens per Sec:     8390, Lr: 0.000500
2021-12-19 23:13:12,446 - INFO - joeynmt.training - Epoch  12, Step:   238200, Batch Loss:     1.617235, Tokens per Sec:     8324, Lr: 0.000500
2021-12-19 23:14:14,678 - INFO - joeynmt.training - Epoch  12, Step:   238400, Batch Loss:     1.339924, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 23:15:16,123 - INFO - joeynmt.training - Epoch  12, Step:   238600, Batch Loss:     1.491926, Tokens per Sec:     8346, Lr: 0.000500
2021-12-19 23:16:18,358 - INFO - joeynmt.training - Epoch  12, Step:   238800, Batch Loss:     1.375445, Tokens per Sec:     8458, Lr: 0.000500
2021-12-19 23:17:20,467 - INFO - joeynmt.training - Epoch  12, Step:   239000, Batch Loss:     1.278416, Tokens per Sec:     8411, Lr: 0.000500
2021-12-19 23:18:22,227 - INFO - joeynmt.training - Epoch  12, Step:   239200, Batch Loss:     1.491347, Tokens per Sec:     8441, Lr: 0.000500
2021-12-19 23:19:24,022 - INFO - joeynmt.training - Epoch  12, Step:   239400, Batch Loss:     1.570854, Tokens per Sec:     8315, Lr: 0.000500
2021-12-19 23:20:25,493 - INFO - joeynmt.training - Epoch  12, Step:   239600, Batch Loss:     1.488746, Tokens per Sec:     8327, Lr: 0.000500
2021-12-19 23:21:27,507 - INFO - joeynmt.training - Epoch  12, Step:   239800, Batch Loss:     1.424402, Tokens per Sec:     8418, Lr: 0.000500
2021-12-19 23:22:29,629 - INFO - joeynmt.training - Epoch  12, Step:   240000, Batch Loss:     1.504419, Tokens per Sec:     8324, Lr: 0.000500
2021-12-19 23:24:55,139 - INFO - joeynmt.training - Example #0
2021-12-19 23:24:55,139 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 23:24:55,139 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 23:24:55,140 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 23:24:55,140 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 23:24:55,140 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 23:24:55,140 - INFO - joeynmt.training - Example #1
2021-12-19 23:24:55,140 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 23:24:55,140 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 23:24:55,140 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 23:24:55,141 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 23:24:55,141 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 23:24:55,141 - INFO - joeynmt.training - Example #2
2021-12-19 23:24:55,141 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 23:24:55,141 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'claiming', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'mass', 'fraud.']
2021-12-19 23:24:55,141 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 23:24:55,141 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 23:24:55,141 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, claiming that electoral fraud is more rare in the United States than the number of people killed by the mass fraud.
2021-12-19 23:24:55,142 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   240000: bleu:  19.05, loss: 172339.4844, ppl:   9.0742, duration: 145.5126s
2021-12-19 23:25:56,872 - INFO - joeynmt.training - Epoch  12, Step:   240200, Batch Loss:     1.516003, Tokens per Sec:     8299, Lr: 0.000500
2021-12-19 23:26:58,876 - INFO - joeynmt.training - Epoch  12, Step:   240400, Batch Loss:     1.529318, Tokens per Sec:     8333, Lr: 0.000500
2021-12-19 23:28:00,243 - INFO - joeynmt.training - Epoch  12, Step:   240600, Batch Loss:     1.700706, Tokens per Sec:     8194, Lr: 0.000500
2021-12-19 23:29:01,881 - INFO - joeynmt.training - Epoch  12, Step:   240800, Batch Loss:     1.511654, Tokens per Sec:     8352, Lr: 0.000500
2021-12-19 23:30:04,140 - INFO - joeynmt.training - Epoch  12, Step:   241000, Batch Loss:     1.410160, Tokens per Sec:     8274, Lr: 0.000500
2021-12-19 23:31:06,355 - INFO - joeynmt.training - Epoch  12, Step:   241200, Batch Loss:     1.548761, Tokens per Sec:     8304, Lr: 0.000500
2021-12-19 23:32:08,379 - INFO - joeynmt.training - Epoch  12, Step:   241400, Batch Loss:     1.620539, Tokens per Sec:     8385, Lr: 0.000500
2021-12-19 23:33:10,937 - INFO - joeynmt.training - Epoch  12, Step:   241600, Batch Loss:     1.672443, Tokens per Sec:     8416, Lr: 0.000500
2021-12-19 23:34:13,285 - INFO - joeynmt.training - Epoch  12, Step:   241800, Batch Loss:     1.378290, Tokens per Sec:     8456, Lr: 0.000500
2021-12-19 23:35:15,303 - INFO - joeynmt.training - Epoch  12, Step:   242000, Batch Loss:     1.495499, Tokens per Sec:     8446, Lr: 0.000500
2021-12-19 23:36:17,277 - INFO - joeynmt.training - Epoch  12, Step:   242200, Batch Loss:     1.500287, Tokens per Sec:     8324, Lr: 0.000500
2021-12-19 23:37:19,407 - INFO - joeynmt.training - Epoch  12, Step:   242400, Batch Loss:     1.550728, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 23:38:21,410 - INFO - joeynmt.training - Epoch  12, Step:   242600, Batch Loss:     1.395779, Tokens per Sec:     8292, Lr: 0.000500
2021-12-19 23:39:23,174 - INFO - joeynmt.training - Epoch  12, Step:   242800, Batch Loss:     1.563932, Tokens per Sec:     8364, Lr: 0.000500
2021-12-19 23:40:25,148 - INFO - joeynmt.training - Epoch  12, Step:   243000, Batch Loss:     1.713779, Tokens per Sec:     8371, Lr: 0.000500
2021-12-19 23:41:27,528 - INFO - joeynmt.training - Epoch  12, Step:   243200, Batch Loss:     1.274940, Tokens per Sec:     8407, Lr: 0.000500
2021-12-19 23:42:29,879 - INFO - joeynmt.training - Epoch  12, Step:   243400, Batch Loss:     1.420596, Tokens per Sec:     8428, Lr: 0.000500
2021-12-19 23:43:31,652 - INFO - joeynmt.training - Epoch  12, Step:   243600, Batch Loss:     1.338664, Tokens per Sec:     8411, Lr: 0.000500
2021-12-19 23:44:33,698 - INFO - joeynmt.training - Epoch  12, Step:   243800, Batch Loss:     1.332751, Tokens per Sec:     8322, Lr: 0.000500
2021-12-19 23:45:35,931 - INFO - joeynmt.training - Epoch  12, Step:   244000, Batch Loss:     1.690158, Tokens per Sec:     8301, Lr: 0.000500
2021-12-19 23:46:38,048 - INFO - joeynmt.training - Epoch  12, Step:   244200, Batch Loss:     1.432535, Tokens per Sec:     8372, Lr: 0.000500
2021-12-19 23:47:39,933 - INFO - joeynmt.training - Epoch  12, Step:   244400, Batch Loss:     1.490531, Tokens per Sec:     8318, Lr: 0.000500
2021-12-19 23:48:41,873 - INFO - joeynmt.training - Epoch  12, Step:   244600, Batch Loss:     1.387465, Tokens per Sec:     8371, Lr: 0.000500
2021-12-19 23:49:43,738 - INFO - joeynmt.training - Epoch  12, Step:   244800, Batch Loss:     1.459368, Tokens per Sec:     8334, Lr: 0.000500
2021-12-19 23:50:45,979 - INFO - joeynmt.training - Epoch  12, Step:   245000, Batch Loss:     1.475480, Tokens per Sec:     8365, Lr: 0.000500
2021-12-19 23:53:10,878 - INFO - joeynmt.training - Hooray! New best validation result [loss]!
2021-12-19 23:53:10,879 - INFO - joeynmt.training - Saving new checkpoint.
2021-12-19 23:53:13,461 - INFO - joeynmt.training - Example #0
2021-12-19 23:53:13,462 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-19 23:53:13,462 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-19 23:53:13,462 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-19 23:53:13,462 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-19 23:53:13,462 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-19 23:53:13,463 - INFO - joeynmt.training - Example #1
2021-12-19 23:53:13,463 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-19 23:53:13,463 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-19 23:53:13,463 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-19 23:53:13,463 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-19 23:53:13,463 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-19 23:53:13,464 - INFO - joeynmt.training - Example #2
2021-12-19 23:53:13,464 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-19 23:53:13,464 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'saying', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'mass', 'of', 'people.']
2021-12-19 23:53:13,464 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-19 23:53:13,464 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-19 23:53:13,464 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, saying that electoral fraud is more rare in the United States than the number of people killed by the mass of people.
2021-12-19 23:53:13,465 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   245000: bleu:  19.18, loss: 171859.8594, ppl:   9.0187, duration: 147.4854s
2021-12-19 23:54:15,816 - INFO - joeynmt.training - Epoch  12, Step:   245200, Batch Loss:     1.570724, Tokens per Sec:     8383, Lr: 0.000500
2021-12-19 23:55:17,659 - INFO - joeynmt.training - Epoch  12, Step:   245400, Batch Loss:     1.371269, Tokens per Sec:     8389, Lr: 0.000500
2021-12-19 23:56:19,394 - INFO - joeynmt.training - Epoch  12, Step:   245600, Batch Loss:     1.482139, Tokens per Sec:     8402, Lr: 0.000500
2021-12-19 23:57:21,366 - INFO - joeynmt.training - Epoch  12, Step:   245800, Batch Loss:     1.306533, Tokens per Sec:     8338, Lr: 0.000500
2021-12-19 23:58:23,253 - INFO - joeynmt.training - Epoch  12, Step:   246000, Batch Loss:     1.614802, Tokens per Sec:     8359, Lr: 0.000500
2021-12-19 23:59:25,195 - INFO - joeynmt.training - Epoch  12, Step:   246200, Batch Loss:     1.449935, Tokens per Sec:     8320, Lr: 0.000500
2021-12-20 00:00:27,293 - INFO - joeynmt.training - Epoch  12, Step:   246400, Batch Loss:     1.544492, Tokens per Sec:     8390, Lr: 0.000500
2021-12-20 00:01:29,333 - INFO - joeynmt.training - Epoch  12, Step:   246600, Batch Loss:     1.428435, Tokens per Sec:     8402, Lr: 0.000500
2021-12-20 00:02:31,360 - INFO - joeynmt.training - Epoch  12, Step:   246800, Batch Loss:     1.376804, Tokens per Sec:     8355, Lr: 0.000500
2021-12-20 00:03:33,176 - INFO - joeynmt.training - Epoch  12, Step:   247000, Batch Loss:     1.496028, Tokens per Sec:     8322, Lr: 0.000500
2021-12-20 00:04:34,943 - INFO - joeynmt.training - Epoch  12, Step:   247200, Batch Loss:     1.338591, Tokens per Sec:     8364, Lr: 0.000500
2021-12-20 00:05:37,213 - INFO - joeynmt.training - Epoch  12, Step:   247400, Batch Loss:     1.771636, Tokens per Sec:     8496, Lr: 0.000500
2021-12-20 00:06:39,453 - INFO - joeynmt.training - Epoch  12, Step:   247600, Batch Loss:     1.473632, Tokens per Sec:     8339, Lr: 0.000500
2021-12-20 00:07:41,181 - INFO - joeynmt.training - Epoch  12, Step:   247800, Batch Loss:     1.582965, Tokens per Sec:     8341, Lr: 0.000500
2021-12-20 00:08:43,380 - INFO - joeynmt.training - Epoch  12, Step:   248000, Batch Loss:     1.425755, Tokens per Sec:     8324, Lr: 0.000500
2021-12-20 00:09:45,518 - INFO - joeynmt.training - Epoch  12, Step:   248200, Batch Loss:     1.421560, Tokens per Sec:     8399, Lr: 0.000500
2021-12-20 00:10:47,156 - INFO - joeynmt.training - Epoch  12, Step:   248400, Batch Loss:     1.361514, Tokens per Sec:     8336, Lr: 0.000500
2021-12-20 00:11:48,732 - INFO - joeynmt.training - Epoch  12, Step:   248600, Batch Loss:     1.545934, Tokens per Sec:     8339, Lr: 0.000500
2021-12-20 00:12:50,448 - INFO - joeynmt.training - Epoch  12, Step:   248800, Batch Loss:     1.409292, Tokens per Sec:     8392, Lr: 0.000500
2021-12-20 00:13:52,875 - INFO - joeynmt.training - Epoch  12, Step:   249000, Batch Loss:     1.304756, Tokens per Sec:     8389, Lr: 0.000500
2021-12-20 00:14:54,926 - INFO - joeynmt.training - Epoch  12, Step:   249200, Batch Loss:     1.575535, Tokens per Sec:     8390, Lr: 0.000500
2021-12-20 00:15:56,901 - INFO - joeynmt.training - Epoch  12, Step:   249400, Batch Loss:     1.458838, Tokens per Sec:     8413, Lr: 0.000500
2021-12-20 00:16:58,542 - INFO - joeynmt.training - Epoch  12, Step:   249600, Batch Loss:     1.291014, Tokens per Sec:     8330, Lr: 0.000500
2021-12-20 00:18:00,775 - INFO - joeynmt.training - Epoch  12, Step:   249800, Batch Loss:     1.575053, Tokens per Sec:     8372, Lr: 0.000500
2021-12-20 00:19:02,412 - INFO - joeynmt.training - Epoch  12, Step:   250000, Batch Loss:     1.377604, Tokens per Sec:     8279, Lr: 0.000500
2021-12-20 00:21:26,653 - INFO - joeynmt.training - Example #0
2021-12-20 00:21:26,653 - DEBUG - joeynmt.training - 	Raw source:     ['Une', 'stratégie', 'ré@@', 'public@@', 'aine', 'pour', 'contrer', 'la', 'ré@@', 'élection', "d'@@", 'Obama']
2021-12-20 00:21:26,653 - DEBUG - joeynmt.training - 	Raw hypothesis: ['A', 're@@', 'publi@@', 'can', 'strategy', 'to', 'counter', 'the', 're-@@', 'election', 'of', 'Obama']
2021-12-20 00:21:26,654 - INFO - joeynmt.training - 	Source:     Une stratégie républicaine pour contrer la réélection d'Obama
2021-12-20 00:21:26,654 - INFO - joeynmt.training - 	Reference:  A Republican strategy to counter the re-election of Obama
2021-12-20 00:21:26,654 - INFO - joeynmt.training - 	Hypothesis: A republican strategy to counter the re-election of Obama
2021-12-20 00:21:26,654 - INFO - joeynmt.training - Example #1
2021-12-20 00:21:26,654 - DEBUG - joeynmt.training - 	Raw source:     ['Les', 'dirigeants', 'ré@@', 'public@@', 'ains', 'justifi@@', 'èrent', 'leur', 'politique', 'par', 'la', 'nécessité', 'de', 'lutter', 'contre', 'la', 'fraude', 'élector@@', 'ale.']
2021-12-20 00:21:26,654 - DEBUG - joeynmt.training - 	Raw hypothesis: ['Re@@', 'publi@@', 'can', 'leaders', 'justify', 'their', 'policy', 'by', 'the', 'need', 'to', 'combat', 'electoral', 'fraud.']
2021-12-20 00:21:26,655 - INFO - joeynmt.training - 	Source:     Les dirigeants républicains justifièrent leur politique par la nécessité de lutter contre la fraude électorale.
2021-12-20 00:21:26,655 - INFO - joeynmt.training - 	Reference:  Republican leaders justified their policy by the need to combat electoral fraud.
2021-12-20 00:21:26,655 - INFO - joeynmt.training - 	Hypothesis: Republican leaders justify their policy by the need to combat electoral fraud.
2021-12-20 00:21:26,655 - INFO - joeynmt.training - Example #2
2021-12-20 00:21:26,655 - DEBUG - joeynmt.training - 	Raw source:     ['Or,', 'le', 'Centre', 'Bren@@', 'n@@', 'an', 'considère', 'cette', 'dernière', 'comme', 'un', 'my@@', 'th@@', 'e,', 'affirmant', 'que', 'la', 'fraude', 'électorale', 'est', 'plus', 'rare', 'aux', 'États-Unis', 'que', 'le', 'nombre', 'de', 'personnes', 'tu@@', 'ées', 'par', 'la', 'fou@@', 'd@@', 're.']
2021-12-20 00:21:26,655 - DEBUG - joeynmt.training - 	Raw hypothesis: ['The', 'Bren@@', 'n@@', 'an', 'Centre', 'considers', 'this', 'to', 'be', 'a', 'my@@', 'th@@', 'y,', 'claiming', 'that', 'electoral', 'fraud', 'is', 'more', 'rare', 'in', 'the', 'United', 'States', 'than', 'the', 'number', 'of', 'people', 'killed', 'by', 'the', 'pl@@', 'ate.']
2021-12-20 00:21:26,655 - INFO - joeynmt.training - 	Source:     Or, le Centre Brennan considère cette dernière comme un mythe, affirmant que la fraude électorale est plus rare aux États-Unis que le nombre de personnes tuées par la foudre.
2021-12-20 00:21:26,656 - INFO - joeynmt.training - 	Reference:  However, the Brennan Centre considers this a myth, stating that electoral fraud is rarer in the United States than the number of people killed by lightning.
2021-12-20 00:21:26,656 - INFO - joeynmt.training - 	Hypothesis: The Brennan Centre considers this to be a mythy, claiming that electoral fraud is more rare in the United States than the number of people killed by the plate.
2021-12-20 00:21:26,656 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step   250000: bleu:  19.24, loss: 172272.8125, ppl:   9.0665, duration: 144.2433s
2021-12-20 00:22:29,658 - INFO - joeynmt.training - Epoch  12, Step:   250200, Batch Loss:     1.702411, Tokens per Sec:     8336, Lr: 0.000500
2021-12-20 00:23:31,516 - INFO - joeynmt.training - Epoch  12, Step:   250400, Batch Loss:     1.403555, Tokens per Sec:     8317, Lr: 0.000500
2021-12-20 00:24:32,861 - INFO - joeynmt.training - Epoch  12, Step:   250600, Batch Loss:     1.531359, Tokens per Sec:     8315, Lr: 0.000500
2021-12-20 00:25:34,869 - INFO - joeynmt.training - Epoch  12, Step:   250800, Batch Loss:     1.705173, Tokens per Sec:     8370, Lr: 0.000500
2021-12-20 00:26:36,759 - INFO - joeynmt.training - Epoch  12, Step:   251000, Batch Loss:     1.527595, Tokens per Sec:     8376, Lr: 0.000500
2021-12-20 00:27:38,598 - INFO - joeynmt.training - Epoch  12, Step:   251200, Batch Loss:     1.462632, Tokens per Sec:     8412, Lr: 0.000500
2021-12-20 00:28:40,448 - INFO - joeynmt.training - Epoch  12, Step:   251400, Batch Loss:     1.716625, Tokens per Sec:     8341, Lr: 0.000500
2021-12-20 00:29:42,596 - INFO - joeynmt.training - Epoch  12, Step:   251600, Batch Loss:     1.466361, Tokens per Sec:     8410, Lr: 0.000500
2021-12-20 00:30:44,859 - INFO - joeynmt.training - Epoch  12, Step:   251800, Batch Loss:     1.494705, Tokens per Sec:     8316, Lr: 0.000500
2021-12-20 00:31:46,303 - INFO - joeynmt.training - Epoch  12, Step:   252000, Batch Loss:     1.477107, Tokens per Sec:     8252, Lr: 0.000500
2021-12-20 00:32:48,235 - INFO - joeynmt.training - Epoch  12, Step:   252200, Batch Loss:     1.348437, Tokens per Sec:     8316, Lr: 0.000500
2021-12-20 00:33:50,721 - INFO - joeynmt.training - Epoch  12, Step:   252400, Batch Loss:     1.417888, Tokens per Sec:     8366, Lr: 0.000500
2021-12-20 00:34:52,832 - INFO - joeynmt.training - Epoch  12, Step:   252600, Batch Loss:     1.427024, Tokens per Sec:     8437, Lr: 0.000500
2021-12-20 00:35:55,328 - INFO - joeynmt.training - Epoch  12, Step:   252800, Batch Loss:     1.340999, Tokens per Sec:     8386, Lr: 0.000500
2021-12-20 00:36:57,178 - INFO - joeynmt.training - Epoch  12, Step:   253000, Batch Loss:     1.283957, Tokens per Sec:     8380, Lr: 0.000500
2021-12-20 00:37:59,527 - INFO - joeynmt.training - Epoch  12, Step:   253200, Batch Loss:     1.552142, Tokens per Sec:     8375, Lr: 0.000500
